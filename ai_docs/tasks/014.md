# Task 014: Monitoring and Logging Infrastructure

## Overview
Implement comprehensive monitoring and logging for both backend and frontend applications, including structured logging, metrics collection, error tracking, and performance monitoring.

## Success Criteria
- [ ] All applications output structured JSON logs
- [ ] Metrics are collected for key performance indicators
- [ ] Errors are tracked with full context
- [ ] Performance metrics track p50, p95, p99 latencies
- [ ] Dashboards visualize system health
- [ ] Alerts trigger for critical issues

## Test First Approach

### Tests to Write BEFORE Implementation:

1. **Logging Tests** (`tests/backend/unit/test_logging.py`):
```python
def test_structured_logging():
    """Test structured log output."""
    # Test JSON log format
    # Test required fields present
    # Test correlation IDs included
    # Test sensitive data masked
    # Test log levels work correctly

def test_request_logging():
    """Test HTTP request logging."""
    # Test request details logged
    # Test response time tracked
    # Test status codes logged
    # Test user ID included
    # Test errors include stack traces

def test_log_aggregation():
    """Test log aggregation setup."""
    # Test logs written to correct location
    # Test log rotation works
    # Test old logs compressed
    # Test aggregation format
```

2. **Metrics Tests** (`tests/backend/unit/test_metrics.py`):
```python
def test_prometheus_metrics():
    """Test Prometheus metrics collection."""
    # Test metrics endpoint exists
    # Test counter increments
    # Test histogram observations
    # Test gauge updates
    # Test labels work correctly

def test_application_metrics():
    """Test custom application metrics."""
    # Test request count metric
    # Test request duration metric
    # Test error rate metric
    # Test business metrics
    # Test metric names follow conventions
```

3. **Error Tracking Tests** (`tests/backend/unit/test_error_tracking.py`):
```python
def test_error_capture():
    """Test error tracking integration."""
    # Test exceptions are captured
    # Test context is included
    # Test user info attached
    # Test breadcrumbs work
    # Test sampling works

def test_error_grouping():
    """Test error grouping logic."""
    # Test similar errors grouped
    # Test fingerprinting works
    # Test ignored errors filtered
    # Test rate limiting
```

## Implementation Details

1. **Logging Configuration** (`app/core/logging.py`):
```python
import structlog
from pythonjsonlogger import jsonlogger

def configure_logging():
    structlog.configure(
        processors=[
            structlog.stdlib.filter_by_level,
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            structlog.stdlib.PositionalArgumentsFormatter(),
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.UnicodeDecoder(),
            add_correlation_id,
            mask_sensitive_data,
            structlog.processors.JSONRenderer()
        ],
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        cache_logger_on_first_use=True,
    )

def add_correlation_id(logger, log_method, event_dict):
    """Add correlation ID to all logs."""
    if correlation_id := correlation_id_var.get():
        event_dict["correlation_id"] = correlation_id
    return event_dict

def mask_sensitive_data(logger, log_method, event_dict):
    """Mask sensitive fields in logs."""
    sensitive_fields = ["password", "token", "api_key", "secret"]
    # Recursively mask sensitive fields
    return event_dict
```

2. **Metrics Collection** (`app/core/metrics.py`):
```python
from prometheus_client import Counter, Histogram, Gauge, Info

# Request metrics
request_count = Counter(
    "http_requests_total",
    "Total HTTP requests",
    ["method", "endpoint", "status"]
)

request_duration = Histogram(
    "http_request_duration_seconds",
    "HTTP request duration",
    ["method", "endpoint"]
)

# Business metrics
documents_processed = Counter(
    "documents_processed_total",
    "Total documents processed",
    ["project_id", "file_type"]
)

embeddings_generated = Counter(
    "embeddings_generated_total",
    "Total embeddings generated",
    ["model", "project_id"]
)

active_users = Gauge(
    "active_users",
    "Currently active users"
)

# System metrics
database_pool_size = Gauge(
    "database_pool_size",
    "Database connection pool size",
    ["state"]  # active, idle
)
```

3. **Middleware Integration** (`app/middleware/monitoring.py`):
```python
class MonitoringMiddleware:
    async def __call__(self, request: Request, call_next):
        start_time = time.time()

        # Generate correlation ID
        correlation_id = str(uuid4())
        correlation_id_var.set(correlation_id)

        # Log request
        logger.info(
            "request_started",
            method=request.method,
            path=request.url.path,
            client_ip=request.client.host
        )

        try:
            response = await call_next(request)
            duration = time.time() - start_time

            # Record metrics
            request_count.labels(
                method=request.method,
                endpoint=request.url.path,
                status=response.status_code
            ).inc()

            request_duration.labels(
                method=request.method,
                endpoint=request.url.path
            ).observe(duration)

            # Log response
            logger.info(
                "request_completed",
                duration=duration,
                status_code=response.status_code
            )

            # Add correlation ID to response
            response.headers["X-Correlation-ID"] = correlation_id

            return response

        except Exception as e:
            duration = time.time() - start_time
            logger.exception(
                "request_failed",
                duration=duration,
                error=str(e)
            )
            raise
```

4. **Error Tracking** (`app/core/error_tracking.py`):
```python
import sentry_sdk
from sentry_sdk.integrations.fastapi import FastApiIntegration
from sentry_sdk.integrations.sqlalchemy import SqlalchemyIntegration

def configure_error_tracking():
    sentry_sdk.init(
        dsn=settings.SENTRY_DSN,
        environment=settings.ENVIRONMENT,
        integrations=[
            FastApiIntegration(transaction_style="endpoint"),
            SqlalchemyIntegration(),
        ],
        traces_sample_rate=0.1,
        profiles_sample_rate=0.1,
        before_send=before_send_filter,
    )

def before_send_filter(event, hint):
    """Filter sensitive data before sending to Sentry."""
    # Remove sensitive headers
    # Mask user data
    # Filter ignored errors
    return event
```

5. **Frontend Monitoring** (`frontend/src/monitoring/index.ts`):
```typescript
// Error boundary for React
export class ErrorBoundary extends Component {
  componentDidCatch(error: Error, errorInfo: ErrorInfo) {
    console.error('React error:', error, errorInfo);

    // Send to error tracking
    if (window.Sentry) {
      window.Sentry.captureException(error, {
        contexts: { react: { componentStack: errorInfo.componentStack } }
      });
    }
  }
}

// Performance monitoring
export const measurePerformance = () => {
  if ('performance' in window) {
    window.addEventListener('load', () => {
      const perfData = window.performance.timing;
      const pageLoadTime = perfData.loadEventEnd - perfData.navigationStart;

      // Send metrics
      sendMetric('page_load_time', pageLoadTime);
    });
  }
};
```

6. **Dashboard Setup**:
   - Grafana dashboards for metrics
   - Log aggregation with Loki/ELK
   - Error tracking dashboard
   - Custom business metrics
   - SLO/SLA monitoring
   - Cost tracking dashboard

## Dependencies
- Task 002: Project Structure
- Task 005: FastAPI Core

## Estimated Time
14-18 hours

## Required Skills
- Structured logging
- Prometheus metrics
- Error tracking tools
- Performance monitoring
- Dashboard creation
- Observability best practices
- Log aggregation systems
