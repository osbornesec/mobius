# Task 017: Hybrid Search Implementation

## Overview
Implement hybrid search functionality that combines Qdrant vector similarity search with PostgreSQL full-text search to provide comprehensive and accurate context retrieval for the Mobius platform.

## Success Criteria
- [ ] Vector search integrated with keyword search
- [ ] Hybrid ranking algorithm balances semantic and lexical relevance
- [ ] Search accuracy improves by 15% over vector-only search
- [ ] Combined search results properly merged and ranked
- [ ] Search latency remains under 200ms
- [ ] Configurable weighting between vector and text search
- [ ] Support for complex queries with filters

## Test First Approach

### Tests to Write BEFORE Implementation:

1. **Hybrid Search Core Tests** (`tests/backend/unit/test_hybrid_search.py`):
```python
import pytest
from app.services.hybrid_search import HybridSearchService

async def test_vector_search_component():
    """Test vector search component of hybrid search."""
    # Test vector similarity search
    # Test vector search ranking
    # Test vector search with metadata filters
    # Test vector search performance

async def test_text_search_component():
    """Test PostgreSQL full-text search component."""
    # Test text search with ts_query
    # Test text search ranking with ts_rank
    # Test text search with filters
    # Test text search performance

async def test_hybrid_ranking():
    """Test combined ranking algorithm."""
    # Test score normalization between vector and text
    # Test weighted ranking (e.g., 0.7 vector + 0.3 text)
    # Test ranking consistency
    # Test edge cases (no vector matches, no text matches)

async def test_result_merging():
    """Test merging and deduplication of results."""
    # Test result deduplication by document ID
    # Test score aggregation for duplicate documents
    # Test result ordering by combined score
    # Test result limit and pagination
```

2. **Search Quality Tests** (`tests/backend/unit/test_search_quality.py`):
```python
async def test_search_accuracy():
    """Test search accuracy against ground truth."""
    # Test with curated query-document pairs
    # Test precision and recall metrics
    # Test ranking quality (NDCG)
    # Verify 15% improvement over vector-only

async def test_query_types():
    """Test different query types and patterns."""
    # Test exact phrase queries
    # Test semantic concept queries
    # Test mixed keyword-semantic queries
    # Test technical term queries

async def test_edge_cases():
    """Test edge cases and error conditions."""
    # Test empty query
    # Test very long query
    # Test queries with special characters
    # Test queries in different languages
```

3. **Performance Tests** (`tests/backend/unit/test_hybrid_performance.py`):
```python
async def test_search_latency():
    """Test search performance under various conditions."""
    # Test search latency with small dataset (1k docs)
    # Test search latency with large dataset (10k docs)
    # Test concurrent search requests
    # Verify latency stays under 200ms

async def test_resource_usage():
    """Test resource consumption during search."""
    # Test memory usage during search
    # Test CPU usage patterns
    # Test database connection usage
```

## Implementation Details

1. **Hybrid Search Service**:
```python
# app/services/hybrid_search.py
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import asyncio
from sqlalchemy import text
from app.core.vector_store import QdrantVectorStore
from app.core.database import AsyncSession

@dataclass
class SearchResult:
    document_id: str
    content: str
    file_path: str
    vector_score: float
    text_score: float
    combined_score: float
    metadata: Dict[str, Any]

class HybridSearchService:
    def __init__(
        self,
        vector_store: QdrantVectorStore,
        db_session: AsyncSession,
        vector_weight: float = 0.7,
        text_weight: float = 0.3
    ):
        self.vector_store = vector_store
        self.db_session = db_session
        self.vector_weight = vector_weight
        self.text_weight = text_weight

    async def search(
        self,
        query: str,
        limit: int = 10,
        filters: Optional[Dict] = None
    ) -> List[SearchResult]:
        """Perform hybrid search combining vector and text search."""

        # Execute both searches concurrently
        vector_task = self._vector_search(query, limit * 2, filters)
        text_task = self._text_search(query, limit * 2, filters)

        vector_results, text_results = await asyncio.gather(
            vector_task, text_task
        )

        # Merge and rank results
        merged_results = self._merge_results(vector_results, text_results)

        # Apply final ranking and limit
        return self._rank_results(merged_results)[:limit]

    async def _vector_search(
        self,
        query: str,
        limit: int,
        filters: Optional[Dict]
    ) -> List[Dict]:
        """Perform vector similarity search."""
        # Generate query embedding
        query_embedding = await self._get_query_embedding(query)

        # Search Qdrant
        qdrant_filters = self._convert_filters_to_qdrant(filters)
        results = await self.vector_store.search_vectors(
            collection_name="documents",
            query_vector=query_embedding,
            limit=limit,
            filters=qdrant_filters
        )

        return [
            {
                "document_id": result.id,
                "vector_score": result.score,
                "metadata": result.payload
            }
            for result in results
        ]

    async def _text_search(
        self,
        query: str,
        limit: int,
        filters: Optional[Dict]
    ) -> List[Dict]:
        """Perform PostgreSQL full-text search."""
        # Prepare text search query
        search_query = self._prepare_text_query(query)

        # Build SQL query with filters
        sql_filters = self._build_sql_filters(filters)

        sql = text(f"""
            SELECT
                d.id as document_id,
                d.content,
                d.file_path,
                d.metadata,
                ts_rank(d.search_vector, plainto_tsquery(:query)) as text_score
            FROM documents d
            WHERE d.search_vector @@ plainto_tsquery(:query)
            {sql_filters}
            ORDER BY text_score DESC
            LIMIT :limit
        """)

        result = await self.db_session.execute(
            sql,
            {"query": search_query, "limit": limit}
        )

        return [
            {
                "document_id": row.document_id,
                "content": row.content,
                "file_path": row.file_path,
                "text_score": float(row.text_score),
                "metadata": row.metadata
            }
            for row in result.fetchall()
        ]

    def _merge_results(
        self,
        vector_results: List[Dict],
        text_results: List[Dict]
    ) -> Dict[str, SearchResult]:
        """Merge vector and text search results."""
        merged = {}

        # Normalize scores to 0-1 range
        vector_scores = [r["vector_score"] for r in vector_results]
        text_scores = [r["text_score"] for r in text_results]

        max_vector_score = max(vector_scores) if vector_scores else 1.0
        max_text_score = max(text_scores) if text_scores else 1.0

        # Process vector results
        for result in vector_results:
            doc_id = result["document_id"]
            normalized_vector_score = result["vector_score"] / max_vector_score

            merged[doc_id] = SearchResult(
                document_id=doc_id,
                content="",  # Will be filled from text results or DB
                file_path=result["metadata"].get("file_path", ""),
                vector_score=normalized_vector_score,
                text_score=0.0,
                combined_score=normalized_vector_score * self.vector_weight,
                metadata=result["metadata"]
            )

        # Process text results and merge
        for result in text_results:
            doc_id = result["document_id"]
            normalized_text_score = result["text_score"] / max_text_score

            if doc_id in merged:
                # Update existing result
                merged[doc_id].text_score = normalized_text_score
                merged[doc_id].combined_score = (
                    merged[doc_id].vector_score * self.vector_weight +
                    normalized_text_score * self.text_weight
                )
                merged[doc_id].content = result["content"]
            else:
                # Add new result
                merged[doc_id] = SearchResult(
                    document_id=doc_id,
                    content=result["content"],
                    file_path=result["file_path"],
                    vector_score=0.0,
                    text_score=normalized_text_score,
                    combined_score=normalized_text_score * self.text_weight,
                    metadata=result["metadata"]
                )

        return merged

    def _rank_results(self, merged_results: Dict[str, SearchResult]) -> List[SearchResult]:
        """Apply final ranking to merged results."""
        return sorted(
            merged_results.values(),
            key=lambda x: x.combined_score,
            reverse=True
        )
```

2. **Search Configuration**:
```python
# app/core/search_config.py
from pydantic import BaseSettings

class SearchSettings(BaseSettings):
    # Hybrid search weights
    VECTOR_SEARCH_WEIGHT: float = 0.7
    TEXT_SEARCH_WEIGHT: float = 0.3

    # Search limits
    MAX_SEARCH_RESULTS: int = 100
    DEFAULT_SEARCH_LIMIT: int = 10

    # Performance settings
    SEARCH_TIMEOUT_SECONDS: int = 5
    CONCURRENT_SEARCH_ENABLED: bool = True

    # Quality thresholds
    MIN_VECTOR_SCORE: float = 0.5
    MIN_TEXT_SCORE: float = 0.1
```

3. **API Integration**:
```python
# app/api/v1/search.py
from fastapi import APIRouter, Depends, Query
from app.services.hybrid_search import HybridSearchService

router = APIRouter(prefix="/search", tags=["search"])

@router.get("/hybrid")
async def hybrid_search(
    query: str = Query(..., description="Search query"),
    limit: int = Query(10, ge=1, le=100, description="Number of results"),
    vector_weight: float = Query(0.7, ge=0.0, le=1.0),
    text_weight: float = Query(0.3, ge=0.0, le=1.0),
    service: HybridSearchService = Depends(get_hybrid_search_service)
):
    """Perform hybrid search with configurable weights."""

    # Validate weights sum to 1.0
    if abs(vector_weight + text_weight - 1.0) > 0.01:
        raise HTTPException(
            status_code=400,
            detail="Vector and text weights must sum to 1.0"
        )

    # Update service weights
    service.vector_weight = vector_weight
    service.text_weight = text_weight

    # Perform search
    results = await service.search(query, limit)

    return {
        "query": query,
        "total_results": len(results),
        "vector_weight": vector_weight,
        "text_weight": text_weight,
        "results": [
            {
                "document_id": r.document_id,
                "file_path": r.file_path,
                "content_preview": r.content[:200] + "..." if len(r.content) > 200 else r.content,
                "scores": {
                    "vector": r.vector_score,
                    "text": r.text_score,
                    "combined": r.combined_score
                },
                "metadata": r.metadata
            }
            for r in results
        ]
    }
```

4. **Search Quality Evaluation**:
```python
# app/utils/search_evaluation.py
from typing import List, Tuple
import numpy as np

class SearchEvaluator:
    def __init__(self):
        self.test_queries = self._load_test_queries()

    async def evaluate_search_quality(
        self,
        search_service: HybridSearchService
    ) -> Dict[str, float]:
        """Evaluate search quality metrics."""
        precision_scores = []
        recall_scores = []
        ndcg_scores = []

        for query, relevant_docs in self.test_queries:
            results = await search_service.search(query, limit=10)
            retrieved_docs = [r.document_id for r in results]

            # Calculate metrics
            precision = self._calculate_precision(retrieved_docs, relevant_docs)
            recall = self._calculate_recall(retrieved_docs, relevant_docs)
            ndcg = self._calculate_ndcg(retrieved_docs, relevant_docs)

            precision_scores.append(precision)
            recall_scores.append(recall)
            ndcg_scores.append(ndcg)

        return {
            "precision": np.mean(precision_scores),
            "recall": np.mean(recall_scores),
            "ndcg": np.mean(ndcg_scores),
            "f1_score": 2 * np.mean(precision_scores) * np.mean(recall_scores) /
                       (np.mean(precision_scores) + np.mean(recall_scores))
        }
```

## Dependencies
- Task 016: Qdrant Vector Database Integration
- Task 006: Vector Embedding Generation
- Task 007: Basic Search API
- Task 003: PostgreSQL Setup (for full-text search)

## Estimated Time
12-16 hours

## Required Skills
- Information retrieval and ranking algorithms
- PostgreSQL full-text search (ts_vector, ts_query)
- Vector similarity search concepts
- Performance optimization and testing
- Search quality evaluation metrics

## Notes
- Hybrid search significantly improves recall for keyword-specific queries
- Weight tuning (vector vs text) should be configurable and domain-specific
- Consider caching frequently searched queries for performance
- Monitor search quality metrics continuously in production
