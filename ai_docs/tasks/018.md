# Task 018: Context Ranking and Relevance Scoring

## Overview
Implement ML-based context ranking and relevance scoring system that learns from user feedback to continuously improve context retrieval quality and relevance for AI-assisted coding tasks.

## Success Criteria
- [ ] Relevance scoring algorithm ranks context by utility for AI coding tasks
- [ ] User feedback mechanism captures relevance ratings
- [ ] Machine learning model learns from feedback to improve scoring
- [ ] 90% user relevance satisfaction achieved
- [ ] Context ranking adapts to user preferences and coding patterns
- [ ] Real-time ranking updates with sub-100ms latency
- [ ] A/B testing framework for ranking algorithm improvements

## Test First Approach

### Tests to Write BEFORE Implementation:

1. **Relevance Scoring Tests** (`tests/test_relevance_scoring.py`):
```python
import pytest
from app.services.relevance_scorer import RelevanceScorer

async def test_base_relevance_features():
    """Test basic relevance scoring features."""
    # Test recency scoring (newer files more relevant)
    # Test frequency scoring (often-accessed files more relevant)
    # Test proximity scoring (files in same directory more relevant)
    # Test file type relevance (source files vs docs)
    # Test size penalty (very large files less relevant)

async def test_semantic_relevance():
    """Test semantic relevance scoring."""
    # Test semantic similarity to current context
    # Test topic modeling relevance
    # Test code similarity metrics
    # Test import/dependency relevance

async def test_user_context_relevance():
    """Test user-specific context factors."""
    # Test recently edited file boost
    # Test frequently accessed file boost
    # Test project-specific relevance patterns
    # Test coding session context awareness

async def test_score_normalization():
    """Test relevance score normalization."""
    # Test score range consistency (0.0 to 1.0)
    # Test score distribution properties
    # Test score stability across different contexts
```

2. **Feedback Learning Tests** (`tests/test_feedback_learning.py`):
```python
async def test_feedback_collection():
    """Test user feedback collection mechanism."""
    # Test explicit feedback (thumbs up/down)
    # Test implicit feedback (time spent, usage patterns)
    # Test feedback data validation
    # Test feedback storage and retrieval

async def test_model_training():
    """Test ML model training from feedback."""
    # Test feature extraction from feedback data
    # Test model training convergence
    # Test model validation and testing
    # Test incremental learning capabilities

async def test_ranking_improvement():
    """Test ranking improvement from learning."""
    # Test ranking changes after feedback
    # Test personalized ranking adaptation
    # Test global ranking improvements
    # Test overfitting prevention
```

3. **Performance Tests** (`tests/test_ranking_performance.py`):
```python
async def test_scoring_latency():
    """Test relevance scoring performance."""
    # Test scoring latency for single document
    # Test batch scoring performance
    # Test scoring under concurrent requests
    # Verify sub-100ms latency requirement

async def test_learning_efficiency():
    """Test learning algorithm efficiency."""
    # Test training time for incremental updates
    # Test memory usage during learning
    # Test model inference performance
```

## Implementation Details

1. **Relevance Scoring Engine**:
```python
# app/services/relevance_scorer.py
from typing import List, Dict, Any, Optional
import numpy as np
from datetime import datetime, timedelta
from dataclasses import dataclass
import pickle
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler

@dataclass
class DocumentContext:
    document_id: str
    file_path: str
    content: str
    last_modified: datetime
    file_size: int
    access_count: int
    last_accessed: datetime
    file_type: str
    metadata: Dict[str, Any]

@dataclass
class UserContext:
    user_id: str
    current_file: Optional[str]
    recent_files: List[str]
    session_context: Dict[str, Any]
    coding_patterns: Dict[str, Any]

class RelevanceScorer:
    def __init__(self):
        self.model = RandomForestRegressor(n_estimators=100, random_state=42)
        self.scaler = StandardScaler()
        self.feature_weights = {
            'recency': 0.25,
            'frequency': 0.20,
            'semantic_similarity': 0.30,
            'file_proximity': 0.15,
            'user_preference': 0.10
        }
        self.is_trained = False
        
    async def score_relevance(
        self,
        documents: List[DocumentContext],
        user_context: UserContext,
        query_context: str
    ) -> List[float]:
        """Score document relevance for given context."""
        
        # Extract features for all documents
        features = []
        for doc in documents:
            doc_features = await self._extract_features(doc, user_context, query_context)
            features.append(doc_features)
        
        features_array = np.array(features)
        
        if self.is_trained:
            # Use trained model for scoring
            features_scaled = self.scaler.transform(features_array)
            scores = self.model.predict(features_scaled)
        else:
            # Use rule-based scoring until model is trained
            scores = self._rule_based_scoring(features_array)
        
        # Ensure scores are in 0-1 range
        scores = np.clip(scores, 0.0, 1.0)
        
        return scores.tolist()
    
    async def _extract_features(
        self,
        document: DocumentContext,
        user_context: UserContext,
        query_context: str
    ) -> List[float]:
        """Extract relevance features for a document."""
        features = []
        
        # Recency features
        recency_score = self._calculate_recency_score(document.last_modified)
        features.append(recency_score)
        
        # Frequency features
        frequency_score = self._calculate_frequency_score(document.access_count)
        features.append(frequency_score)
        
        # File proximity features
        proximity_score = self._calculate_proximity_score(
            document.file_path, 
            user_context.current_file
        )
        features.append(proximity_score)
        
        # File type relevance
        type_score = self._calculate_type_relevance(document.file_type)
        features.append(type_score)
        
        # Size penalty
        size_score = self._calculate_size_score(document.file_size)
        features.append(size_score)
        
        # Semantic similarity (requires embedding comparison)
        semantic_score = await self._calculate_semantic_similarity(
            document.content, 
            query_context
        )
        features.append(semantic_score)
        
        # User preference features
        user_pref_score = self._calculate_user_preference(
            document, 
            user_context
        )
        features.append(user_pref_score)
        
        return features
    
    def _calculate_recency_score(self, last_modified: datetime) -> float:
        """Calculate recency-based relevance score."""
        now = datetime.utcnow()
        age_days = (now - last_modified).days
        
        # Exponential decay with half-life of 30 days
        score = np.exp(-age_days / 30.0)
        return score
    
    def _calculate_frequency_score(self, access_count: int) -> float:
        """Calculate frequency-based relevance score."""
        # Log-scale frequency scoring
        if access_count <= 0:
            return 0.0
        
        # Normalize using log scale
        score = np.log(access_count + 1) / np.log(1000)  # Cap at 1000 accesses
        return min(score, 1.0)
    
    def _calculate_proximity_score(
        self, 
        doc_path: str, 
        current_file: Optional[str]
    ) -> float:
        """Calculate file proximity relevance score."""
        if not current_file:
            return 0.5  # Neutral score
        
        doc_parts = doc_path.split('/')
        current_parts = current_file.split('/')
        
        # Calculate common directory depth
        common_depth = 0
        for i, (d, c) in enumerate(zip(doc_parts, current_parts)):
            if d == c:
                common_depth = i + 1
            else:
                break
        
        # Score based on shared directory depth
        max_depth = max(len(doc_parts), len(current_parts))
        score = common_depth / max_depth if max_depth > 0 else 0.0
        
        return score
    
    def _calculate_type_relevance(self, file_type: str) -> float:
        """Calculate file type relevance for coding context."""
        type_scores = {
            'python': 1.0,
            'javascript': 1.0,
            'typescript': 1.0,
            'java': 0.9,
            'cpp': 0.9,
            'json': 0.8,
            'yaml': 0.7,
            'markdown': 0.6,
            'txt': 0.4,
            'log': 0.2
        }
        
        return type_scores.get(file_type.lower(), 0.5)
    
    def _calculate_size_score(self, file_size: int) -> float:
        """Calculate size-based relevance score."""
        # Penalty for very large files
        optimal_size = 10000  # 10KB optimal
        max_size = 100000     # 100KB max before heavy penalty
        
        if file_size <= optimal_size:
            return 1.0
        elif file_size <= max_size:
            # Linear decay from optimal to max
            return 1.0 - (file_size - optimal_size) / (max_size - optimal_size) * 0.5
        else:
            # Heavy penalty for very large files
            return 0.1
    
    async def _calculate_semantic_similarity(
        self, 
        content: str, 
        query_context: str
    ) -> float:
        """Calculate semantic similarity score."""
        # This would integrate with the embedding service
        # For now, return a placeholder
        # TODO: Implement actual semantic similarity using embeddings
        return 0.5
    
    def _calculate_user_preference(
        self, 
        document: DocumentContext, 
        user_context: UserContext
    ) -> float:
        """Calculate user-specific preference score."""
        score = 0.0
        
        # Boost for recently accessed files
        if document.document_id in user_context.recent_files:
            score += 0.3
        
        # Boost based on user coding patterns
        if document.file_type in user_context.coding_patterns.get('preferred_types', []):
            score += 0.2
        
        return min(score, 1.0)
    
    def _rule_based_scoring(self, features_array: np.ndarray) -> np.ndarray:
        """Rule-based scoring before ML model is trained."""
        # Weighted combination of features
        weights = list(self.feature_weights.values())
        scores = np.average(features_array, axis=1, weights=weights)
        return scores
```

2. **Feedback Collection System**:
```python
# app/services/feedback_collector.py
from typing import Optional
from datetime import datetime
from sqlalchemy.orm import Session

class FeedbackCollector:
    def __init__(self, db_session: Session):
        self.db_session = db_session
    
    async def record_explicit_feedback(
        self,
        user_id: str,
        document_id: str,
        query_context: str,
        relevance_score: float,  # 0.0 to 1.0
        feedback_type: str = "explicit"
    ):
        """Record explicit user feedback on document relevance."""
        feedback = RelevanceFeedback(
            user_id=user_id,
            document_id=document_id,
            query_context=query_context,
            relevance_score=relevance_score,
            feedback_type=feedback_type,
            timestamp=datetime.utcnow()
        )
        
        self.db_session.add(feedback)
        await self.db_session.commit()
    
    async def record_implicit_feedback(
        self,
        user_id: str,
        document_id: str,
        query_context: str,
        interaction_data: Dict[str, Any]
    ):
        """Record implicit feedback from user interactions."""
        # Convert interaction data to relevance score
        relevance_score = self._infer_relevance_from_interaction(interaction_data)
        
        await self.record_explicit_feedback(
            user_id=user_id,
            document_id=document_id,
            query_context=query_context,
            relevance_score=relevance_score,
            feedback_type="implicit"
        )
    
    def _infer_relevance_from_interaction(self, interaction_data: Dict) -> float:
        """Infer relevance score from interaction patterns."""
        score = 0.0
        
        # Time spent reading
        time_spent = interaction_data.get('time_spent_seconds', 0)
        if time_spent > 30:
            score += 0.4
        elif time_spent > 10:
            score += 0.2
        
        # Whether user copied from document
        if interaction_data.get('copied_content', False):
            score += 0.3
        
        # Whether user opened the file
        if interaction_data.get('opened_file', False):
            score += 0.3
        
        return min(score, 1.0)
```

3. **Learning Engine**:
```python
# app/services/learning_engine.py
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

class RelevanceLearningEngine:
    def __init__(self, relevance_scorer: RelevanceScorer):
        self.scorer = relevance_scorer
        self.training_data = []
        self.min_training_samples = 100
    
    async def update_model(self):
        """Update the relevance model with new feedback data."""
        # Collect training data from feedback
        training_data = await self._collect_training_data()
        
        if len(training_data) < self.min_training_samples:
            return False  # Not enough data to train
        
        # Prepare features and targets
        X, y = self._prepare_training_data(training_data)
        
        # Split data for validation
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # Scale features
        X_train_scaled = self.scorer.scaler.fit_transform(X_train)
        X_test_scaled = self.scorer.scaler.transform(X_test)
        
        # Train model
        self.scorer.model.fit(X_train_scaled, y_train)
        
        # Validate model performance
        y_pred = self.scorer.model.predict(X_test_scaled)
        r2 = r2_score(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)
        
        # Only update if model performance is acceptable
        if r2 > 0.3:  # Minimum R² threshold
            self.scorer.is_trained = True
            return True
        
        return False
    
    async def _collect_training_data(self) -> List[Dict]:
        """Collect training data from feedback database."""
        # Query feedback database for training examples
        # This would integrate with the feedback collection system
        pass
    
    def _prepare_training_data(self, raw_data: List[Dict]) -> tuple:
        """Prepare features and targets for model training."""
        features = []
        targets = []
        
        for example in raw_data:
            features.append(example['features'])
            targets.append(example['relevance_score'])
        
        return np.array(features), np.array(targets)
```

4. **API Integration**:
```python
# app/api/v1/relevance.py
from fastapi import APIRouter, Depends
from app.services.relevance_scorer import RelevanceScorer

router = APIRouter(prefix="/relevance", tags=["relevance"])

@router.post("/feedback")
async def submit_feedback(
    document_id: str,
    relevance_score: float,
    feedback_type: str = "explicit",
    user_id: str = Depends(get_current_user_id),
    feedback_collector: FeedbackCollector = Depends(get_feedback_collector)
):
    """Submit relevance feedback for a document."""
    await feedback_collector.record_explicit_feedback(
        user_id=user_id,
        document_id=document_id,
        query_context="",  # Would be provided in request
        relevance_score=relevance_score,
        feedback_type=feedback_type
    )
    
    return {"status": "feedback_recorded"}

@router.get("/score/{document_id}")
async def get_relevance_score(
    document_id: str,
    query_context: str,
    scorer: RelevanceScorer = Depends(get_relevance_scorer)
):
    """Get relevance score for a specific document."""
    # This would integrate with document retrieval
    # and user context to provide relevance score
    pass
```

## Dependencies
- Task 017: Hybrid Search Implementation
- Task 007: Basic Search API
- Task 012: Analytics Foundation
- Task 005: FastAPI Core Application

## Estimated Time
20-24 hours

## Required Skills
- Machine learning and feature engineering
- User feedback collection and analysis
- Information retrieval metrics and evaluation
- Real-time model updating and deployment
- A/B testing and experimentation

## Notes
- Start with rule-based scoring and gradually introduce ML as feedback accumulates
- Focus on implicit feedback collection to minimize user friction
- Consider cold start problem for new users with no feedback history
- Implement proper validation to prevent model degradation over time