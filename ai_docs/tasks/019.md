# Task 019: Multi-File Context Aggregation

## Overview
Implement intelligent context aggregation that combines relevant content from multiple files into optimized context packages for AI coding assistants, reducing token usage while maintaining context quality.

## Success Criteria
- [ ] Smart context merging eliminates redundant information
- [ ] Context size reduced by 50% through intelligent aggregation
- [ ] Context quality maintained or improved (no loss of important information)
- [ ] Overlap detection identifies and handles duplicate content
- [ ] Dependency-aware aggregation includes related files intelligently
- [ ] Token-aware aggregation respects AI model context limits
- [ ] Real-time aggregation with sub-200ms latency

## Test First Approach

### Tests to Write BEFORE Implementation:

1. **Context Merging Tests** (`tests/test_context_aggregation.py`):
```python
import pytest
from app.services.context_aggregator import ContextAggregator

async def test_duplicate_content_removal():
    """Test removal of duplicate content across files."""
    # Test identical function definitions
    # Test similar code blocks
    # Test repeated imports/dependencies
    # Test common boilerplate code removal

async def test_dependency_resolution():
    """Test intelligent dependency inclusion."""
    # Test import statement resolution
    # Test function call dependency tracking
    # Test class inheritance relationships
    # Test module interdependencies

async def test_content_prioritization():
    """Test prioritization of important content."""
    # Test main function prioritization
    # Test class definition prioritization
    # Test recently modified content priority
    # Test user-specified priority hints

async def test_size_optimization():
    """Test context size reduction."""
    # Test 50% size reduction target
    # Test preservation of critical information
    # Test token counting accuracy
    # Test adaptive compression based on limits
```

2. **Aggregation Quality Tests** (`tests/test_aggregation_quality.py`):
```python
async def test_context_completeness():
    """Test that aggregated context remains complete."""
    # Test function signature preservation
    # Test important comments retention
    # Test error handling code inclusion
    # Test configuration and constants

async def test_code_coherence():
    """Test that aggregated code remains coherent."""
    # Test that related functions stay together
    # Test that class methods aren't separated
    # Test that try-catch blocks stay intact
    # Test that async/await patterns are preserved

async def test_edge_cases():
    """Test edge cases in aggregation."""
    # Test very large files
    # Test binary files handling
    # Test empty files
    # Test files with encoding issues
```

3. **Performance Tests** (`tests/test_aggregation_performance.py`):
```python
async def test_aggregation_latency():
    """Test aggregation performance."""
    # Test single file aggregation speed
    # Test multi-file aggregation (5-10 files)
    # Test large codebase aggregation (100+ files)
    # Verify sub-200ms latency requirement

async def test_memory_efficiency():
    """Test memory usage during aggregation."""
    # Test memory usage with large files
    # Test memory cleanup after aggregation
    # Test concurrent aggregation requests
```

## Implementation Details

1. **Context Aggregation Engine**:
```python
# app/services/context_aggregator.py
from typing import List, Dict, Any, Set, Tuple
import ast
import re
from dataclasses import dataclass
from collections import defaultdict
import hashlib

@dataclass
class CodeElement:
    """Represents a code element for aggregation."""
    content: str
    element_type: str  # function, class, import, variable, etc.
    name: str
    file_path: str
    line_start: int
    line_end: int
    dependencies: Set[str]
    importance_score: float

@dataclass
class AggregatedContext:
    """Result of context aggregation."""
    content: str
    included_files: List[str]
    excluded_elements: List[str]
    compression_ratio: float
    token_count: int
    quality_metrics: Dict[str, float]

class ContextAggregator:
    def __init__(self, max_tokens: int = 8000):
        self.max_tokens = max_tokens
        self.token_estimator = self._init_token_estimator()
        
    async def aggregate_context(
        self,
        file_contents: List[Dict[str, Any]],
        query_context: str = "",
        user_preferences: Dict[str, Any] = None
    ) -> AggregatedContext:
        """Aggregate multiple files into optimized context."""
        
        # Parse all files into code elements
        code_elements = []
        for file_data in file_contents:
            elements = await self._parse_file_elements(file_data)
            code_elements.extend(elements)
        
        # Remove duplicates
        deduplicated_elements = self._remove_duplicates(code_elements)
        
        # Resolve dependencies
        dependency_graph = self._build_dependency_graph(deduplicated_elements)
        
        # Score importance
        scored_elements = self._score_importance(
            deduplicated_elements,
            dependency_graph,
            query_context,
            user_preferences
        )
        
        # Select optimal subset
        selected_elements = self._select_optimal_subset(
            scored_elements,
            dependency_graph
        )
        
        # Generate aggregated content
        aggregated_content = self._generate_aggregated_content(selected_elements)
        
        # Calculate metrics
        original_size = sum(len(f['content']) for f in file_contents)
        final_size = len(aggregated_content)
        compression_ratio = 1 - (final_size / original_size)
        
        return AggregatedContext(
            content=aggregated_content,
            included_files=[e.file_path for e in selected_elements],
            excluded_elements=[e.name for e in scored_elements if e not in selected_elements],
            compression_ratio=compression_ratio,
            token_count=self._estimate_tokens(aggregated_content),
            quality_metrics=self._calculate_quality_metrics(selected_elements, scored_elements)
        )
    
    async def _parse_file_elements(self, file_data: Dict[str, Any]) -> List[CodeElement]:
        """Parse a file into its constituent code elements."""
        file_path = file_data['file_path']
        content = file_data['content']
        file_type = file_data.get('file_type', '')
        
        if file_type == 'python':
            return self._parse_python_file(content, file_path)
        elif file_type in ['javascript', 'typescript']:
            return self._parse_js_file(content, file_path)
        else:
            # Generic text-based parsing
            return self._parse_generic_file(content, file_path)
    
    def _parse_python_file(self, content: str, file_path: str) -> List[CodeElement]:
        """Parse Python file using AST."""
        elements = []
        
        try:
            tree = ast.parse(content)
        except SyntaxError:
            # Fallback to generic parsing if AST fails
            return self._parse_generic_file(content, file_path)
        
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                element = self._extract_function_element(node, content, file_path)
                elements.append(element)
            elif isinstance(node, ast.ClassDef):
                element = self._extract_class_element(node, content, file_path)
                elements.append(element)
            elif isinstance(node, ast.Import):
                element = self._extract_import_element(node, content, file_path)
                elements.append(element)
            elif isinstance(node, ast.ImportFrom):
                element = self._extract_import_from_element(node, content, file_path)
                elements.append(element)
        
        return elements
    
    def _extract_function_element(
        self, 
        node: ast.FunctionDef, 
        content: str, 
        file_path: str
    ) -> CodeElement:
        """Extract function definition as code element."""
        lines = content.split('\n')
        
        # Get function content
        start_line = node.lineno - 1
        end_line = node.end_lineno if hasattr(node, 'end_lineno') else start_line + 1
        function_content = '\n'.join(lines[start_line:end_line])
        
        # Extract dependencies (calls to other functions)
        dependencies = set()
        for child in ast.walk(node):
            if isinstance(child, ast.Call) and isinstance(child.func, ast.Name):
                dependencies.add(child.func.id)
        
        return CodeElement(
            content=function_content,
            element_type='function',
            name=node.name,
            file_path=file_path,
            line_start=start_line,
            line_end=end_line,
            dependencies=dependencies,
            importance_score=0.0  # Will be calculated later
        )
    
    def _extract_class_element(
        self, 
        node: ast.ClassDef, 
        content: str, 
        file_path: str
    ) -> CodeElement:
        """Extract class definition as code element."""
        lines = content.split('\n')
        
        start_line = node.lineno - 1
        end_line = node.end_lineno if hasattr(node, 'end_lineno') else start_line + 1
        class_content = '\n'.join(lines[start_line:end_line])
        
        # Extract base classes and method calls
        dependencies = set()
        for base in node.bases:
            if isinstance(base, ast.Name):
                dependencies.add(base.id)
        
        return CodeElement(
            content=class_content,
            element_type='class',
            name=node.name,
            file_path=file_path,
            line_start=start_line,
            line_end=end_line,
            dependencies=dependencies,
            importance_score=0.0
        )
    
    def _remove_duplicates(self, elements: List[CodeElement]) -> List[CodeElement]:
        """Remove duplicate code elements."""
        seen_hashes = set()
        deduplicated = []
        
        for element in elements:
            # Create content hash for duplicate detection
            content_hash = hashlib.md5(
                element.content.encode('utf-8')
            ).hexdigest()
            
            if content_hash not in seen_hashes:
                seen_hashes.add(content_hash)
                deduplicated.append(element)
        
        return deduplicated
    
    def _build_dependency_graph(
        self, 
        elements: List[CodeElement]
    ) -> Dict[str, Set[str]]:
        """Build dependency graph between code elements."""
        element_names = {elem.name for elem in elements}
        dependency_graph = defaultdict(set)
        
        for element in elements:
            # Only include dependencies that exist in our element set
            real_dependencies = element.dependencies.intersection(element_names)
            dependency_graph[element.name] = real_dependencies
        
        return dependency_graph
    
    def _score_importance(
        self,
        elements: List[CodeElement],
        dependency_graph: Dict[str, Set[str]],
        query_context: str,
        user_preferences: Dict[str, Any]
    ) -> List[CodeElement]:
        """Score importance of each code element."""
        
        # Calculate dependency scores (how many things depend on this)
        dependency_counts = defaultdict(int)
        for deps in dependency_graph.values():
            for dep in deps:
                dependency_counts[dep] += 1
        
        for element in elements:
            score = 0.0
            
            # Base score by element type
            type_scores = {
                'class': 0.8,
                'function': 0.6,
                'import': 0.4,
                'variable': 0.2
            }
            score += type_scores.get(element.element_type, 0.3)
            
            # Dependency score (how many things depend on this)
            dep_count = dependency_counts.get(element.name, 0)
            score += min(dep_count * 0.1, 0.3)
            
            # Recency score (if we have modification times)
            # TODO: Add recency scoring when file metadata is available
            
            # Query relevance score
            if query_context and query_context.lower() in element.content.lower():
                score += 0.2
            
            # User preference score
            if user_preferences:
                if element.element_type in user_preferences.get('preferred_types', []):
                    score += 0.1
            
            element.importance_score = min(score, 1.0)
        
        return sorted(elements, key=lambda x: x.importance_score, reverse=True)
    
    def _select_optimal_subset(
        self,
        scored_elements: List[CodeElement],
        dependency_graph: Dict[str, Set[str]]
    ) -> List[CodeElement]:
        """Select optimal subset of elements within token limits."""
        selected = []
        selected_names = set()
        current_tokens = 0
        
        # Always include high-importance elements first
        for element in scored_elements:
            element_tokens = self._estimate_tokens(element.content)
            
            # Check if adding this element would exceed token limit
            if current_tokens + element_tokens > self.max_tokens:
                break
            
            # Add element
            selected.append(element)
            selected_names.add(element.name)
            current_tokens += element_tokens
            
            # Add required dependencies if they fit
            for dep_name in element.dependencies:
                if dep_name not in selected_names:
                    dep_element = next(
                        (e for e in scored_elements if e.name == dep_name), 
                        None
                    )
                    if dep_element:
                        dep_tokens = self._estimate_tokens(dep_element.content)
                        if current_tokens + dep_tokens <= self.max_tokens:
                            selected.append(dep_element)
                            selected_names.add(dep_name)
                            current_tokens += dep_tokens
        
        return selected
    
    def _generate_aggregated_content(self, elements: List[CodeElement]) -> str:
        """Generate the final aggregated content."""
        content_parts = []
        
        # Group elements by file
        file_groups = defaultdict(list)
        for element in elements:
            file_groups[element.file_path].append(element)
        
        # Generate content with file headers
        for file_path, file_elements in file_groups.items():
            content_parts.append(f"# File: {file_path}")
            content_parts.append("")
            
            # Sort elements by line number within file
            file_elements.sort(key=lambda x: x.line_start)
            
            for element in file_elements:
                content_parts.append(element.content)
                content_parts.append("")
        
        return "\n".join(content_parts)
    
    def _estimate_tokens(self, content: str) -> int:
        """Estimate token count for content."""
        # Simple approximation: ~4 characters per token
        return len(content) // 4
    
    def _calculate_quality_metrics(
        self,
        selected_elements: List[CodeElement],
        all_elements: List[CodeElement]
    ) -> Dict[str, float]:
        """Calculate quality metrics for the aggregation."""
        if not all_elements:
            return {}
        
        # Coverage metrics
        selected_count = len(selected_elements)
        total_count = len(all_elements)
        element_coverage = selected_count / total_count
        
        # Importance coverage
        total_importance = sum(e.importance_score for e in all_elements)
        selected_importance = sum(e.importance_score for e in selected_elements)
        importance_coverage = selected_importance / total_importance if total_importance > 0 else 0
        
        # Dependency completeness
        selected_names = {e.name for e in selected_elements}
        missing_deps = set()
        for element in selected_elements:
            missing_deps.update(element.dependencies - selected_names)
        
        dependency_completeness = 1.0 - (len(missing_deps) / max(len(selected_names), 1))
        
        return {
            'element_coverage': element_coverage,
            'importance_coverage': importance_coverage,
            'dependency_completeness': dependency_completeness
        }
```

2. **API Integration**:
```python
# app/api/v1/context.py
from fastapi import APIRouter, Depends, HTTPException
from app.services.context_aggregator import ContextAggregator

router = APIRouter(prefix="/context", tags=["context"])

@router.post("/aggregate")
async def aggregate_context(
    file_ids: List[str],
    query_context: str = "",
    max_tokens: int = 8000,
    aggregator: ContextAggregator = Depends(get_context_aggregator)
):
    """Aggregate multiple files into optimized context."""
    
    # Retrieve file contents (this would integrate with document service)
    file_contents = await get_file_contents(file_ids)
    
    # Set token limit
    aggregator.max_tokens = max_tokens
    
    # Perform aggregation
    result = await aggregator.aggregate_context(
        file_contents=file_contents,
        query_context=query_context
    )
    
    return {
        "aggregated_content": result.content,
        "compression_ratio": result.compression_ratio,
        "token_count": result.token_count,
        "included_files": result.included_files,
        "quality_metrics": result.quality_metrics
    }
```

3. **Caching Layer**:
```python
# app/services/aggregation_cache.py
import hashlib
import json
from typing import Optional

class AggregationCache:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.cache_ttl = 3600  # 1 hour
    
    async def get_cached_aggregation(
        self,
        file_ids: List[str],
        query_context: str,
        max_tokens: int
    ) -> Optional[AggregatedContext]:
        """Get cached aggregation result."""
        cache_key = self._generate_cache_key(file_ids, query_context, max_tokens)
        
        cached_data = await self.redis.get(cache_key)
        if cached_data:
            return AggregatedContext(**json.loads(cached_data))
        
        return None
    
    async def cache_aggregation(
        self,
        file_ids: List[str],
        query_context: str,
        max_tokens: int,
        result: AggregatedContext
    ):
        """Cache aggregation result."""
        cache_key = self._generate_cache_key(file_ids, query_context, max_tokens)
        
        # Convert result to JSON
        result_data = {
            'content': result.content,
            'included_files': result.included_files,
            'excluded_elements': result.excluded_elements,
            'compression_ratio': result.compression_ratio,
            'token_count': result.token_count,
            'quality_metrics': result.quality_metrics
        }
        
        await self.redis.setex(
            cache_key,
            self.cache_ttl,
            json.dumps(result_data)
        )
    
    def _generate_cache_key(
        self,
        file_ids: List[str],
        query_context: str,
        max_tokens: int
    ) -> str:
        """Generate cache key for aggregation parameters."""
        key_data = {
            'file_ids': sorted(file_ids),
            'query_context': query_context,
            'max_tokens': max_tokens
        }
        
        key_string = json.dumps(key_data, sort_keys=True)
        key_hash = hashlib.md5(key_string.encode()).hexdigest()
        
        return f"aggregation:{key_hash}"
```

## Dependencies
- Task 018: Context Ranking and Relevance Scoring
- Task 016: Qdrant Vector Database Integration
- Task 006: Vector Embedding Generation
- Task 004: Redis Setup (for caching)

## Estimated Time
16-20 hours

## Required Skills
- Abstract Syntax Tree (AST) parsing for multiple languages
- Graph algorithms for dependency resolution
- Code analysis and static analysis techniques
- Performance optimization for large codebases
- Token counting and language model constraints

## Notes
- Focus on Python and JavaScript/TypeScript initially, expand to other languages later
- Implement comprehensive caching to avoid re-aggregating the same file sets
- Consider using language-specific parsers (tree-sitter) for better accuracy
- Monitor aggregation quality metrics in production to tune algorithms