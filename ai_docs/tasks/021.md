# Task 021: OpenAI/Anthropic API Integration

## Overview
Implement robust OpenAI and Anthropic API integration with streaming support, comprehensive error handling, rate limiting, and fallback mechanisms for the Mobius multi-agent system.

## Success Criteria
- [ ] AsyncOpenAI and AsyncAnthropic clients properly configured
- [ ] Streaming responses with real-time delta processing
- [ ] Comprehensive error handling for all API status codes
- [ ] Rate limiting with exponential backoff and retry logic
- [ ] API quota monitoring and usage tracking
- [ ] Fallback mechanisms between providers
- [ ] Response caching for duplicate requests
- [ ] Request latency consistently under 3 seconds

## Test First Approach

### Tests to Write BEFORE Implementation:

1. **API Client Configuration Tests** (`tests/backend/unit/test_api_clients.py`):
```python
import pytest
from app.services.ai_providers import OpenAIProvider, AnthropicProvider

async def test_openai_client_initialization():
    """Test OpenAI client setup with proper configuration."""
    # Test AsyncOpenAI client creation
    # Test API key validation
    # Test timeout configuration
    # Test retry configuration

async def test_anthropic_client_initialization():
    """Test Anthropic client setup with proper configuration."""
    # Test AsyncAnthropic client creation
    # Test API key validation
    # Test timeout configuration
    # Test retry configuration

async def test_client_health_checks():
    """Test API client health check mechanisms."""
    # Test OpenAI health check
    # Test Anthropic health check
    # Test provider availability detection
```

2. **Streaming Response Tests** (`tests/backend/unit/test_streaming.py`):
```python
async def test_openai_streaming():
    """Test OpenAI streaming chat completions."""
    # Test streaming chat completion
    # Test event type handling (content.delta, content.done)
    # Test stream termination
    # Test stream error handling

async def test_anthropic_streaming():
    """Test Anthropic streaming message responses."""
    # Test streaming message creation
    # Test event type handling (text, content_block_stop)
    # Test stream completion
    # Test stream interruption

async def test_stream_aggregation():
    """Test streaming response aggregation."""
    # Test text delta accumulation
    # Test final message reconstruction
    # Test streaming latency under 3 seconds
```

3. **Error Handling Tests** (`tests/backend/unit/test_api_error_handling.py`):
```python
async def test_rate_limit_handling():
    """Test rate limit error handling."""
    # Test 429 rate limit detection
    # Test exponential backoff logic
    # Test retry after headers
    # Test max retry limits

async def test_api_error_recovery():
    """Test various API error scenarios."""
    # Test APIConnectionError handling
    # Test authentication errors (401)
    # Test server errors (500+)
    # Test timeout handling

async def test_provider_fallback():
    """Test fallback between API providers."""
    # Test OpenAI to Anthropic fallback
    # Test provider health-based routing
    # Test request context preservation
```

## Implementation Details

1. **AI Provider Abstraction Layer**:
```python
# app/services/ai_providers/base.py
from abc import ABC, abstractmethod
from typing import Dict, Any, AsyncGenerator, Optional
from dataclasses import dataclass
from enum import Enum

class ProviderType(Enum):
    OPENAI = "openai"
    ANTHROPIC = "anthropic"

@dataclass
class APIResponse:
    content: str
    provider: ProviderType
    model: str
    usage: Dict[str, int]
    latency: float
    request_id: Optional[str] = None

@dataclass
class StreamingChunk:
    delta: str
    chunk_type: str
    provider: ProviderType
    finish_reason: Optional[str] = None

class AIProvider(ABC):
    def __init__(self, api_key: str, model: str):
        self.api_key = api_key
        self.model = model
        self.client = None

    @abstractmethod
    async def initialize(self) -> bool:
        """Initialize the provider client."""
        pass

    @abstractmethod
    async def generate_response(
        self,
        messages: List[Dict],
        max_tokens: int = 1000,
        temperature: float = 0.7
    ) -> APIResponse:
        """Generate a non-streaming response."""
        pass

    @abstractmethod
    async def stream_response(
        self,
        messages: List[Dict],
        max_tokens: int = 1000,
        temperature: float = 0.7
    ) -> AsyncGenerator[StreamingChunk, None]:
        """Generate a streaming response."""
        pass

    @abstractmethod
    async def health_check(self) -> bool:
        """Check if provider is available."""
        pass
```

2. **OpenAI Provider Implementation**:
```python
# app/services/ai_providers/openai_provider.py
import asyncio
from openai import AsyncOpenAI
import openai
from datetime import datetime
from typing import List, Dict, Any, AsyncGenerator

class OpenAIProvider(AIProvider):
    def __init__(self, api_key: str, model: str = "gpt-4o-2024-08-06"):
        super().__init__(api_key, model)
        self.max_retries = 3
        self.timeout = 30.0

    async def initialize(self) -> bool:
        """Initialize AsyncOpenAI client with configuration."""
        try:
            self.client = AsyncOpenAI(
                api_key=self.api_key,
                max_retries=self.max_retries,
                timeout=self.timeout
            )

            # Test connection with a simple health check
            await self.health_check()
            return True

        except Exception as e:
            logger.error(f"Failed to initialize OpenAI client: {e}")
            return False

    async def generate_response(
        self,
        messages: List[Dict],
        max_tokens: int = 1000,
        temperature: float = 0.7
    ) -> APIResponse:
        """Generate a complete response from OpenAI."""
        start_time = datetime.utcnow()

        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature
            )

            latency = (datetime.utcnow() - start_time).total_seconds()

            return APIResponse(
                content=response.choices[0].message.content,
                provider=ProviderType.OPENAI,
                model=self.model,
                usage={
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                },
                latency=latency,
                request_id=getattr(response, '_request_id', None)
            )

        except openai.RateLimitError as e:
            raise RateLimitException(f"OpenAI rate limit exceeded: {e}")
        except openai.APIConnectionError as e:
            raise ConnectionException(f"OpenAI connection failed: {e}")
        except openai.APIStatusError as e:
            raise APIException(f"OpenAI API error {e.status_code}: {e.response}")

    async def stream_response(
        self,
        messages: List[Dict],
        max_tokens: int = 1000,
        temperature: float = 0.7
    ) -> AsyncGenerator[StreamingChunk, None]:
        """Stream response from OpenAI with proper event handling."""
        try:
            async with self.client.chat.completions.stream(
                model=self.model,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature
            ) as stream:
                async for event in stream:
                    if event.type == 'content.delta':
                        yield StreamingChunk(
                            delta=event.delta,
                            chunk_type='content.delta',
                            provider=ProviderType.OPENAI
                        )
                    elif event.type == 'content.done':
                        yield StreamingChunk(
                            delta="",
                            chunk_type='content.done',
                            provider=ProviderType.OPENAI,
                            finish_reason='completed'
                        )

        except openai.RateLimitError as e:
            raise RateLimitException(f"OpenAI streaming rate limit: {e}")
        except Exception as e:
            logger.error(f"OpenAI streaming error: {e}")
            raise StreamingException(f"OpenAI streaming failed: {e}")

    async def health_check(self) -> bool:
        """Check OpenAI API health."""
        try:
            # Simple test request
            await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": "test"}],
                max_tokens=1
            )
            return True
        except Exception:
            return False
```

3. **Anthropic Provider Implementation**:
```python
# app/services/ai_providers/anthropic_provider.py
import asyncio
from anthropic import AsyncAnthropic
import anthropic
from datetime import datetime
from typing import List, Dict, Any, AsyncGenerator

class AnthropicProvider(AIProvider):
    def __init__(self, api_key: str, model: str = "claude-3-5-sonnet-latest"):
        super().__init__(api_key, model)
        self.max_retries = 3
        self.timeout = 30.0

    async def initialize(self) -> bool:
        """Initialize AsyncAnthropic client with configuration."""
        try:
            self.client = AsyncAnthropic(
                api_key=self.api_key,
                max_retries=self.max_retries,
                timeout=self.timeout
            )

            # Test connection with health check
            await self.health_check()
            return True

        except Exception as e:
            logger.error(f"Failed to initialize Anthropic client: {e}")
            return False

    async def generate_response(
        self,
        messages: List[Dict],
        max_tokens: int = 1000,
        temperature: float = 0.7
    ) -> APIResponse:
        """Generate a complete response from Anthropic."""
        start_time = datetime.utcnow()

        try:
            message = await self.client.messages.create(
                model=self.model,
                max_tokens=max_tokens,
                messages=messages,
                temperature=temperature
            )

            latency = (datetime.utcnow() - start_time).total_seconds()

            # Extract text content from Anthropic response
            content = ""
            for block in message.content:
                if block.type == "text":
                    content += block.text

            return APIResponse(
                content=content,
                provider=ProviderType.ANTHROPIC,
                model=self.model,
                usage={
                    "prompt_tokens": message.usage.input_tokens,
                    "completion_tokens": message.usage.output_tokens,
                    "total_tokens": message.usage.input_tokens + message.usage.output_tokens
                },
                latency=latency,
                request_id=getattr(message, '_request_id', None)
            )

        except anthropic.RateLimitError as e:
            raise RateLimitException(f"Anthropic rate limit exceeded: {e}")
        except anthropic.APIConnectionError as e:
            raise ConnectionException(f"Anthropic connection failed: {e}")
        except anthropic.APIStatusError as e:
            raise APIException(f"Anthropic API error {e.status_code}: {e.response}")

    async def stream_response(
        self,
        messages: List[Dict],
        max_tokens: int = 1000,
        temperature: float = 0.7
    ) -> AsyncGenerator[StreamingChunk, None]:
        """Stream response from Anthropic with proper event handling."""
        try:
            async with self.client.messages.stream(
                model=self.model,
                max_tokens=max_tokens,
                messages=messages,
                temperature=temperature
            ) as stream:
                async for event in stream:
                    if event.type == "text":
                        yield StreamingChunk(
                            delta=event.text,
                            chunk_type='text',
                            provider=ProviderType.ANTHROPIC
                        )
                    elif event.type == 'content_block_stop':
                        yield StreamingChunk(
                            delta="",
                            chunk_type='content_block_stop',
                            provider=ProviderType.ANTHROPIC,
                            finish_reason='completed'
                        )

        except anthropic.RateLimitError as e:
            raise RateLimitException(f"Anthropic streaming rate limit: {e}")
        except Exception as e:
            logger.error(f"Anthropic streaming error: {e}")
            raise StreamingException(f"Anthropic streaming failed: {e}")

    async def health_check(self) -> bool:
        """Check Anthropic API health."""
        try:
            # Simple test request
            await self.client.messages.create(
                model=self.model,
                max_tokens=1,
                messages=[{"role": "user", "content": "test"}]
            )
            return True
        except Exception:
            return False
```

4. **Provider Manager with Fallback Logic**:
```python
# app/services/ai_providers/provider_manager.py
import asyncio
from typing import List, Dict, Optional, AsyncGenerator
from enum import Enum
import random

class LoadBalancingStrategy(Enum):
    ROUND_ROBIN = "round_robin"
    RANDOM = "random"
    HEALTH_BASED = "health_based"
    LATENCY_BASED = "latency_based"

class ProviderManager:
    def __init__(self):
        self.providers: Dict[ProviderType, AIProvider] = {}
        self.provider_health: Dict[ProviderType, bool] = {}
        self.provider_latencies: Dict[ProviderType, float] = {}
        self.current_provider_index = 0
        self.load_balancing = LoadBalancingStrategy.HEALTH_BASED

    async def register_provider(self, provider: AIProvider) -> bool:
        """Register and initialize a provider."""
        try:
            success = await provider.initialize()
            if success:
                provider_type = ProviderType.OPENAI if "openai" in provider.__class__.__name__.lower() else ProviderType.ANTHROPIC
                self.providers[provider_type] = provider
                self.provider_health[provider_type] = True
                self.provider_latencies[provider_type] = 0.0
                logger.info(f"Registered {provider_type.value} provider")
                return True
            return False
        except Exception as e:
            logger.error(f"Failed to register provider: {e}")
            return False

    async def get_best_provider(self) -> Optional[AIProvider]:
        """Get the best available provider based on strategy."""
        healthy_providers = [
            (ptype, provider) for ptype, provider in self.providers.items()
            if self.provider_health.get(ptype, False)
        ]

        if not healthy_providers:
            # Update health status and try again
            await self._update_provider_health()
            healthy_providers = [
                (ptype, provider) for ptype, provider in self.providers.items()
                if self.provider_health.get(ptype, False)
            ]

        if not healthy_providers:
            return None

        if self.load_balancing == LoadBalancingStrategy.RANDOM:
            return random.choice(healthy_providers)[1]
        elif self.load_balancing == LoadBalancingStrategy.LATENCY_BASED:
            # Choose provider with lowest latency
            best_provider = min(
                healthy_providers,
                key=lambda x: self.provider_latencies.get(x[0], float('inf'))
            )
            return best_provider[1]
        else:  # HEALTH_BASED (default)
            # Prefer OpenAI if available, fallback to Anthropic
            for ptype, provider in healthy_providers:
                if ptype == ProviderType.OPENAI:
                    return provider
            return healthy_providers[0][1]

    async def generate_with_fallback(
        self,
        messages: List[Dict],
        max_tokens: int = 1000,
        temperature: float = 0.7,
        max_retries: int = 2
    ) -> APIResponse:
        """Generate response with automatic fallback."""
        last_exception = None

        for attempt in range(max_retries + 1):
            provider = await self.get_best_provider()
            if not provider:
                raise AllProvidersFailedException("No healthy providers available")

            try:
                response = await provider.generate_response(
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature
                )

                # Update latency tracking
                provider_type = ProviderType.OPENAI if "openai" in provider.__class__.__name__.lower() else ProviderType.ANTHROPIC
                self.provider_latencies[provider_type] = response.latency

                return response

            except RateLimitException as e:
                logger.warning(f"Rate limit hit on {provider.__class__.__name__}: {e}")
                # Mark provider as temporarily unhealthy
                provider_type = ProviderType.OPENAI if "openai" in provider.__class__.__name__.lower() else ProviderType.ANTHROPIC
                self.provider_health[provider_type] = False
                last_exception = e

                # Wait before retry
                await asyncio.sleep(2 ** attempt)

            except (ConnectionException, APIException) as e:
                logger.error(f"Provider {provider.__class__.__name__} failed: {e}")
                # Mark provider as unhealthy
                provider_type = ProviderType.OPENAI if "openai" in provider.__class__.__name__.lower() else ProviderType.ANTHROPIC
                self.provider_health[provider_type] = False
                last_exception = e

        raise last_exception or AllProvidersFailedException("All providers failed")

    async def stream_with_fallback(
        self,
        messages: List[Dict],
        max_tokens: int = 1000,
        temperature: float = 0.7
    ) -> AsyncGenerator[StreamingChunk, None]:
        """Stream response with fallback capability."""
        provider = await self.get_best_provider()
        if not provider:
            raise AllProvidersFailedException("No healthy providers available")

        try:
            async for chunk in provider.stream_response(
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature
            ):
                yield chunk

        except (RateLimitException, ConnectionException, StreamingException) as e:
            logger.error(f"Streaming failed on {provider.__class__.__name__}: {e}")
            # Mark provider as unhealthy
            provider_type = ProviderType.OPENAI if "openai" in provider.__class__.__name__.lower() else ProviderType.ANTHROPIC
            self.provider_health[provider_type] = False

            # Try fallback provider
            fallback_provider = await self.get_best_provider()
            if fallback_provider and fallback_provider != provider:
                logger.info("Attempting fallback provider for streaming")
                async for chunk in fallback_provider.stream_response(
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature
                ):
                    yield chunk
            else:
                raise e

    async def _update_provider_health(self):
        """Update health status of all providers."""
        for provider_type, provider in self.providers.items():
            try:
                is_healthy = await provider.health_check()
                self.provider_health[provider_type] = is_healthy
            except Exception:
                self.provider_health[provider_type] = False
```

5. **Custom Exception Classes**:
```python
# app/services/ai_providers/exceptions.py
class AIProviderException(Exception):
    """Base exception for AI provider errors."""
    pass

class RateLimitException(AIProviderException):
    """Raised when API rate limit is exceeded."""
    pass

class ConnectionException(AIProviderException):
    """Raised when connection to API fails."""
    pass

class APIException(AIProviderException):
    """Raised when API returns an error status."""
    pass

class StreamingException(AIProviderException):
    """Raised when streaming operation fails."""
    pass

class AllProvidersFailedException(AIProviderException):
    """Raised when all providers are unavailable."""
    pass
```

6. **API Integration Service**:
```python
# app/api/v1/ai_providers.py
from fastapi import APIRouter, Depends, HTTPException
from app.services.ai_providers.provider_manager import ProviderManager

router = APIRouter(prefix="/ai", tags=["ai"])

@router.post("/generate")
async def generate_response(
    messages: List[Dict],
    max_tokens: int = 1000,
    temperature: float = 0.7,
    provider_manager: ProviderManager = Depends(get_provider_manager)
):
    """Generate AI response with automatic provider fallback."""
    try:
        response = await provider_manager.generate_with_fallback(
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature
        )

        return {
            "content": response.content,
            "provider": response.provider.value,
            "model": response.model,
            "usage": response.usage,
            "latency": response.latency
        }

    except AllProvidersFailedException as e:
        raise HTTPException(status_code=503, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"AI generation failed: {e}")

@router.post("/stream")
async def stream_response(
    messages: List[Dict],
    max_tokens: int = 1000,
    temperature: float = 0.7,
    provider_manager: ProviderManager = Depends(get_provider_manager)
):
    """Stream AI response with real-time updates."""
    async def generate():
        try:
            async for chunk in provider_manager.stream_with_fallback(
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature
            ):
                yield f"data: {json.dumps({
                    'delta': chunk.delta,
                    'type': chunk.chunk_type,
                    'provider': chunk.provider.value,
                    'finish_reason': chunk.finish_reason
                })}\n\n"

        except Exception as e:
            yield f"data: {json.dumps({'error': str(e)})}\n\n"

    return StreamingResponse(generate(), media_type="text/plain")

@router.get("/providers/health")
async def get_provider_health(
    provider_manager: ProviderManager = Depends(get_provider_manager)
):
    """Get health status of all AI providers."""
    return {
        "providers": {
            ptype.value: {
                "healthy": provider_manager.provider_health.get(ptype, False),
                "latency": provider_manager.provider_latencies.get(ptype, 0.0)
            }
            for ptype in provider_manager.providers.keys()
        }
    }
```

## Dependencies
- Task 020: Multi-Agent Coordination Framework
- Task 005: FastAPI Core Application
- Task 004: Redis Setup (for response caching)
- Task 012: Analytics Foundation (for usage tracking)

## Estimated Time
20-24 hours

## Required Skills
- OpenAI and Anthropic API expertise
- Streaming response handling
- Asynchronous Python programming
- Error handling and retry logic
- Load balancing and fallback strategies
- Rate limiting and quota management

## Notes
- Start with basic API integration, then add streaming and fallback
- Implement comprehensive monitoring for API usage and costs
- Consider implementing response caching for frequently asked questions
- Monitor API latencies and adjust timeouts accordingly
- Plan for quota management and cost optimization strategies
