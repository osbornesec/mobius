# Task 022: Context-Aware Prompt Engineering

## Overview
Implement an intelligent prompt engineering system that dynamically constructs context-aware prompts with few-shot examples, chain-of-thought reasoning, and adaptive prompt optimization for different AI coding tasks.

## Success Criteria
- [ ] Dynamic prompt templates adapt to different coding tasks
- [ ] Few-shot example selection based on task similarity
- [ ] Chain-of-thought reasoning integration for complex problems
- [ ] Context window optimization for different model limits
- [ ] Prompt effectiveness tracking and continuous improvement
- [ ] A/B testing framework for prompt variations
- [ ] Role-based prompting for specialized coding tasks
- [ ] 90% task completion accuracy with optimized prompts

## Test First Approach

### Tests to Write BEFORE Implementation:

1. **Prompt Template Tests** (`tests/backend/unit/test_prompt_templates.py`):
```python
import pytest
from app.services.prompt_engine import PromptEngine, PromptTemplate

async def test_dynamic_template_generation():
    """Test dynamic prompt template creation for different tasks."""
    # Test code generation template
    # Test debugging template
    # Test explanation template
    # Test refactoring template

async def test_context_window_optimization():
    """Test prompt length optimization for different models."""
    # Test prompt truncation while preserving important context
    # Test context prioritization
    # Test token counting accuracy
    # Test model-specific optimizations (GPT-4 vs Claude)

async def test_role_prompting():
    """Test role-based prompt engineering."""
    # Test senior developer role
    # Test code reviewer role
    # Test architecture consultant role
    # Test debugging specialist role
```

2. **Few-Shot Learning Tests** (`tests/backend/unit/test_few_shot.py`):
```python
async def test_example_selection():
    """Test intelligent selection of few-shot examples."""
    # Test similarity-based example selection
    # Test diversity in example selection
    # Test quality scoring of examples
    # Test example relevance to current task

async def test_few_shot_formatting():
    """Test consistent formatting of few-shot examples."""
    # Test input-output format consistency
    # Test example concatenation
    # Test example ordering by relevance
    # Test removal of irrelevant examples

async def test_example_database():
    """Test few-shot example storage and retrieval."""
    # Test example categorization
    # Test example search and filtering
    # Test example quality metrics
    # Test example versioning
```

3. **Chain-of-Thought Tests** (`tests/backend/unit/test_chain_of_thought.py`):
```python
async def test_cot_integration():
    """Test chain-of-thought reasoning integration."""
    # Test step-by-step reasoning prompts
    # Test intermediate step validation
    # Test reasoning path optimization
    # Test CoT for complex debugging tasks

async def test_reasoning_patterns():
    """Test different reasoning patterns."""
    # Test analytical reasoning for code review
    # Test sequential reasoning for implementation
    # Test diagnostic reasoning for debugging
    # Test creative reasoning for architecture
```

## Implementation Details

1. **Prompt Template Engine**:
```python
# app/services/prompt_engine/prompt_engine.py
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from enum import Enum
import json
from datetime import datetime

class TaskType(Enum):
    CODE_GENERATION = "code_generation"
    DEBUG_ANALYSIS = "debug_analysis"
    CODE_EXPLANATION = "code_explanation"
    CODE_REVIEW = "code_review"
    REFACTORING = "refactoring"
    ARCHITECTURE_DESIGN = "architecture_design"
    TEST_GENERATION = "test_generation"

class ReasoningMode(Enum):
    DIRECT = "direct"
    CHAIN_OF_THOUGHT = "chain_of_thought"
    STEP_BY_STEP = "step_by_step"
    ANALYTICAL = "analytical"

@dataclass
class PromptContext:
    task_type: TaskType
    user_request: str
    code_context: Optional[str] = None
    file_context: List[str] = None
    error_context: Optional[str] = None
    user_level: str = "intermediate"  # beginner, intermediate, expert
    preferred_style: str = "concise"  # verbose, concise, detailed
    language: str = "python"

@dataclass
class FewShotExample:
    task_type: TaskType
    input: str
    output: str
    reasoning: Optional[str] = None
    quality_score: float = 0.0
    tags: List[str] = None
    created_at: datetime = None

@dataclass
class PromptTemplate:
    template_id: str
    task_type: TaskType
    system_prompt: str
    user_prompt_template: str
    few_shot_count: int = 3
    use_cot: bool = False
    role_description: str = ""
    context_sections: List[str] = None

class PromptEngine:
    def __init__(self, embedding_service, vector_store):
        self.embedding_service = embedding_service
        self.vector_store = vector_store
        self.templates = self._load_templates()
        self.few_shot_examples = {}

    async def generate_prompt(
        self,
        context: PromptContext,
        max_tokens: int = 8000,
        model_type: str = "gpt-4"
    ) -> Dict[str, Any]:
        """Generate optimized prompt for given context."""

        # Select appropriate template
        template = self._select_template(context)

        # Build system prompt with role
        system_prompt = self._build_system_prompt(template, context)

        # Select few-shot examples
        examples = await self._select_few_shot_examples(context, template.few_shot_count)

        # Build user prompt
        user_prompt = self._build_user_prompt(template, context, examples)

        # Optimize for token limits
        optimized_prompt = self._optimize_prompt_length(
            system_prompt, user_prompt, max_tokens, model_type
        )

        return {
            "system": optimized_prompt["system"],
            "user": optimized_prompt["user"],
            "template_id": template.template_id,
            "examples_used": len(examples),
            "reasoning_mode": ReasoningMode.CHAIN_OF_THOUGHT if template.use_cot else ReasoningMode.DIRECT,
            "estimated_tokens": optimized_prompt["token_count"]
        }

    def _select_template(self, context: PromptContext) -> PromptTemplate:
        """Select the most appropriate template for the task."""
        task_templates = [t for t in self.templates if t.task_type == context.task_type]

        if not task_templates:
            # Fallback to general template
            return self._get_default_template(context.task_type)

        # For now, return the first matching template
        # TODO: Add template scoring based on context
        return task_templates[0]

    def _build_system_prompt(self, template: PromptTemplate, context: PromptContext) -> str:
        """Build system prompt with role and context."""
        system_parts = []

        # Add role description
        if template.role_description:
            system_parts.append(template.role_description)

        # Add base system prompt
        system_parts.append(template.system_prompt)

        # Add context-specific instructions
        if context.user_level == "beginner":
            system_parts.append(
                "Provide clear explanations and avoid complex jargon. "
                "Include step-by-step reasoning when helpful."
            )
        elif context.user_level == "expert":
            system_parts.append(
                "Assume advanced knowledge. Focus on efficiency and best practices. "
                "Provide concise, technical responses."
            )

        # Add language-specific instructions
        language_instructions = self._get_language_instructions(context.language)
        if language_instructions:
            system_parts.append(language_instructions)

        return "\n\n".join(system_parts)

    def _build_user_prompt(
        self,
        template: PromptTemplate,
        context: PromptContext,
        examples: List[FewShotExample]
    ) -> str:
        """Build user prompt with few-shot examples and context."""
        prompt_parts = []

        # Add few-shot examples
        if examples:
            prompt_parts.append("Here are some examples of similar tasks:")
            for i, example in enumerate(examples, 1):
                example_text = f"\nExample {i}:\nInput: {example.input}\nOutput: {example.output}"
                if template.use_cot and example.reasoning:
                    example_text += f"\nReasoning: {example.reasoning}"
                prompt_parts.append(example_text)
            prompt_parts.append("\nNow, please help with the following task:")

        # Add context sections
        if context.code_context:
            prompt_parts.append(f"Code Context:\n```{context.language}\n{context.code_context}\n```")

        if context.file_context:
            prompt_parts.append("Related Files:")
            for file_path in context.file_context[:5]:  # Limit to 5 files
                prompt_parts.append(f"- {file_path}")

        if context.error_context:
            prompt_parts.append(f"Error Context:\n{context.error_context}")

        # Add main request
        formatted_request = template.user_prompt_template.format(
            user_request=context.user_request,
            language=context.language
        )
        prompt_parts.append(formatted_request)

        # Add chain-of-thought trigger if enabled
        if template.use_cot:
            prompt_parts.append("Let's think through this step by step:")

        return "\n\n".join(prompt_parts)

    async def _select_few_shot_examples(
        self,
        context: PromptContext,
        count: int
    ) -> List[FewShotExample]:
        """Select most relevant few-shot examples."""
        if count == 0:
            return []

        # Get examples for task type
        task_examples = self.few_shot_examples.get(context.task_type, [])
        if not task_examples:
            return []

        # Create embedding for current request
        request_embedding = await self.embedding_service.embed_text(context.user_request)

        # Score examples by similarity
        scored_examples = []
        for example in task_examples:
            # Calculate similarity score
            example_embedding = await self.embedding_service.embed_text(example.input)
            similarity = self._calculate_similarity(request_embedding, example_embedding)

            # Combine with quality score
            combined_score = (similarity * 0.7) + (example.quality_score * 0.3)
            scored_examples.append((example, combined_score))

        # Sort by score and return top examples
        scored_examples.sort(key=lambda x: x[1], reverse=True)
        return [example for example, score in scored_examples[:count]]

    def _optimize_prompt_length(
        self,
        system_prompt: str,
        user_prompt: str,
        max_tokens: int,
        model_type: str
    ) -> Dict[str, Any]:
        """Optimize prompt length for token limits."""
        # Estimate tokens (rough approximation: 4 chars per token)
        system_tokens = len(system_prompt) // 4
        user_tokens = len(user_prompt) // 4
        total_tokens = system_tokens + user_tokens

        # Reserve tokens for response (typically 20-30% of context)
        response_buffer = max_tokens * 0.25
        available_tokens = max_tokens - response_buffer

        if total_tokens <= available_tokens:
            return {
                "system": system_prompt,
                "user": user_prompt,
                "token_count": total_tokens
            }

        # Need to truncate - prioritize user prompt over system
        if system_tokens < available_tokens * 0.3:
            # System prompt is reasonable, truncate user prompt
            target_user_tokens = available_tokens - system_tokens
            truncated_user = self._truncate_text(user_prompt, target_user_tokens * 4)
            return {
                "system": system_prompt,
                "user": truncated_user,
                "token_count": system_tokens + len(truncated_user) // 4
            }
        else:
            # Both prompts need truncation
            target_system_tokens = available_tokens * 0.3
            target_user_tokens = available_tokens * 0.7

            truncated_system = self._truncate_text(system_prompt, target_system_tokens * 4)
            truncated_user = self._truncate_text(user_prompt, target_user_tokens * 4)

            return {
                "system": truncated_system,
                "user": truncated_user,
                "token_count": (len(truncated_system) + len(truncated_user)) // 4
            }

    def _truncate_text(self, text: str, max_chars: int) -> str:
        """Intelligently truncate text while preserving structure."""
        if len(text) <= max_chars:
            return text

        # Try to truncate at natural boundaries
        sections = text.split("\n\n")
        truncated_sections = []
        current_length = 0

        for section in sections:
            if current_length + len(section) + 2 <= max_chars:
                truncated_sections.append(section)
                current_length += len(section) + 2
            else:
                # Add partial section if there's room
                remaining_chars = max_chars - current_length - 10  # Leave room for "..."
                if remaining_chars > 50:
                    partial = section[:remaining_chars] + "..."
                    truncated_sections.append(partial)
                break

        return "\n\n".join(truncated_sections)
```

2. **Template Storage and Management**:
```python
# app/services/prompt_engine/template_manager.py
class TemplateManager:
    def __init__(self):
        self.templates = self._initialize_templates()

    def _initialize_templates(self) -> List[PromptTemplate]:
        """Initialize default prompt templates."""
        return [
            # Code Generation Template
            PromptTemplate(
                template_id="code_gen_python",
                task_type=TaskType.CODE_GENERATION,
                system_prompt="""You are an expert Python developer with deep knowledge of best practices,
                design patterns, and modern Python features. Your code is clean, efficient, and well-documented.""",
                user_prompt_template="""Please generate Python code for the following request:

{user_request}

Requirements:
- Follow PEP 8 style guidelines
- Include appropriate type hints
- Add docstrings for functions and classes
- Handle edge cases and errors appropriately
- Write clean, readable, and maintainable code""",
                few_shot_count=2,
                use_cot=True,
                role_description="You are a senior Python developer and code architect.",
                context_sections=["code_context", "file_context"]
            ),

            # Debug Analysis Template
            PromptTemplate(
                template_id="debug_analysis",
                task_type=TaskType.DEBUG_ANALYSIS,
                system_prompt="""You are an expert debugging specialist with extensive experience
                identifying and resolving complex software issues. You approach problems systematically
                and provide clear, actionable solutions.""",
                user_prompt_template="""Please analyze the following debugging request:

{user_request}

Please provide:
1. Root cause analysis
2. Step-by-step debugging approach
3. Potential solutions
4. Prevention strategies""",
                few_shot_count=3,
                use_cot=True,
                role_description="You are a senior debugging specialist and troubleshooter."
            ),

            # Code Review Template
            PromptTemplate(
                template_id="code_review",
                task_type=TaskType.CODE_REVIEW,
                system_prompt="""You are an experienced code reviewer who focuses on code quality,
                maintainability, security, and performance. You provide constructive feedback
                with specific suggestions for improvement.""",
                user_prompt_template="""Please review the following code:

{user_request}

Focus on:
- Code quality and readability
- Performance optimizations
- Security considerations
- Best practices adherence
- Potential bugs or issues
- Suggestions for improvement""",
                few_shot_count=2,
                use_cot=False,
                role_description="You are a senior code reviewer and technical lead."
            ),

            # Architecture Design Template
            PromptTemplate(
                template_id="architecture_design",
                task_type=TaskType.ARCHITECTURE_DESIGN,
                system_prompt="""You are a software architect with expertise in designing scalable,
                maintainable systems. You consider trade-offs, patterns, and long-term implications
                of architectural decisions.""",
                user_prompt_template="""Please provide architectural guidance for:

{user_request}

Consider:
- System requirements and constraints
- Scalability and performance
- Maintainability and extensibility
- Technology choices and trade-offs
- Implementation strategy""",
                few_shot_count=1,
                use_cot=True,
                role_description="You are a senior software architect and system designer."
            )
        ]
```

3. **Few-Shot Example Management**:
```python
# app/services/prompt_engine/few_shot_manager.py
class FewShotManager:
    def __init__(self, db_session, embedding_service):
        self.db_session = db_session
        self.embedding_service = embedding_service

    async def add_example(self, example: FewShotExample) -> bool:
        """Add a new few-shot example to the database."""
        try:
            # Generate embedding for the example
            embedding = await self.embedding_service.embed_text(example.input)

            # Store in database
            db_example = FewShotExampleModel(
                task_type=example.task_type.value,
                input_text=example.input,
                output_text=example.output,
                reasoning=example.reasoning,
                quality_score=example.quality_score,
                tags=json.dumps(example.tags) if example.tags else None,
                embedding=embedding
            )

            self.db_session.add(db_example)
            await self.db_session.commit()
            return True

        except Exception as e:
            logger.error(f"Failed to add few-shot example: {e}")
            return False

    async def get_examples_by_similarity(
        self,
        query_text: str,
        task_type: TaskType,
        limit: int = 10
    ) -> List[FewShotExample]:
        """Get examples similar to query text."""
        try:
            # Generate embedding for query
            query_embedding = await self.embedding_service.embed_text(query_text)

            # Search for similar examples
            # This would use vector similarity search in the database
            similar_examples = await self._vector_search(
                query_embedding, task_type, limit
            )

            return similar_examples

        except Exception as e:
            logger.error(f"Failed to get similar examples: {e}")
            return []

    async def rate_example_quality(
        self,
        example_id: str,
        rating: float,
        feedback: str = ""
    ) -> bool:
        """Rate the quality of a few-shot example."""
        try:
            # Update example quality score
            await self.db_session.execute(
                update(FewShotExampleModel)
                .where(FewShotExampleModel.id == example_id)
                .values(quality_score=rating)
            )

            # Store feedback if provided
            if feedback:
                feedback_record = ExampleFeedback(
                    example_id=example_id,
                    rating=rating,
                    feedback=feedback,
                    created_at=datetime.utcnow()
                )
                self.db_session.add(feedback_record)

            await self.db_session.commit()
            return True

        except Exception as e:
            logger.error(f"Failed to rate example: {e}")
            return False
```

4. **Prompt Optimization and A/B Testing**:
```python
# app/services/prompt_engine/prompt_optimizer.py
class PromptOptimizer:
    def __init__(self, analytics_service):
        self.analytics_service = analytics_service
        self.active_experiments = {}

    async def create_ab_test(
        self,
        template_a: PromptTemplate,
        template_b: PromptTemplate,
        test_name: str,
        traffic_split: float = 0.5
    ) -> str:
        """Create an A/B test for prompt templates."""
        experiment = {
            "id": self._generate_experiment_id(),
            "name": test_name,
            "template_a": template_a,
            "template_b": template_b,
            "traffic_split": traffic_split,
            "start_time": datetime.utcnow(),
            "metrics": {"a": [], "b": []}
        }

        self.active_experiments[experiment["id"]] = experiment
        return experiment["id"]

    async def get_template_for_experiment(
        self,
        experiment_id: str,
        user_id: str
    ) -> PromptTemplate:
        """Get template for user in A/B test."""
        if experiment_id not in self.active_experiments:
            raise ValueError(f"Experiment {experiment_id} not found")

        experiment = self.active_experiments[experiment_id]

        # Deterministic assignment based on user_id
        import hashlib
        user_hash = hashlib.md5(user_id.encode()).hexdigest()
        hash_value = int(user_hash[:8], 16) / (16**8)

        if hash_value < experiment["traffic_split"]:
            return experiment["template_a"]
        else:
            return experiment["template_b"]

    async def record_experiment_result(
        self,
        experiment_id: str,
        template_used: str,  # "a" or "b"
        success: bool,
        latency: float,
        user_satisfaction: Optional[float] = None
    ):
        """Record result of experiment."""
        if experiment_id not in self.active_experiments:
            return

        experiment = self.active_experiments[experiment_id]
        result = {
            "success": success,
            "latency": latency,
            "user_satisfaction": user_satisfaction,
            "timestamp": datetime.utcnow()
        }

        experiment["metrics"][template_used].append(result)

    async def analyze_experiment_results(self, experiment_id: str) -> Dict[str, Any]:
        """Analyze A/B test results."""
        if experiment_id not in self.active_experiments:
            raise ValueError(f"Experiment {experiment_id} not found")

        experiment = self.active_experiments[experiment_id]
        metrics_a = experiment["metrics"]["a"]
        metrics_b = experiment["metrics"]["b"]

        if not metrics_a or not metrics_b:
            return {"status": "insufficient_data"}

        # Calculate success rates
        success_rate_a = sum(1 for m in metrics_a if m["success"]) / len(metrics_a)
        success_rate_b = sum(1 for m in metrics_b if m["success"]) / len(metrics_b)

        # Calculate average latencies
        avg_latency_a = sum(m["latency"] for m in metrics_a) / len(metrics_a)
        avg_latency_b = sum(m["latency"] for m in metrics_b) / len(metrics_b)

        # Calculate satisfaction scores
        satisfaction_a = [m["user_satisfaction"] for m in metrics_a if m["user_satisfaction"]]
        satisfaction_b = [m["user_satisfaction"] for m in metrics_b if m["user_satisfaction"]]

        avg_satisfaction_a = sum(satisfaction_a) / len(satisfaction_a) if satisfaction_a else None
        avg_satisfaction_b = sum(satisfaction_b) / len(satisfaction_b) if satisfaction_b else None

        return {
            "status": "complete",
            "template_a": {
                "success_rate": success_rate_a,
                "avg_latency": avg_latency_a,
                "avg_satisfaction": avg_satisfaction_a,
                "sample_size": len(metrics_a)
            },
            "template_b": {
                "success_rate": success_rate_b,
                "avg_latency": avg_latency_b,
                "avg_satisfaction": avg_satisfaction_b,
                "sample_size": len(metrics_b)
            },
            "winner": "a" if success_rate_a > success_rate_b else "b",
            "confidence": self._calculate_statistical_significance(metrics_a, metrics_b)
        }
```

5. **API Integration**:
```python
# app/api/v1/prompt_engine.py
from fastapi import APIRouter, Depends, HTTPException
from app.services.prompt_engine import PromptEngine, PromptContext, TaskType

router = APIRouter(prefix="/prompts", tags=["prompts"])

@router.post("/generate")
async def generate_prompt(
    context: PromptContext,
    max_tokens: int = 8000,
    model_type: str = "gpt-4",
    prompt_engine: PromptEngine = Depends(get_prompt_engine)
):
    """Generate optimized prompt for given context."""
    try:
        prompt = await prompt_engine.generate_prompt(
            context=context,
            max_tokens=max_tokens,
            model_type=model_type
        )

        return {
            "prompt": prompt,
            "optimization_applied": prompt["estimated_tokens"] < max_tokens,
            "template_used": prompt["template_id"]
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Prompt generation failed: {e}")

@router.post("/examples")
async def add_few_shot_example(
    example: FewShotExample,
    few_shot_manager: FewShotManager = Depends(get_few_shot_manager)
):
    """Add a new few-shot example."""
    success = await few_shot_manager.add_example(example)
    if not success:
        raise HTTPException(status_code=500, detail="Failed to add example")

    return {"status": "example_added"}

@router.get("/experiments/{experiment_id}/results")
async def get_experiment_results(
    experiment_id: str,
    optimizer: PromptOptimizer = Depends(get_prompt_optimizer)
):
    """Get A/B test results."""
    try:
        results = await optimizer.analyze_experiment_results(experiment_id)
        return results
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
```

## Dependencies
- Task 021: OpenAI/Anthropic API Integration
- Task 020: Multi-Agent Coordination Framework
- Task 018: Context Ranking and Relevance Scoring
- Task 006: Vector Embedding Generation

## Estimated Time
18-22 hours

## Required Skills
- Prompt engineering and optimization techniques
- Few-shot learning and in-context learning
- Chain-of-thought reasoning implementation
- Natural language processing and text similarity
- A/B testing and statistical analysis
- Machine learning for prompt optimization

## Notes
- Start with basic template system, then add few-shot learning
- Implement comprehensive tracking for prompt effectiveness
- Consider implementing reinforcement learning for continuous optimization
- Monitor token usage costs and optimize for efficiency
- Build feedback loops to continuously improve prompt quality
