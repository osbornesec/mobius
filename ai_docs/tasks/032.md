# Task 032: Context Builder Agent Implementation

## Overview
Implement the Context Builder Agent, a specialized AI agent responsible for intelligent context aggregation, analysis, and optimization. This agent will analyze code context, identify relevant information, and build optimized context packages for other agents and AI models.

## Success Criteria
- [ ] Agent can analyze code files and extract semantic context
- [ ] Context relevance scoring achieves >90% accuracy on test queries
- [ ] Agent can build multi-file context packages efficiently
- [ ] Context optimization reduces token usage by >30% while maintaining quality
- [ ] Agent integrates seamlessly with the multi-agent framework
- [ ] Performance meets <200ms context building time for typical requests

## Test First Approach

### Tests to Write BEFORE Implementation:

1. **Context Analysis Tests** (`tests/backend/unit/test_context_builder.py`):
```python
def test_semantic_context_extraction():
    """Test extraction of semantic context from code files."""
    # Test function signature extraction
    # Test class hierarchy analysis
    # Test import dependency mapping
    # Test documentation extraction
    # Test variable and type inference

def test_relevance_scoring():
    """Test context relevance scoring accuracy."""
    # Test query-context relevance calculation
    # Test semantic similarity scoring
    # Test context ranking algorithms
    # Test relevance threshold optimization
    # Test multi-dimensional relevance factors

def test_context_aggregation():
    """Test multi-file context aggregation."""
    # Test related file discovery
    # Test context merging strategies
    # Test duplicate content detection
    # Test context size optimization
    # Test hierarchical context building
```

2. **Context Optimization Tests** (`tests/backend/unit/test_context_optimization.py`):
```python
def test_token_optimization():
    """Test context token usage optimization."""
    # Test redundancy elimination
    # Test content summarization
    # Test priority-based pruning
    # Test token budget management
    # Test quality preservation metrics

def test_context_packaging():
    """Test context package generation."""
    # Test structured context output
    # Test format consistency
    # Test metadata inclusion
    # Test context versioning
    # Test incremental updates

def test_performance_benchmarks():
    """Test context building performance."""
    # Test <200ms context building time
    # Test memory usage optimization
    # Test concurrent request handling
    # Test caching effectiveness
    # Test scalability under load
```

3. **Agent Integration Tests** (`tests/backend/integration/test_context_agent_integration.py`):
```python
def test_agent_communication():
    """Test communication with other agents."""
    # Test message handling protocols
    # Test request-response patterns
    # Test async communication
    # Test error handling and recovery
    # Test coordination with retrieval agents

def test_framework_integration():
    """Test integration with agent framework."""
    # Test agent registration
    # Test capability advertisement
    # Test health check responses
    # Test graceful shutdown
    # Test metrics reporting
```

## Implementation Details

1. **Context Builder Agent** (`app/agents/context_builder.py`):
```python
from typing import Dict, Any, List, Optional, Set
from dataclasses import dataclass
import asyncio
import ast
import re
from datetime import datetime

from app.agents.base_agent import BaseAgent, AgentCapability
from app.processing.parsers.base import BaseParser
from app.processing.embedders.base import BaseEmbedder
from app.models.domain.context import ContextItem, ContextPackage
from app.utils.semantic_analyzer import SemanticAnalyzer

@dataclass
class ContextBuildRequest:
    query: str
    file_paths: List[str]
    context_type: str  # "function", "class", "module", "project"
    max_tokens: Optional[int] = None
    include_dependencies: bool = True
    optimization_level: str = "balanced"  # "speed", "balanced", "quality"

@dataclass
class ContextBuildResult:
    context_package: ContextPackage
    relevance_scores: Dict[str, float]
    token_count: int
    build_time: float
    metadata: Dict[str, Any]

class ContextBuilderAgent(BaseAgent):
    def __init__(self):
        capabilities = [
            AgentCapability("context_building", "1.0", 10, {"supports_optimization": True}),
            AgentCapability("semantic_analysis", "1.0", 8, {"supports_multilang": True}),
            AgentCapability("code_analysis", "1.0", 9, {"supports_ast": True})
        ]
        super().__init__("context_builder", "context_builder", capabilities)

        self.semantic_analyzer = SemanticAnalyzer()
        self.parsers: Dict[str, BaseParser] = {}
        self.embedder: Optional[BaseEmbedder] = None
        self.context_cache: Dict[str, ContextPackage] = {}

    async def on_start(self):
        """Initialize the context builder agent."""
        # Initialize parsers for different file types
        await self._initialize_parsers()

        # Initialize embedder for semantic analysis
        await self._initialize_embedder()

        # Load pre-trained models for optimization
        await self._load_optimization_models()

    async def handle_message(self, message) -> Optional[Any]:
        """Handle incoming context building requests."""
        if message.payload.get("action") == "build_context":
            request = ContextBuildRequest(**message.payload.get("request", {}))
            result = await self.build_context(request)
            return result.__dict__
        elif message.payload.get("action") == "analyze_relevance":
            return await self.analyze_relevance(
                message.payload.get("query"),
                message.payload.get("content")
            )
        elif message.payload.get("action") == "optimize_context":
            return await self.optimize_context(
                message.payload.get("context"),
                message.payload.get("target_tokens")
            )
        else:
            return {"error": "Unknown action"}

    async def build_context(self, request: ContextBuildRequest) -> ContextBuildResult:
        """Build optimized context from multiple sources."""
        start_time = datetime.utcnow()

        try:
            # 1. Parse and analyze individual files
            file_contexts = await self._parse_files(request.file_paths)

            # 2. Extract semantic information
            semantic_info = await self._extract_semantic_info(file_contexts, request.query)

            # 3. Calculate relevance scores
            relevance_scores = await self._calculate_relevance_scores(
                semantic_info, request.query
            )

            # 4. Build dependency graph if requested
            if request.include_dependencies:
                dependency_graph = await self._build_dependency_graph(file_contexts)
                # Add dependent files based on relevance
                additional_contexts = await self._get_relevant_dependencies(
                    dependency_graph, relevance_scores
                )
                file_contexts.extend(additional_contexts)

            # 5. Aggregate and optimize context
            context_package = await self._aggregate_contexts(
                file_contexts, relevance_scores, request
            )

            # 6. Apply optimization based on level
            if request.max_tokens:
                context_package = await self._optimize_for_tokens(
                    context_package, request.max_tokens, request.optimization_level
                )

            # 7. Calculate final metrics
            token_count = await self._calculate_token_count(context_package)
            build_time = (datetime.utcnow() - start_time).total_seconds()

            return ContextBuildResult(
                context_package=context_package,
                relevance_scores=relevance_scores,
                token_count=token_count,
                build_time=build_time,
                metadata={
                    "optimization_level": request.optimization_level,
                    "files_processed": len(request.file_paths),
                    "dependencies_included": request.include_dependencies
                }
            )

        except Exception as e:
            self.logger.error(f"Context building failed: {str(e)}")
            raise

    async def _parse_files(self, file_paths: List[str]) -> List[ContextItem]:
        """Parse files and extract structured information."""
        contexts = []

        for file_path in file_paths:
            try:
                # Determine file type and select appropriate parser
                parser = await self._get_parser_for_file(file_path)

                # Parse file content
                parsed_content = await parser.parse_file(file_path)

                # Create context item
                context_item = ContextItem(
                    file_path=file_path,
                    content_type=parser.content_type,
                    structured_content=parsed_content,
                    metadata={"parser": parser.__class__.__name__}
                )

                contexts.append(context_item)

            except Exception as e:
                self.logger.warning(f"Failed to parse {file_path}: {str(e)}")
                continue

        return contexts

    async def _extract_semantic_info(self, contexts: List[ContextItem], query: str) -> Dict[str, Any]:
        """Extract semantic information from contexts."""
        semantic_info = {
            "query_embedding": None,
            "content_embeddings": {},
            "ast_analysis": {},
            "symbol_tables": {},
            "call_graphs": {}
        }

        # Generate query embedding
        if self.embedder:
            semantic_info["query_embedding"] = await self.embedder.embed_text(query)

        for context in contexts:
            file_path = context.file_path

            # Generate content embeddings
            if self.embedder and context.structured_content:
                content_text = await self._extract_text_for_embedding(context)
                semantic_info["content_embeddings"][file_path] = await self.embedder.embed_text(content_text)

            # AST analysis for code files
            if context.content_type in ["python", "javascript", "typescript"]:
                semantic_info["ast_analysis"][file_path] = await self._analyze_ast(context)

            # Symbol table extraction
            semantic_info["symbol_tables"][file_path] = await self._extract_symbols(context)

            # Call graph analysis
            semantic_info["call_graphs"][file_path] = await self._analyze_call_graph(context)

        return semantic_info

    async def _calculate_relevance_scores(self, semantic_info: Dict[str, Any], query: str) -> Dict[str, float]:
        """Calculate relevance scores for each file."""
        scores = {}
        query_embedding = semantic_info.get("query_embedding")

        for file_path, content_embedding in semantic_info.get("content_embeddings", {}).items():
            score = 0.0

            # Semantic similarity score (40% weight)
            if query_embedding is not None and content_embedding is not None:
                semantic_score = await self._calculate_cosine_similarity(query_embedding, content_embedding)
                score += semantic_score * 0.4

            # Keyword matching score (20% weight)
            keyword_score = await self._calculate_keyword_similarity(query, file_path, semantic_info)
            score += keyword_score * 0.2

            # Symbol relevance score (25% weight)
            symbol_score = await self._calculate_symbol_relevance(query, file_path, semantic_info)
            score += symbol_score * 0.25

            # Structural importance score (15% weight)
            structural_score = await self._calculate_structural_importance(file_path, semantic_info)
            score += structural_score * 0.15

            scores[file_path] = min(score, 1.0)  # Cap at 1.0

        return scores

    async def _optimize_for_tokens(self, context_package: ContextPackage, max_tokens: int, level: str) -> ContextPackage:
        """Optimize context package to fit within token budget."""
        current_tokens = await self._calculate_token_count(context_package)

        if current_tokens <= max_tokens:
            return context_package

        if level == "speed":
            # Fast optimization: simple truncation
            return await self._truncate_context(context_package, max_tokens)
        elif level == "balanced":
            # Balanced optimization: smart pruning
            return await self._smart_prune_context(context_package, max_tokens)
        elif level == "quality":
            # Quality optimization: intelligent summarization
            return await self._summarize_context(context_package, max_tokens)
        else:
            return context_package

    async def _smart_prune_context(self, context_package: ContextPackage, max_tokens: int) -> ContextPackage:
        """Intelligently prune context while preserving quality."""
        # Sort contexts by relevance score
        sorted_contexts = sorted(
            context_package.contexts,
            key=lambda x: x.metadata.get("relevance_score", 0.0),
            reverse=True
        )

        optimized_contexts = []
        current_tokens = 0

        for context in sorted_contexts:
            context_tokens = await self._calculate_context_tokens(context)

            if current_tokens + context_tokens <= max_tokens:
                optimized_contexts.append(context)
                current_tokens += context_tokens
            else:
                # Try to include partial content
                remaining_tokens = max_tokens - current_tokens
                if remaining_tokens > 100:  # Minimum viable context
                    pruned_context = await self._prune_context_content(context, remaining_tokens)
                    if pruned_context:
                        optimized_contexts.append(pruned_context)
                break

        return ContextPackage(
            contexts=optimized_contexts,
            metadata={
                **context_package.metadata,
                "optimization": "smart_pruned",
                "original_token_count": await self._calculate_token_count(context_package),
                "optimized_token_count": current_tokens
            }
        )

    async def analyze_relevance(self, query: str, content: str) -> Dict[str, float]:
        """Analyze relevance between query and content."""
        if not self.embedder:
            return {"relevance_score": 0.0}

        query_embedding = await self.embedder.embed_text(query)
        content_embedding = await self.embedder.embed_text(content)

        similarity = await self._calculate_cosine_similarity(query_embedding, content_embedding)

        return {
            "relevance_score": similarity,
            "keyword_matches": await self._count_keyword_matches(query, content),
            "semantic_similarity": similarity
        }

    async def _initialize_parsers(self):
        """Initialize parsers for different file types."""
        from app.processing.parsers.python_parser import PythonParser
        from app.processing.parsers.javascript_parser import JavaScriptParser
        from app.processing.parsers.typescript_parser import TypeScriptParser
        from app.processing.parsers.markdown_parser import MarkdownParser

        self.parsers = {
            ".py": PythonParser(),
            ".js": JavaScriptParser(),
            ".ts": TypeScriptParser(),
            ".tsx": TypeScriptParser(),
            ".jsx": JavaScriptParser(),
            ".md": MarkdownParser(),
            ".txt": MarkdownParser()  # Fallback
        }

    async def _initialize_embedder(self):
        """Initialize the embedding model."""
        from app.processing.embedders.openai_embedder import OpenAIEmbedder
        self.embedder = OpenAIEmbedder()

    async def _calculate_cosine_similarity(self, embedding1: List[float], embedding2: List[float]) -> float:
        """Calculate cosine similarity between two embeddings."""
        import numpy as np

        vec1 = np.array(embedding1)
        vec2 = np.array(embedding2)

        dot_product = np.dot(vec1, vec2)
        magnitude = np.linalg.norm(vec1) * np.linalg.norm(vec2)

        if magnitude == 0:
            return 0.0

        return float(dot_product / magnitude)
```

2. **Semantic Analyzer** (`app/utils/semantic_analyzer.py`):
```python
from typing import Dict, Any, List, Set
import ast
import re
from dataclasses import dataclass

@dataclass
class SemanticElement:
    name: str
    element_type: str  # "function", "class", "variable", "import"
    line_number: int
    signature: Optional[str] = None
    docstring: Optional[str] = None
    dependencies: List[str] = None

class SemanticAnalyzer:
    def __init__(self):
        self.keyword_patterns = {
            "function_call": r'\b(\w+)\s*\(',
            "class_usage": r'\b([A-Z]\w*)\s*\(',
            "variable_assignment": r'(\w+)\s*=',
            "import_statement": r'(?:from\s+(\w+)|import\s+(\w+))'
        }

    async def analyze_python_code(self, code: str) -> List[SemanticElement]:
        """Analyze Python code and extract semantic elements."""
        elements = []

        try:
            tree = ast.parse(code)

            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    elements.append(SemanticElement(
                        name=node.name,
                        element_type="function",
                        line_number=node.lineno,
                        signature=self._extract_function_signature(node),
                        docstring=ast.get_docstring(node),
                        dependencies=self._extract_function_dependencies(node)
                    ))
                elif isinstance(node, ast.ClassDef):
                    elements.append(SemanticElement(
                        name=node.name,
                        element_type="class",
                        line_number=node.lineno,
                        signature=self._extract_class_signature(node),
                        docstring=ast.get_docstring(node),
                        dependencies=self._extract_class_dependencies(node)
                    ))

        except SyntaxError as e:
            # Handle syntax errors gracefully
            pass

        return elements

    def _extract_function_signature(self, node: ast.FunctionDef) -> str:
        """Extract function signature from AST node."""
        args = []
        for arg in node.args.args:
            args.append(arg.arg)
        return f"{node.name}({', '.join(args)})"

    def _extract_function_dependencies(self, node: ast.FunctionDef) -> List[str]:
        """Extract function dependencies from AST node."""
        dependencies = set()

        for child in ast.walk(node):
            if isinstance(child, ast.Call) and isinstance(child.func, ast.Name):
                dependencies.add(child.func.id)

        return list(dependencies)
```

3. **Context Package Models** (`app/models/domain/context.py`):
```python
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
from datetime import datetime

@dataclass
class ContextItem:
    file_path: str
    content_type: str
    structured_content: Dict[str, Any]
    metadata: Dict[str, Any]
    relevance_score: Optional[float] = None
    created_at: datetime = None

    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.utcnow()

@dataclass
class ContextPackage:
    contexts: List[ContextItem]
    metadata: Dict[str, Any]
    created_at: datetime = None

    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.utcnow()

    def get_total_size(self) -> int:
        """Calculate total size of context package."""
        return sum(len(str(context.structured_content)) for context in self.contexts)

    def get_files(self) -> List[str]:
        """Get list of files included in context package."""
        return [context.file_path for context in self.contexts]

    def filter_by_relevance(self, min_score: float) -> 'ContextPackage':
        """Filter contexts by minimum relevance score."""
        filtered_contexts = [
            context for context in self.contexts
            if context.relevance_score and context.relevance_score >= min_score
        ]
        return ContextPackage(
            contexts=filtered_contexts,
            metadata={**self.metadata, "filtered_by_relevance": min_score}
        )
```

## Dependencies
- Task 031: Multi-Agent Coordination Framework
- Task 006: File Processing System
- Task 007: Vector Embeddings
- Task 012: Code Parsing
- Task 013: Semantic Analysis

## Estimated Time
20-24 hours

## Required Skills
- Advanced Python AST manipulation
- Natural language processing
- Vector embeddings and similarity
- Code analysis and parsing
- Machine learning optimization
- Multi-agent system integration
