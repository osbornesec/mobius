# Task 033: Retrieval Agent Implementation

## Overview
Implement the Retrieval Agent, a specialized AI agent responsible for intelligent context retrieval using machine learning ranking, hybrid search capabilities, and adaptive retrieval strategies. This agent will optimize retrieval performance and accuracy using advanced ML techniques.

## Success Criteria
- [ ] Agent achieves >95% retrieval accuracy on test queries
- [ ] ML ranking improves relevance scores by >25% over baseline
- [ ] Hybrid search (vector + keyword) outperforms single-method retrieval
- [ ] Adaptive retrieval strategies optimize for different query types
- [ ] Agent handles 1000+ concurrent queries with <100ms response time
- [ ] Integration with agent framework is seamless and robust

## Test First Approach

### Tests to Write BEFORE Implementation:

1. **Retrieval Accuracy Tests** (`tests/backend/unit/test_retrieval_agent.py`):
```python
def test_vector_retrieval_accuracy():
    """Test vector-based retrieval accuracy."""
    # Test semantic similarity retrieval
    # Test embedding quality validation
    # Test vector index performance
    # Test similarity threshold optimization
    # Test query expansion effectiveness

def test_keyword_retrieval_accuracy():
    """Test keyword-based retrieval accuracy."""
    # Test exact keyword matching
    # Test fuzzy keyword matching
    # Test term frequency optimization
    # Test boolean query support
    # Test phrase matching accuracy

def test_hybrid_search_performance():
    """Test hybrid search combining vector and keyword."""
    # Test score fusion algorithms
    # Test weight optimization
    # Test query routing logic
    # Test performance vs single methods
    # Test result diversity metrics
```

2. **ML Ranking Tests** (`tests/backend/unit/test_ml_ranking.py`):
```python
def test_ranking_model_performance():
    """Test ML ranking model accuracy."""
    # Test feature extraction quality
    # Test model prediction accuracy
    # Test ranking relevance metrics (NDCG, MAP)
    # Test model training convergence
    # Test inference performance

def test_adaptive_ranking():
    """Test adaptive ranking strategies."""
    # Test query type classification
    # Test strategy selection accuracy
    # Test performance per query type
    # Test adaptation effectiveness
    # Test fallback mechanisms

def test_ranking_optimization():
    """Test ranking optimization techniques."""
    # Test feature importance analysis
    # Test hyperparameter optimization
    # Test model ensemble performance
    # Test incremental learning
    # Test performance degradation detection
```

3. **Performance Tests** (`tests/backend/unit/test_retrieval_performance.py`):
```python
def test_concurrent_query_handling():
    """Test handling of concurrent queries."""
    # Test 1000+ concurrent queries
    # Test <100ms response time
    # Test resource utilization
    # Test queue management
    # Test graceful degradation

def test_caching_effectiveness():
    """Test retrieval caching strategies."""
    # Test cache hit rates
    # Test cache invalidation
    # Test memory usage optimization
    # Test cache coherency
    # Test performance improvements

def test_scalability_metrics():
    """Test agent scalability."""
    # Test query throughput scaling
    # Test memory usage patterns
    # Test CPU utilization
    # Test index size impact
    # Test network overhead
```

## Implementation Details

1. **Retrieval Agent Core** (`app/agents/retrieval_agent.py`):
```python
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import asyncio
import numpy as np
from datetime import datetime

from app.agents.base_agent import BaseAgent, AgentCapability
from app.processing.embedders.base import BaseEmbedder
from app.services.vector_service import VectorService
from app.services.search_service import SearchService
from app.models.domain.retrieval import RetrievalRequest, RetrievalResult, QueryType

class RetrievalStrategy(Enum):
    VECTOR_ONLY = "vector_only"
    KEYWORD_ONLY = "keyword_only"
    HYBRID = "hybrid"
    ADAPTIVE = "adaptive"

@dataclass
class RetrievalRequest:
    query: str
    query_type: Optional[QueryType] = None
    strategy: RetrievalStrategy = RetrievalStrategy.ADAPTIVE
    max_results: int = 10
    min_relevance: float = 0.5
    include_metadata: bool = True
    rerank: bool = True
    filter_criteria: Optional[Dict[str, Any]] = None

@dataclass
class RetrievalResult:
    items: List[Dict[str, Any]]
    total_found: int
    strategy_used: RetrievalStrategy
    response_time: float
    relevance_scores: List[float]
    metadata: Dict[str, Any]

class RetrievalAgent(BaseAgent):
    def __init__(self):
        capabilities = [
            AgentCapability("vector_retrieval", "1.0", 10, {"supports_semantic": True}),
            AgentCapability("keyword_retrieval", "1.0", 9, {"supports_fuzzy": True}),
            AgentCapability("hybrid_search", "1.0", 10, {"supports_fusion": True}),
            AgentCapability("ml_ranking", "1.0", 8, {"supports_learning": True})
        ]
        super().__init__("retrieval_agent", "retrieval", capabilities)

        self.vector_service: Optional[VectorService] = None
        self.search_service: Optional[SearchService] = None
        self.embedder: Optional[BaseEmbedder] = None
        self.ml_ranker: Optional[MLRanker] = None
        self.query_classifier: Optional[QueryClassifier] = None
        self.cache = {}

    async def on_start(self):
        """Initialize the retrieval agent."""
        # Initialize services
        await self._initialize_services()

        # Load ML models
        await self._load_ml_models()

        # Initialize caching
        await self._initialize_cache()

        # Start performance monitoring
        asyncio.create_task(self._monitor_performance())

    async def handle_message(self, message) -> Optional[Any]:
        """Handle retrieval requests."""
        action = message.payload.get("action")

        if action == "retrieve":
            request = RetrievalRequest(**message.payload.get("request", {}))
            result = await self.retrieve_context(request)
            return result.__dict__
        elif action == "rerank":
            return await self.rerank_results(
                message.payload.get("query"),
                message.payload.get("results")
            )
        elif action == "classify_query":
            return await self.classify_query(message.payload.get("query"))
        elif action == "optimize_strategy":
            return await self.optimize_strategy(message.payload.get("query_history"))
        else:
            return {"error": "Unknown action"}

    async def retrieve_context(self, request: RetrievalRequest) -> RetrievalResult:
        """Main retrieval method with strategy selection."""
        start_time = datetime.utcnow()

        try:
            # Check cache first
            cache_key = self._generate_cache_key(request)
            if cache_key in self.cache:
                cached_result = self.cache[cache_key]
                cached_result.metadata["from_cache"] = True
                return cached_result

            # Classify query type if not provided
            if request.query_type is None:
                request.query_type = await self.classify_query(request.query)

            # Select strategy
            strategy = await self._select_strategy(request)

            # Execute retrieval based on strategy
            if strategy == RetrievalStrategy.VECTOR_ONLY:
                results = await self._vector_retrieval(request)
            elif strategy == RetrievalStrategy.KEYWORD_ONLY:
                results = await self._keyword_retrieval(request)
            elif strategy == RetrievalStrategy.HYBRID:
                results = await self._hybrid_retrieval(request)
            else:  # ADAPTIVE
                results = await self._adaptive_retrieval(request)

            # Apply ML ranking if requested
            if request.rerank and self.ml_ranker:
                results = await self._apply_ml_ranking(request.query, results)

            # Calculate response time
            response_time = (datetime.utcnow() - start_time).total_seconds()

            # Create result object
            result = RetrievalResult(
                items=results[:request.max_results],
                total_found=len(results),
                strategy_used=strategy,
                response_time=response_time,
                relevance_scores=[item.get("relevance_score", 0.0) for item in results[:request.max_results]],
                metadata={
                    "query_type": request.query_type.value if request.query_type else "unknown",
                    "reranked": request.rerank,
                    "cached": False
                }
            )

            # Cache result
            if response_time < 1.0:  # Only cache fast results
                self.cache[cache_key] = result

            return result

        except Exception as e:
            self.logger.error(f"Retrieval failed: {str(e)}")
            raise

    async def _vector_retrieval(self, request: RetrievalRequest) -> List[Dict[str, Any]]:
        """Perform vector-based semantic retrieval."""
        if not self.vector_service or not self.embedder:
            raise RuntimeError("Vector services not initialized")

        # Generate query embedding
        query_embedding = await self.embedder.embed_text(request.query)

        # Search vector index
        results = await self.vector_service.similarity_search(
            query_embedding,
            limit=request.max_results * 2,  # Get more for reranking
            min_score=request.min_relevance,
            filter_criteria=request.filter_criteria
        )

        # Convert to standard format
        return [
            {
                "id": result.id,
                "content": result.content,
                "metadata": result.metadata,
                "relevance_score": result.score,
                "retrieval_method": "vector"
            }
            for result in results
        ]

    async def _keyword_retrieval(self, request: RetrievalRequest) -> List[Dict[str, Any]]:
        """Perform keyword-based retrieval."""
        if not self.search_service:
            raise RuntimeError("Search service not initialized")

        # Parse query for keywords
        keywords = await self._extract_keywords(request.query)

        # Build search query
        search_query = await self._build_search_query(keywords, request.filter_criteria)

        # Execute search
        results = await self.search_service.search(
            search_query,
            limit=request.max_results * 2,
            min_score=request.min_relevance
        )

        # Convert to standard format
        return [
            {
                "id": result.id,
                "content": result.content,
                "metadata": result.metadata,
                "relevance_score": result.score,
                "retrieval_method": "keyword"
            }
            for result in results
        ]

    async def _hybrid_retrieval(self, request: RetrievalRequest) -> List[Dict[str, Any]]:
        """Perform hybrid vector + keyword retrieval."""
        # Execute both retrieval methods
        vector_results = await self._vector_retrieval(request)
        keyword_results = await self._keyword_retrieval(request)

        # Fuse results using hybrid ranking
        fused_results = await self._fuse_results(
            vector_results, keyword_results, request.query
        )

        return fused_results

    async def _adaptive_retrieval(self, request: RetrievalRequest) -> List[Dict[str, Any]]:
        """Adaptively select best retrieval strategy."""
        query_type = request.query_type

        # Strategy selection based on query type and historical performance
        if query_type == QueryType.SEMANTIC:
            return await self._vector_retrieval(request)
        elif query_type == QueryType.KEYWORD:
            return await self._keyword_retrieval(request)
        elif query_type == QueryType.MIXED:
            return await self._hybrid_retrieval(request)
        else:
            # Default to hybrid for unknown types
            return await self._hybrid_retrieval(request)

    async def _fuse_results(self, vector_results: List[Dict], keyword_results: List[Dict], query: str) -> List[Dict]:
        """Fuse vector and keyword results using learned weights."""
        # Create combined result set
        all_results = {}

        # Add vector results
        for result in vector_results:
            result_id = result["id"]
            all_results[result_id] = {
                **result,
                "vector_score": result["relevance_score"],
                "keyword_score": 0.0,
                "fusion_score": 0.0
            }

        # Add keyword results
        for result in keyword_results:
            result_id = result["id"]
            if result_id in all_results:
                all_results[result_id]["keyword_score"] = result["relevance_score"]
            else:
                all_results[result_id] = {
                    **result,
                    "vector_score": 0.0,
                    "keyword_score": result["relevance_score"],
                    "fusion_score": 0.0
                }

        # Calculate fusion scores
        for result in all_results.values():
            # Learned fusion weights (could be adaptive)
            vector_weight = 0.7
            keyword_weight = 0.3

            result["fusion_score"] = (
                vector_weight * result["vector_score"] +
                keyword_weight * result["keyword_score"]
            )
            result["relevance_score"] = result["fusion_score"]
            result["retrieval_method"] = "hybrid"

        # Sort by fusion score
        sorted_results = sorted(
            all_results.values(),
            key=lambda x: x["fusion_score"],
            reverse=True
        )

        return sorted_results

    async def _apply_ml_ranking(self, query: str, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Apply ML-based reranking to results."""
        if not self.ml_ranker or len(results) <= 1:
            return results

        # Extract features for ranking
        features = await self._extract_ranking_features(query, results)

        # Predict relevance scores
        ml_scores = await self.ml_ranker.predict(features)

        # Update results with ML scores
        for i, result in enumerate(results):
            result["ml_score"] = ml_scores[i]
            result["original_score"] = result["relevance_score"]
            result["relevance_score"] = ml_scores[i]

        # Re-sort by ML scores
        return sorted(results, key=lambda x: x["ml_score"], reverse=True)

    async def classify_query(self, query: str) -> QueryType:
        """Classify query type for strategy selection."""
        if not self.query_classifier:
            return QueryType.MIXED  # Default

        return await self.query_classifier.classify(query)

    async def _select_strategy(self, request: RetrievalRequest) -> RetrievalStrategy:
        """Select optimal retrieval strategy."""
        if request.strategy != RetrievalStrategy.ADAPTIVE:
            return request.strategy

        # Use query type and historical performance to select strategy
        query_type = request.query_type

        if query_type == QueryType.SEMANTIC:
            return RetrievalStrategy.VECTOR_ONLY
        elif query_type == QueryType.KEYWORD:
            return RetrievalStrategy.KEYWORD_ONLY
        else:
            return RetrievalStrategy.HYBRID

    async def _initialize_services(self):
        """Initialize retrieval services."""
        from app.services.vector_service import VectorService
        from app.services.search_service import SearchService
        from app.processing.embedders.openai_embedder import OpenAIEmbedder

        self.vector_service = VectorService()
        self.search_service = SearchService()
        self.embedder = OpenAIEmbedder()

    async def _load_ml_models(self):
        """Load ML models for ranking and classification."""
        from app.ml.ranker import MLRanker
        from app.ml.query_classifier import QueryClassifier

        self.ml_ranker = MLRanker()
        self.query_classifier = QueryClassifier()

        await self.ml_ranker.load_model()
        await self.query_classifier.load_model()
```

2. **ML Ranker** (`app/ml/ranker.py`):
```python
from typing import List, Dict, Any, Tuple
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
import joblib
import asyncio

class MLRanker:
    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        self.feature_names = [
            "query_length", "content_length", "title_match", "content_match",
            "semantic_similarity", "keyword_density", "position_score",
            "freshness_score", "authority_score", "interaction_score"
        ]

    async def load_model(self):
        """Load pre-trained ranking model."""
        try:
            self.model = joblib.load("models/ranking_model.pkl")
            self.scaler = joblib.load("models/ranking_scaler.pkl")
        except FileNotFoundError:
            # Initialize and train a new model
            await self._train_initial_model()

    async def predict(self, features: List[List[float]]) -> List[float]:
        """Predict relevance scores for features."""
        if not self.model:
            return [0.5] * len(features)  # Default scores

        # Scale features
        scaled_features = self.scaler.transform(features)

        # Predict scores
        scores = self.model.predict(scaled_features)

        # Normalize to [0, 1] range
        scores = np.clip(scores, 0.0, 1.0)

        return scores.tolist()

    async def extract_features(self, query: str, results: List[Dict[str, Any]]) -> List[List[float]]:
        """Extract features for ranking."""
        features = []

        for result in results:
            feature_vector = await self._extract_single_feature_vector(query, result)
            features.append(feature_vector)

        return features

    async def _extract_single_feature_vector(self, query: str, result: Dict[str, Any]) -> List[float]:
        """Extract feature vector for a single result."""
        content = result.get("content", "")
        metadata = result.get("metadata", {})

        features = [
            len(query),  # query_length
            len(content),  # content_length
            self._calculate_title_match(query, metadata.get("title", "")),  # title_match
            self._calculate_content_match(query, content),  # content_match
            result.get("relevance_score", 0.0),  # semantic_similarity
            self._calculate_keyword_density(query, content),  # keyword_density
            metadata.get("position", 0),  # position_score
            self._calculate_freshness_score(metadata.get("last_modified")),  # freshness_score
            metadata.get("authority_score", 0.5),  # authority_score
            metadata.get("interaction_score", 0.0)  # interaction_score
        ]

        return features

    def _calculate_title_match(self, query: str, title: str) -> float:
        """Calculate title match score."""
        if not title:
            return 0.0

        query_words = set(query.lower().split())
        title_words = set(title.lower().split())

        if not query_words:
            return 0.0

        intersection = query_words.intersection(title_words)
        return len(intersection) / len(query_words)

    def _calculate_content_match(self, query: str, content: str) -> float:
        """Calculate content match score."""
        if not content:
            return 0.0

        query_lower = query.lower()
        content_lower = content.lower()

        # Simple substring matching
        matches = content_lower.count(query_lower)

        # Normalize by content length
        if len(content) == 0:
            return 0.0

        return min(matches / (len(content) / 100), 1.0)

    def _calculate_keyword_density(self, query: str, content: str) -> float:
        """Calculate keyword density in content."""
        if not content or not query:
            return 0.0

        query_words = query.lower().split()
        content_words = content.lower().split()

        if not content_words:
            return 0.0

        matches = sum(1 for word in content_words if word in query_words)
        return matches / len(content_words)

    def _calculate_freshness_score(self, last_modified: str) -> float:
        """Calculate freshness score based on last modified date."""
        if not last_modified:
            return 0.5  # Default for unknown

        # Implementation depends on date format
        # For now, return default
        return 0.5

    async def _train_initial_model(self):
        """Train initial ranking model with synthetic data."""
        # Generate synthetic training data
        X, y = await self._generate_training_data()

        # Fit scaler
        self.scaler.fit(X)
        X_scaled = self.scaler.transform(X)

        # Train model
        self.model = RandomForestRegressor(n_estimators=100, random_state=42)
        self.model.fit(X_scaled, y)

        # Save model
        joblib.dump(self.model, "models/ranking_model.pkl")
        joblib.dump(self.scaler, "models/ranking_scaler.pkl")

    async def _generate_training_data(self) -> Tuple[np.ndarray, np.ndarray]:
        """Generate synthetic training data."""
        n_samples = 1000
        n_features = len(self.feature_names)

        X = np.random.rand(n_samples, n_features)

        # Generate target scores based on simple rules
        y = np.zeros(n_samples)
        for i in range(n_samples):
            # Higher scores for higher semantic similarity
            y[i] += X[i][4] * 0.5  # semantic_similarity
            # Higher scores for title matches
            y[i] += X[i][2] * 0.3  # title_match
            # Higher scores for content matches
            y[i] += X[i][3] * 0.2  # content_match

        y = np.clip(y, 0.0, 1.0)

        return X, y
```

3. **Query Classifier** (`app/ml/query_classifier.py`):
```python
from typing import List, Dict
from enum import Enum
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
import joblib

class QueryType(Enum):
    SEMANTIC = "semantic"
    KEYWORD = "keyword"
    MIXED = "mixed"
    SPECIFIC = "specific"

class QueryClassifier:
    def __init__(self):
        self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        self.classifier = MultinomialNB()
        self.patterns = {
            QueryType.KEYWORD: [
                r'\b(find|search|look for)\b',
                r'\b(where|what|how|when|why)\b',
                r'\b(function|class|method|variable)\s+\w+\b'
            ],
            QueryType.SEMANTIC: [
                r'\b(explain|understand|mean|purpose)\b',
                r'\b(why does|how does|what does)\b',
                r'\b(similar to|like|related to)\b'
            ],
            QueryType.SPECIFIC: [
                r'\b\w+\(\)',  # function calls
                r'\b[A-Z]\w*\b',  # class names
                r'\b\w+\.\w+\b'  # method calls
            ]
        }

    async def load_model(self):
        """Load or train the query classifier."""
        try:
            self.vectorizer = joblib.load("models/query_vectorizer.pkl")
            self.classifier = joblib.load("models/query_classifier.pkl")
        except FileNotFoundError:
            await self._train_initial_classifier()

    async def classify(self, query: str) -> QueryType:
        """Classify query type."""
        # First try pattern matching
        pattern_type = self._classify_by_patterns(query)
        if pattern_type:
            return pattern_type

        # Use ML classifier if available
        if self.classifier:
            try:
                query_vector = self.vectorizer.transform([query])
                prediction = self.classifier.predict(query_vector)[0]
                return QueryType(prediction)
            except:
                pass

        # Default to mixed
        return QueryType.MIXED

    def _classify_by_patterns(self, query: str) -> QueryType:
        """Classify query using regex patterns."""
        query_lower = query.lower()

        for query_type, patterns in self.patterns.items():
            for pattern in patterns:
                if re.search(pattern, query_lower):
                    return query_type

        return None

    async def _train_initial_classifier(self):
        """Train initial classifier with synthetic data."""
        # Generate synthetic training data
        training_data = self._generate_training_data()

        queries = [item["query"] for item in training_data]
        labels = [item["type"].value for item in training_data]

        # Fit vectorizer and classifier
        query_vectors = self.vectorizer.fit_transform(queries)
        self.classifier.fit(query_vectors, labels)

        # Save models
        joblib.dump(self.vectorizer, "models/query_vectorizer.pkl")
        joblib.dump(self.classifier, "models/query_classifier.pkl")

    def _generate_training_data(self) -> List[Dict]:
        """Generate synthetic training data for query classification."""
        return [
            # Keyword queries
            {"query": "find function authenticate", "type": QueryType.KEYWORD},
            {"query": "search for login method", "type": QueryType.KEYWORD},
            {"query": "where is UserService", "type": QueryType.KEYWORD},

            # Semantic queries
            {"query": "explain how authentication works", "type": QueryType.SEMANTIC},
            {"query": "what does this function do", "type": QueryType.SEMANTIC},
            {"query": "why is this pattern used", "type": QueryType.SEMANTIC},

            # Specific queries
            {"query": "UserService.authenticate()", "type": QueryType.SPECIFIC},
            {"query": "LoginForm.validate", "type": QueryType.SPECIFIC},
            {"query": "auth.login", "type": QueryType.SPECIFIC},

            # Mixed queries
            {"query": "how to implement user authentication", "type": QueryType.MIXED},
            {"query": "find examples of login validation", "type": QueryType.MIXED},
            {"query": "best practices for password handling", "type": QueryType.MIXED}
        ]
```

## Dependencies
- Task 031: Multi-Agent Coordination Framework
- Task 007: Vector Embeddings
- Task 009: Qdrant Integration
- Task 025: Redis Integration
- Task 030: Performance Monitoring

## Estimated Time
18-22 hours

## Required Skills
- Machine learning and ranking algorithms
- Vector databases and similarity search
- Information retrieval techniques
- Performance optimization
- Caching strategies
- Statistical analysis and evaluation
