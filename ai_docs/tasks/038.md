# Task 038: Dynamic Persona Adaptation Engine Implementation

## Overview
Implement the Dynamic Persona Adaptation Engine that enables real-time learning and adjustment of AI persona characteristics based on user feedback, interaction patterns, and contextual information. This system will use reinforcement learning principles to continuously optimize persona behavior for improved user satisfaction.

## Success Criteria
- [ ] Adaptation engine learns from user feedback with >90% accuracy in preference detection
- [ ] Real-time adaptation responds to changes within <100ms
- [ ] Learning algorithms converge to stable preferences within 10-15 interactions
- [ ] User satisfaction scores improve by >30% with adaptive personas
- [ ] System prevents over-adaptation and maintains persona consistency
- [ ] Multi-user adaptation handles conflicting preferences effectively

## Test First Approach

### Tests to Write BEFORE Implementation:

1. **Learning Algorithm Tests** (`tests/backend/unit/test_learning_algorithms.py`):
```python
def test_preference_learning():
    """Test user preference learning accuracy."""
    # Test explicit feedback learning
    # Test implicit feedback detection
    # Test preference weight updates
    # Test convergence detection
    # Test learning rate adaptation

def test_reinforcement_learning():
    """Test RL-based adaptation mechanisms."""
    # Test reward signal generation
    # Test policy gradient updates
    # Test Q-learning for preference states
    # Test exploration vs exploitation balance
    # Test batch vs online learning

def test_multi_armed_bandit():
    """Test multi-armed bandit for persona selection."""
    # Test UCB algorithm implementation
    # Test Thompson sampling
    # Test epsilon-greedy exploration
    # Test contextual bandit adaptation
    # Test regret minimization
```

2. **Adaptation Control Tests** (`tests/backend/unit/test_adaptation_control.py`):
```python
def test_adaptation_constraints():
    """Test adaptation constraint enforcement."""
    # Test maximum change rate limits
    # Test persona boundary preservation
    # Test consistency maintenance
    # Test rollback mechanisms
    # Test safety constraints

def test_over_adaptation_prevention():
    """Test prevention of over-adaptation."""
    # Test adaptation decay mechanisms
    # Test stability detection
    # Test oscillation prevention
    # Test confidence thresholds
    # Test adaptation freezing

def test_multi_user_adaptation():
    """Test handling multiple user preferences."""
    # Test preference conflict resolution
    # Test user clustering algorithms
    # Test weighted adaptation
    # Test privacy preservation
    # Test session isolation
```

3. **Feedback Processing Tests** (`tests/backend/unit/test_feedback_processing.py`):
```python
def test_feedback_interpretation():
    """Test feedback signal interpretation."""
    # Test explicit rating processing
    # Test implicit behavior analysis
    # Test sentiment analysis accuracy
    # Test context-aware feedback
    # Test feedback noise filtering

def test_signal_extraction():
    """Test signal extraction from interactions."""
    # Test engagement metric calculation
    # Test completion rate analysis
    # Test response time patterns
    # Test follow-up question detection
    # Test user correction patterns

def test_feedback_aggregation():
    """Test feedback aggregation across sessions."""
    # Test temporal weighting
    # Test confidence scoring
    # Test outlier detection
    # Test trend analysis
    # Test preference stability measurement
```

## Implementation Details

1. **Dynamic Adaptation Engine Core** (`app/persona/adaptation_engine.py`):
```python
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import numpy as np
import asyncio
from datetime import datetime, timedelta
from collections import deque, defaultdict
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import json

class FeedbackType(Enum):
    EXPLICIT_RATING = "explicit_rating"
    IMPLICIT_BEHAVIOR = "implicit_behavior"
    CORRECTION = "correction"
    PREFERENCE_STATEMENT = "preference_statement"
    COMPLETION_RATE = "completion_rate"
    ENGAGEMENT_SCORE = "engagement_score"

class AdaptationStrategy(Enum):
    POLICY_GRADIENT = "policy_gradient"
    Q_LEARNING = "q_learning"
    MULTI_ARMED_BANDIT = "multi_armed_bandit"
    BAYESIAN_OPTIMIZATION = "bayesian_optimization"
    EVOLUTIONARY = "evolutionary"

@dataclass
class FeedbackSignal:
    feedback_type: FeedbackType
    value: float  # Normalized to [-1, 1]
    confidence: float  # 0.0 to 1.0
    context: Dict[str, Any]
    timestamp: datetime
    user_id: str
    session_id: str

@dataclass
class AdaptationAction:
    characteristic: str
    change_direction: str  # "increase", "decrease", "maintain"
    magnitude: float  # 0.0 to 1.0
    confidence: float
    reason: str

@dataclass
class UserPreferenceState:
    user_id: str
    preferences: Dict[str, float]  # characteristic -> preference strength
    confidence_scores: Dict[str, float]
    learning_history: List[FeedbackSignal]
    last_updated: datetime
    stability_score: float
    interaction_count: int

class DynamicAdaptationEngine:
    def __init__(self):
        self.user_states: Dict[str, UserPreferenceState] = {}
        self.adaptation_strategies: Dict[AdaptationStrategy, 'BaseAdaptationStrategy'] = {}
        self.feedback_processors: Dict[FeedbackType, 'BaseFeedbackProcessor'] = {}
        self.learning_rates = {
            "formality_level": 0.08,
            "technical_depth": 0.12,
            "empathy_level": 0.06,
            "creativity_level": 0.10,
            "proactivity": 0.09,
            "communication_style": 0.07
        }

        # Adaptation constraints
        self.max_change_per_step = 0.15
        self.min_confidence_threshold = 0.3
        self.stability_threshold = 0.85
        self.adaptation_decay_rate = 0.95

        # Multi-user adaptation
        self.user_clusters: Dict[str, List[str]] = {}
        self.cluster_preferences: Dict[str, Dict[str, float]] = {}

        # Initialize strategies and processors
        asyncio.create_task(self._initialize_components())

    async def _initialize_components(self):
        """Initialize adaptation strategies and feedback processors."""
        # Initialize adaptation strategies
        self.adaptation_strategies = {
            AdaptationStrategy.POLICY_GRADIENT: PolicyGradientStrategy(self.learning_rates),
            AdaptationStrategy.Q_LEARNING: QLearningStrategy(self.learning_rates),
            AdaptationStrategy.MULTI_ARMED_BANDIT: MultiArmedBanditStrategy(),
            AdaptationStrategy.BAYESIAN_OPTIMIZATION: BayesianOptimizationStrategy()
        }

        # Initialize feedback processors
        self.feedback_processors = {
            FeedbackType.EXPLICIT_RATING: ExplicitRatingProcessor(),
            FeedbackType.IMPLICIT_BEHAVIOR: ImplicitBehaviorProcessor(),
            FeedbackType.CORRECTION: CorrectionProcessor(),
            FeedbackType.PREFERENCE_STATEMENT: PreferenceStatementProcessor(),
            FeedbackType.COMPLETION_RATE: CompletionRateProcessor(),
            FeedbackType.ENGAGEMENT_SCORE: EngagementScoreProcessor()
        }

    async def process_feedback(self, feedback_data: Dict[str, Any],
                             user_id: str, session_id: str) -> List[AdaptationAction]:
        """Process incoming feedback and generate adaptation actions."""
        # Extract feedback signals
        feedback_signals = await self._extract_feedback_signals(feedback_data, user_id, session_id)

        # Update user preference state
        await self._update_user_state(user_id, feedback_signals)

        # Generate adaptation actions
        adaptation_actions = await self._generate_adaptation_actions(user_id, feedback_signals)

        # Apply adaptation constraints
        constrained_actions = await self._apply_adaptation_constraints(user_id, adaptation_actions)

        return constrained_actions

    async def _extract_feedback_signals(self, feedback_data: Dict[str, Any],
                                      user_id: str, session_id: str) -> List[FeedbackSignal]:
        """Extract structured feedback signals from raw feedback data."""
        signals = []
        timestamp = datetime.utcnow()

        for feedback_type, processor in self.feedback_processors.items():
            if processor.can_process(feedback_data):
                signal = await processor.process(feedback_data, user_id, session_id, timestamp)
                if signal:
                    signals.append(signal)

        return signals

    async def _update_user_state(self, user_id: str, feedback_signals: List[FeedbackSignal]):
        """Update user preference state with new feedback signals."""
        if user_id not in self.user_states:
            self.user_states[user_id] = UserPreferenceState(
                user_id=user_id,
                preferences={},
                confidence_scores={},
                learning_history=[],
                last_updated=datetime.utcnow(),
                stability_score=0.0,
                interaction_count=0
            )

        user_state = self.user_states[user_id]

        # Add new feedback signals to history
        user_state.learning_history.extend(feedback_signals)

        # Keep only recent history (last 100 interactions)
        if len(user_state.learning_history) > 100:
            user_state.learning_history = user_state.learning_history[-100:]

        # Update preference estimates
        await self._update_preference_estimates(user_state, feedback_signals)

        # Update confidence scores
        await self._update_confidence_scores(user_state)

        # Calculate stability score
        user_state.stability_score = await self._calculate_stability_score(user_state)

        user_state.last_updated = datetime.utcnow()
        user_state.interaction_count += len(feedback_signals)

    async def _update_preference_estimates(self, user_state: UserPreferenceState,
                                         feedback_signals: List[FeedbackSignal]):
        """Update preference estimates using weighted learning."""
        for signal in feedback_signals:
            # Extract characteristic implications from feedback
            characteristic_updates = await self._extract_characteristic_implications(signal)

            for characteristic, update_value in characteristic_updates.items():
                current_pref = user_state.preferences.get(characteristic, 0.0)
                learning_rate = self.learning_rates.get(characteristic, 0.1)

                # Apply temporal weighting (recent feedback has more weight)
                time_weight = self._calculate_temporal_weight(signal.timestamp)
                confidence_weight = signal.confidence

                # Update preference with weighted learning
                weighted_update = update_value * learning_rate * time_weight * confidence_weight
                new_preference = current_pref + weighted_update

                # Clamp to valid range [-1, 1]
                user_state.preferences[characteristic] = np.clip(new_preference, -1.0, 1.0)

    async def _extract_characteristic_implications(self, signal: FeedbackSignal) -> Dict[str, float]:
        """Extract characteristic implications from feedback signal."""
        implications = {}

        if signal.feedback_type == FeedbackType.EXPLICIT_RATING:
            # Direct rating implies overall satisfaction
            if signal.value > 0:
                # Positive rating - reinforce current characteristics
                context_characteristics = signal.context.get("current_characteristics", {})
                for char, value in context_characteristics.items():
                    implications[char] = signal.value * 0.1  # Small reinforcement
            else:
                # Negative rating - suggest opposite characteristics
                context_characteristics = signal.context.get("current_characteristics", {})
                for char, value in context_characteristics.items():
                    implications[char] = signal.value * 0.2  # Stronger correction

        elif signal.feedback_type == FeedbackType.CORRECTION:
            # User corrections indicate specific preferences
            correction_type = signal.context.get("correction_type")
            if correction_type == "too_formal":
                implications["formality_level"] = -0.3
            elif correction_type == "too_casual":
                implications["formality_level"] = 0.3
            elif correction_type == "too_technical":
                implications["technical_depth"] = -0.4
            elif correction_type == "not_detailed_enough":
                implications["technical_depth"] = 0.3
                implications["proactivity"] = 0.2
            elif correction_type == "too_verbose":
                implications["proactivity"] = -0.3

        elif signal.feedback_type == FeedbackType.ENGAGEMENT_SCORE:
            # High engagement suggests good characteristic alignment
            engagement_level = signal.value
            if engagement_level > 0.7:
                # Reinforce current characteristics
                context_characteristics = signal.context.get("current_characteristics", {})
                for char, value in context_characteristics.items():
                    implications[char] = engagement_level * 0.1

        return implications

    async def _generate_adaptation_actions(self, user_id: str,
                                         feedback_signals: List[FeedbackSignal]) -> List[AdaptationAction]:
        """Generate adaptation actions based on user state and feedback."""
        user_state = self.user_states[user_id]
        actions = []

        # Select adaptation strategy based on user interaction history
        strategy = await self._select_adaptation_strategy(user_state)

        # Generate actions using selected strategy
        strategy_actions = await strategy.generate_actions(user_state, feedback_signals)
        actions.extend(strategy_actions)

        # Add contextual adaptations
        contextual_actions = await self._generate_contextual_adaptations(user_state, feedback_signals)
        actions.extend(contextual_actions)

        # Prioritize and filter actions
        prioritized_actions = await self._prioritize_actions(actions, user_state)

        return prioritized_actions

    async def _apply_adaptation_constraints(self, user_id: str,
                                          actions: List[AdaptationAction]) -> List[AdaptationAction]:
        """Apply constraints to adaptation actions."""
        user_state = self.user_states[user_id]
        constrained_actions = []

        for action in actions:
            # Check confidence threshold
            if action.confidence < self.min_confidence_threshold:
                continue

            # Check maximum change rate
            if action.magnitude > self.max_change_per_step:
                action.magnitude = self.max_change_per_step

            # Check stability constraints
            if user_state.stability_score > self.stability_threshold:
                # Reduce adaptation magnitude for stable users
                action.magnitude *= 0.5

            # Check persona boundaries
            current_value = user_state.preferences.get(action.characteristic, 0.0)
            if action.change_direction == "increase":
                if current_value >= 0.9:  # Near maximum
                    continue
            elif action.change_direction == "decrease":
                if current_value <= -0.9:  # Near minimum
                    continue

            constrained_actions.append(action)

        return constrained_actions

    async def _select_adaptation_strategy(self, user_state: UserPreferenceState) -> 'BaseAdaptationStrategy':
        """Select appropriate adaptation strategy based on user state."""
        if user_state.interaction_count < 5:
            # Use multi-armed bandit for exploration with new users
            return self.adaptation_strategies[AdaptationStrategy.MULTI_ARMED_BANDIT]
        elif user_state.stability_score < 0.5:
            # Use policy gradient for unstable preferences
            return self.adaptation_strategies[AdaptationStrategy.POLICY_GRADIENT]
        else:
            # Use Q-learning for stable users
            return self.adaptation_strategies[AdaptationStrategy.Q_LEARNING]

    def _calculate_temporal_weight(self, feedback_timestamp: datetime) -> float:
        """Calculate temporal weight for feedback (recent feedback has higher weight)."""
        time_diff = (datetime.utcnow() - feedback_timestamp).total_seconds()
        # Exponential decay with half-life of 1 hour
        half_life = 3600  # seconds
        return np.exp(-time_diff * np.log(2) / half_life)

    async def _calculate_stability_score(self, user_state: UserPreferenceState) -> float:
        """Calculate preference stability score."""
        if len(user_state.learning_history) < 10:
            return 0.0

        # Calculate variance in recent preferences
        recent_signals = user_state.learning_history[-10:]
        preference_changes = []

        for i in range(1, len(recent_signals)):
            # Calculate magnitude of preference change
            change_magnitude = abs(recent_signals[i].value - recent_signals[i-1].value)
            preference_changes.append(change_magnitude)

        if not preference_changes:
            return 1.0

        # Stability is inverse of variance (normalized)
        variance = np.var(preference_changes)
        stability = 1.0 / (1.0 + variance)

        return min(stability, 1.0)

    async def get_adaptation_metrics(self, user_id: str) -> Dict[str, Any]:
        """Get adaptation metrics for a user."""
        if user_id not in self.user_states:
            return {"error": "User not found"}

        user_state = self.user_states[user_id]

        # Calculate learning progress
        learning_progress = min(user_state.interaction_count / 15.0, 1.0)  # Target 15 interactions

        # Calculate preference confidence
        avg_confidence = np.mean(list(user_state.confidence_scores.values())) if user_state.confidence_scores else 0.0

        # Calculate adaptation rate
        recent_adaptations = [s for s in user_state.learning_history if (datetime.utcnow() - s.timestamp).days < 1]
        adaptation_rate = len(recent_adaptations)

        return {
            "user_id": user_id,
            "learning_progress": learning_progress,
            "stability_score": user_state.stability_score,
            "average_confidence": avg_confidence,
            "total_interactions": user_state.interaction_count,
            "recent_adaptation_rate": adaptation_rate,
            "current_preferences": user_state.preferences,
            "confidence_scores": user_state.confidence_scores,
            "last_updated": user_state.last_updated.isoformat()
        }

    async def reset_user_adaptation(self, user_id: str):
        """Reset adaptation state for a user."""
        if user_id in self.user_states:
            del self.user_states[user_id]

    async def export_user_preferences(self, user_id: str) -> Optional[Dict[str, Any]]:
        """Export user preferences for backup or analysis."""
        if user_id not in self.user_states:
            return None

        user_state = self.user_states[user_id]

        return {
            "user_id": user_id,
            "preferences": user_state.preferences,
            "confidence_scores": user_state.confidence_scores,
            "stability_score": user_state.stability_score,
            "interaction_count": user_state.interaction_count,
            "export_timestamp": datetime.utcnow().isoformat()
        }

    async def import_user_preferences(self, preferences_data: Dict[str, Any]):
        """Import user preferences from backup."""
        user_id = preferences_data["user_id"]

        user_state = UserPreferenceState(
            user_id=user_id,
            preferences=preferences_data["preferences"],
            confidence_scores=preferences_data["confidence_scores"],
            learning_history=[],
            last_updated=datetime.utcnow(),
            stability_score=preferences_data["stability_score"],
            interaction_count=preferences_data["interaction_count"]
        )

        self.user_states[user_id] = user_state
```

2. **Feedback Processors** (`app/persona/feedback_processors.py`):
```python
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional
from datetime import datetime
import re
import numpy as np

class BaseFeedbackProcessor(ABC):
    @abstractmethod
    def can_process(self, feedback_data: Dict[str, Any]) -> bool:
        """Check if this processor can handle the feedback data."""
        pass

    @abstractmethod
    async def process(self, feedback_data: Dict[str, Any], user_id: str,
                     session_id: str, timestamp: datetime) -> Optional[FeedbackSignal]:
        """Process feedback data into a structured signal."""
        pass

class ExplicitRatingProcessor(BaseFeedbackProcessor):
    def can_process(self, feedback_data: Dict[str, Any]) -> bool:
        return "rating" in feedback_data or "satisfaction" in feedback_data

    async def process(self, feedback_data: Dict[str, Any], user_id: str,
                     session_id: str, timestamp: datetime) -> Optional[FeedbackSignal]:
        # Extract rating (assume 1-5 scale, convert to -1 to 1)
        rating = feedback_data.get("rating") or feedback_data.get("satisfaction")
        if rating is None:
            return None

        # Normalize rating to [-1, 1] range
        if isinstance(rating, (int, float)):
            if 1 <= rating <= 5:
                normalized_value = (rating - 3) / 2  # Convert 1-5 to -1 to 1
            else:
                normalized_value = np.clip(rating, -1, 1)
        else:
            return None

        confidence = 0.9  # High confidence for explicit ratings

        return FeedbackSignal(
            feedback_type=FeedbackType.EXPLICIT_RATING,
            value=normalized_value,
            confidence=confidence,
            context={
                "original_rating": rating,
                "rating_scale": feedback_data.get("scale", "1-5"),
                "current_characteristics": feedback_data.get("persona_state", {})
            },
            timestamp=timestamp,
            user_id=user_id,
            session_id=session_id
        )

class ImplicitBehaviorProcessor(BaseFeedbackProcessor):
    def can_process(self, feedback_data: Dict[str, Any]) -> bool:
        return any(key in feedback_data for key in [
            "response_time", "engagement_duration", "scroll_behavior",
            "interaction_pattern", "completion_rate"
        ])

    async def process(self, feedback_data: Dict[str, Any], user_id: str,
                     session_id: str, timestamp: datetime) -> Optional[FeedbackSignal]:
        # Calculate engagement score from multiple implicit signals
        engagement_factors = []

        # Response time (faster indicates engagement)
        if "response_time" in feedback_data:
            response_time = feedback_data["response_time"]
            if response_time < 2:  # Very fast response
                engagement_factors.append(0.8)
            elif response_time < 5:  # Normal response
                engagement_factors.append(0.5)
            else:  # Slow response
                engagement_factors.append(0.2)

        # Engagement duration
        if "engagement_duration" in feedback_data:
            duration = feedback_data["engagement_duration"]
            if duration > 30:  # Long engagement
                engagement_factors.append(0.9)
            elif duration > 10:  # Medium engagement
                engagement_factors.append(0.6)
            else:  # Short engagement
                engagement_factors.append(0.3)

        # Completion rate
        if "completion_rate" in feedback_data:
            completion_rate = feedback_data["completion_rate"]
            engagement_factors.append(completion_rate)

        if not engagement_factors:
            return None

        # Calculate overall engagement score
        engagement_score = np.mean(engagement_factors)

        # Convert to feedback value (-1 to 1, where 1 is high engagement)
        feedback_value = (engagement_score - 0.5) * 2

        confidence = 0.6  # Medium confidence for implicit signals

        return FeedbackSignal(
            feedback_type=FeedbackType.IMPLICIT_BEHAVIOR,
            value=feedback_value,
            confidence=confidence,
            context={
                "engagement_factors": engagement_factors,
                "raw_data": feedback_data
            },
            timestamp=timestamp,
            user_id=user_id,
            session_id=session_id
        )

class CorrectionProcessor(BaseFeedbackProcessor):
    def can_process(self, feedback_data: Dict[str, Any]) -> bool:
        return "correction" in feedback_data or "user_request" in feedback_data

    async def process(self, feedback_data: Dict[str, Any], user_id: str,
                     session_id: str, timestamp: datetime) -> Optional[FeedbackSignal]:
        correction_text = feedback_data.get("correction") or feedback_data.get("user_request", "")

        if not correction_text:
            return None

        # Analyze correction type using pattern matching
        correction_type = await self._analyze_correction_type(correction_text)

        if not correction_type:
            return None

        # Map correction types to feedback values
        correction_values = {
            "too_formal": -0.8,
            "too_casual": 0.8,
            "too_technical": -0.7,
            "not_technical_enough": 0.7,
            "too_verbose": -0.6,
            "not_detailed_enough": 0.6,
            "too_fast": -0.5,
            "too_slow": 0.5
        }

        feedback_value = correction_values.get(correction_type, 0.0)
        confidence = 0.95  # Very high confidence for explicit corrections

        return FeedbackSignal(
            feedback_type=FeedbackType.CORRECTION,
            value=feedback_value,
            confidence=confidence,
            context={
                "correction_type": correction_type,
                "original_text": correction_text
            },
            timestamp=timestamp,
            user_id=user_id,
            session_id=session_id
        )

    async def _analyze_correction_type(self, text: str) -> Optional[str]:
        """Analyze correction text to determine type."""
        text_lower = text.lower()

        # Pattern matching for common correction types
        patterns = {
            "too_formal": [r"too formal", r"less formal", r"more casual", r"relax"],
            "too_casual": [r"too casual", r"more formal", r"professional", r"formal"],
            "too_technical": [r"too technical", r"simpler", r"less technical", r"jargon"],
            "not_technical_enough": [r"more technical", r"detailed", r"specific", r"deeper"],
            "too_verbose": [r"too long", r"shorter", r"concise", r"brief"],
            "not_detailed_enough": [r"more detail", r"explain more", r"elaborate", r"expand"],
            "too_fast": [r"too fast", r"slow down", r"slower"],
            "too_slow": [r"too slow", r"faster", r"speed up"]
        }

        for correction_type, pattern_list in patterns.items():
            for pattern in pattern_list:
                if re.search(pattern, text_lower):
                    return correction_type

        return None

class EngagementScoreProcessor(BaseFeedbackProcessor):
    def can_process(self, feedback_data: Dict[str, Any]) -> bool:
        return "engagement_metrics" in feedback_data

    async def process(self, feedback_data: Dict[str, Any], user_id: str,
                     session_id: str, timestamp: datetime) -> Optional[FeedbackSignal]:
        metrics = feedback_data["engagement_metrics"]

        # Calculate composite engagement score
        score_components = []

        if "time_on_task" in metrics:
            # Normalize time on task (optimal range 10-60 seconds)
            time_score = min(metrics["time_on_task"] / 60.0, 1.0)
            score_components.append(time_score)

        if "interaction_frequency" in metrics:
            # Higher interaction frequency indicates engagement
            freq_score = min(metrics["interaction_frequency"] / 10.0, 1.0)
            score_components.append(freq_score)

        if "task_completion" in metrics:
            # Direct completion indicator
            score_components.append(metrics["task_completion"])

        if not score_components:
            return None

        engagement_score = np.mean(score_components)
        feedback_value = (engagement_score - 0.5) * 2  # Convert to -1 to 1

        confidence = 0.7

        return FeedbackSignal(
            feedback_type=FeedbackType.ENGAGEMENT_SCORE,
            value=feedback_value,
            confidence=confidence,
            context={"metrics": metrics},
            timestamp=timestamp,
            user_id=user_id,
            session_id=session_id
        )
```

3. **Adaptation Strategies** (`app/persona/adaptation_strategies.py`):
```python
from abc import ABC, abstractmethod
from typing import List, Dict, Any
import numpy as np
from collections import defaultdict

class BaseAdaptationStrategy(ABC):
    @abstractmethod
    async def generate_actions(self, user_state: UserPreferenceState,
                             feedback_signals: List[FeedbackSignal]) -> List[AdaptationAction]:
        """Generate adaptation actions based on user state and feedback."""
        pass

class PolicyGradientStrategy(BaseAdaptationStrategy):
    def __init__(self, learning_rates: Dict[str, float]):
        self.learning_rates = learning_rates
        self.policy_parameters = defaultdict(lambda: defaultdict(float))

    async def generate_actions(self, user_state: UserPreferenceState,
                             feedback_signals: List[FeedbackSignal]) -> List[AdaptationAction]:
        actions = []

        # Calculate policy gradients for each characteristic
        for characteristic in self.learning_rates.keys():
            gradient = await self._calculate_policy_gradient(
                characteristic, user_state, feedback_signals
            )

            if abs(gradient) > 0.1:  # Threshold for action generation
                direction = "increase" if gradient > 0 else "decrease"
                magnitude = min(abs(gradient), 1.0)
                confidence = min(abs(gradient) * 2, 1.0)

                action = AdaptationAction(
                    characteristic=characteristic,
                    change_direction=direction,
                    magnitude=magnitude,
                    confidence=confidence,
                    reason=f"Policy gradient optimization (gradient: {gradient:.3f})"
                )
                actions.append(action)

        return actions

    async def _calculate_policy_gradient(self, characteristic: str,
                                       user_state: UserPreferenceState,
                                       feedback_signals: List[FeedbackSignal]) -> float:
        """Calculate policy gradient for a characteristic."""
        gradient = 0.0

        for signal in feedback_signals[-5:]:  # Recent signals only
            # Reward signal
            reward = signal.value * signal.confidence

            # Current action probability (based on current preference)
            current_pref = user_state.preferences.get(characteristic, 0.0)

            # Calculate gradient contribution
            gradient += reward * self.learning_rates[characteristic] * (1 - abs(current_pref))

        return gradient

class MultiArmedBanditStrategy(BaseAdaptationStrategy):
    def __init__(self):
        self.arm_rewards = defaultdict(lambda: defaultdict(list))  # char -> direction -> rewards
        self.arm_counts = defaultdict(lambda: defaultdict(int))
        self.exploration_rate = 0.1

    async def generate_actions(self, user_state: UserPreferenceState,
                             feedback_signals: List[FeedbackSignal]) -> List[AdaptationAction]:
        actions = []

        # Update arm rewards with latest feedback
        await self._update_arm_rewards(user_state, feedback_signals)

        for characteristic in ["formality_level", "technical_depth", "empathy_level"]:
            # Use UCB (Upper Confidence Bound) for arm selection
            action = await self._select_arm_ucb(characteristic, user_state)
            if action:
                actions.append(action)

        return actions

    async def _update_arm_rewards(self, user_state: UserPreferenceState,
                                feedback_signals: List[FeedbackSignal]):
        """Update arm rewards based on feedback."""
        for signal in feedback_signals:
            reward = signal.value * signal.confidence

            # Determine which characteristics this feedback affects
            for characteristic in self.arm_rewards.keys():
                # Simple heuristic: positive feedback reinforces current direction
                current_pref = user_state.preferences.get(characteristic, 0.0)
                direction = "increase" if current_pref >= 0 else "decrease"

                self.arm_rewards[characteristic][direction].append(reward)
                if len(self.arm_rewards[characteristic][direction]) > 20:
                    self.arm_rewards[characteristic][direction] = \
                        self.arm_rewards[characteristic][direction][-20:]

    async def _select_arm_ucb(self, characteristic: str,
                            user_state: UserPreferenceState) -> Optional[AdaptationAction]:
        """Select arm using Upper Confidence Bound algorithm."""
        directions = ["increase", "decrease"]
        total_pulls = sum(self.arm_counts[characteristic][d] for d in directions)

        if total_pulls == 0:
            # No history - random exploration
            direction = np.random.choice(directions)
            confidence = 0.5
        else:
            # Calculate UCB values for each direction
            ucb_values = {}
            for direction in directions:
                pulls = self.arm_counts[characteristic][direction]
                if pulls == 0:
                    ucb_values[direction] = float('inf')
                else:
                    rewards = self.arm_rewards[characteristic][direction]
                    mean_reward = np.mean(rewards) if rewards else 0
                    confidence_bonus = np.sqrt(2 * np.log(total_pulls) / pulls)
                    ucb_values[direction] = mean_reward + confidence_bonus

            # Select direction with highest UCB value
            direction = max(ucb_values, key=ucb_values.get)
            confidence = min(ucb_values[direction], 1.0)

        # Update counts
        self.arm_counts[characteristic][direction] += 1

        if confidence > 0.3:  # Threshold for action generation
            return AdaptationAction(
                characteristic=characteristic,
                change_direction=direction,
                magnitude=0.1,  # Conservative changes for exploration
                confidence=confidence,
                reason=f"Multi-armed bandit exploration (UCB)"
            )

        return None
```

## Dependencies
- Task 037: AI Persona System Framework
- Task 031: Multi-Agent Coordination Framework
- Task 035: Advanced Memory System
- scikit-learn for clustering and machine learning
- numpy for numerical computations
- asyncio for asynchronous processing

## Estimated Time
22-26 hours

## Required Skills
- Reinforcement learning algorithms
- Machine learning model adaptation
- User behavior analysis
- Real-time system optimization
- Statistical analysis and pattern recognition
- Multi-user system design
