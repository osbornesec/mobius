# Task 043: Real-time Performance Monitoring System Implementation

## Overview
Implement a comprehensive real-time performance monitoring system that provides continuous visibility into platform performance, resource utilization, and user experience. This system will enable proactive performance optimization, capacity planning, and SLA monitoring with advanced analytics and alerting capabilities.

## Success Criteria
- [ ] Real-time monitoring captures performance metrics with <1 second latency
- [ ] System handles >1 million metrics per minute with <99.9% data loss
- [ ] Performance dashboards provide actionable insights for optimization
- [ ] Automated alerting detects performance issues within <30 seconds
- [ ] Capacity planning analytics predict resource needs with >90% accuracy
- [ ] SLA monitoring tracks and reports compliance with >99.5% accuracy

## Test First Approach

### Tests to Write BEFORE Implementation:

1. **Metrics Collection Tests** (`tests/backend/unit/test_metrics_collection.py`):
```python
def test_real_time_metrics_capture():
    """Test real-time metrics collection."""
    # Test application performance metrics
    # Test infrastructure resource metrics
    # Test user experience metrics
    # Test custom business metrics
    # Test metric timestamp accuracy

def test_high_volume_ingestion():
    """Test high-volume metrics ingestion."""
    # Test >1M metrics/minute ingestion
    # Test data loss prevention <0.1%
    # Test ingestion latency <1 second
    # Test backpressure handling
    # Test metric aggregation accuracy

def test_metric_aggregation():
    """Test metric aggregation and rollups."""
    # Test real-time aggregation accuracy
    # Test time-series rollup calculations
    # Test percentile calculations
    # Test rate calculations
    # Test statistical aggregations
```

2. **Performance Analytics Tests** (`tests/backend/unit/test_performance_analytics.py`):
```python
def test_anomaly_detection():
    """Test performance anomaly detection."""
    # Test statistical anomaly detection
    # Test machine learning anomaly models
    # Test threshold-based detection
    # Test trend analysis accuracy
    # Test false positive rate <5%

def test_capacity_planning():
    """Test capacity planning analytics."""
    # Test resource usage forecasting
    # Test growth trend analysis
    # Test capacity recommendation accuracy
    # Test scenario modeling
    # Test cost optimization analysis

def test_performance_profiling():
    """Test application performance profiling."""
    # Test code-level performance tracking
    # Test database query analysis
    # Test API endpoint performance
    # Test memory usage profiling
    # Test CPU utilization analysis
```

3. **Alerting System Tests** (`tests/backend/unit/test_alerting_system.py`):
```python
def test_real_time_alerting():
    """Test real-time alert generation."""
    # Test alert trigger accuracy
    # Test alert response time <30 seconds
    # Test alert escalation logic
    # Test alert deduplication
    # Test alert correlation

def test_sla_monitoring():
    """Test SLA monitoring and reporting."""
    # Test SLA threshold tracking
    # Test availability calculations
    # Test performance SLA monitoring
    # Test SLA violation detection
    # Test compliance reporting

def test_notification_delivery():
    """Test alert notification delivery."""
    # Test multi-channel notifications
    # Test notification reliability
    # Test notification rate limiting
    # Test on-call rotation integration
    # Test notification acknowledgment
```

## Implementation Details

1. **Performance Monitoring Core** (`app/monitoring/performance_monitor.py`):
```python
from typing import Dict, Any, List, Optional, Set, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime, timedelta
import asyncio
import time
import psutil
import resource
import gc
import threading
from collections import defaultdict, deque
import statistics
import numpy as np
from abc import ABC, abstractmethod

class MetricType(Enum):
    COUNTER = "counter"
    GAUGE = "gauge"
    HISTOGRAM = "histogram"
    SUMMARY = "summary"
    TIMER = "timer"

class AlertSeverity(Enum):
    INFO = "info"
    WARNING = "warning"
    CRITICAL = "critical"
    EMERGENCY = "emergency"

class SLAType(Enum):
    AVAILABILITY = "availability"
    RESPONSE_TIME = "response_time"
    THROUGHPUT = "throughput"
    ERROR_RATE = "error_rate"

@dataclass
class Metric:
    name: str
    value: Union[int, float]
    metric_type: MetricType
    timestamp: datetime
    labels: Dict[str, str] = field(default_factory=dict)
    unit: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class Alert:
    alert_id: str
    name: str
    description: str
    severity: AlertSeverity
    triggered_at: datetime
    metric_name: str
    current_value: Union[int, float]
    threshold_value: Union[int, float]
    condition: str  # "greater_than", "less_than", "equals", etc.
    labels: Dict[str, str] = field(default_factory=dict)
    resolved_at: Optional[datetime] = None
    acknowledged_at: Optional[datetime] = None
    acknowledged_by: Optional[str] = None

@dataclass
class SLATarget:
    sla_id: str
    name: str
    sla_type: SLAType
    target_value: float
    measurement_window: timedelta
    metric_names: List[str]
    conditions: Dict[str, Any]
    enabled: bool = True

@dataclass
class PerformanceSnapshot:
    timestamp: datetime
    cpu_usage: float
    memory_usage: float
    disk_usage: float
    network_io: Dict[str, float]
    active_connections: int
    request_rate: float
    response_time_p95: float
    error_rate: float
    custom_metrics: Dict[str, float] = field(default_factory=dict)

class PerformanceMonitor:
    def __init__(self):
        self.metrics_storage: Dict[str, deque] = defaultdict(lambda: deque(maxlen=10000))
        self.aggregated_metrics: Dict[str, Dict[str, Any]] = defaultdict(dict)
        self.alert_rules: Dict[str, Dict[str, Any]] = {}
        self.active_alerts: Dict[str, Alert] = {}
        self.sla_targets: Dict[str, SLATarget] = {}
        
        # Collectors for different metric types
        self.metric_collectors: Dict[str, 'BaseMetricCollector'] = {}
        self.alert_handlers: List['BaseAlertHandler'] = []
        self.sla_monitors: Dict[str, 'BaseSLAMonitor'] = {}
        
        # Performance tracking
        self.performance_snapshots: deque = deque(maxlen=1440)  # 24 hours at 1min intervals
        self.collection_stats = {
            "metrics_collected": 0,
            "collection_errors": 0,
            "collection_time_total": 0.0,
            "last_collection": None
        }
        
        # Configuration
        self.config = {
            "collection_interval": 1,  # seconds
            "aggregation_interval": 60,  # seconds
            "retention_days": 30,
            "alert_evaluation_interval": 10,  # seconds
            "sla_evaluation_interval": 300,  # 5 minutes
            "max_metrics_per_second": 100000
        }
        
        # Rate limiting and performance protection
        self.rate_limiter = MetricsRateLimiter(self.config["max_metrics_per_second"])
        
        # Initialize monitoring system
        asyncio.create_task(self._initialize_monitoring())
        
    async def _initialize_monitoring(self):
        """Initialize performance monitoring system."""
        # Initialize metric collectors
        await self._initialize_metric_collectors()
        
        # Initialize alert handlers
        await self._initialize_alert_handlers()
        
        # Initialize SLA monitors
        await self._initialize_sla_monitors()
        
        # Start background tasks
        asyncio.create_task(self._collect_metrics_continuously())
        asyncio.create_task(self._aggregate_metrics_periodically())
        asyncio.create_task(self._evaluate_alerts_continuously())
        asyncio.create_task(self._monitor_slas_continuously())
        asyncio.create_task(self._cleanup_old_metrics())
        
    async def record_metric(self, name: str, value: Union[int, float], 
                          metric_type: MetricType, labels: Dict[str, str] = None,
                          unit: str = None) -> bool:
        """Record a performance metric."""
        # Apply rate limiting
        if not await self.rate_limiter.allow_metric():
            return False
            
        try:
            metric = Metric(
                name=name,
                value=value,
                metric_type=metric_type,
                timestamp=datetime.utcnow(),
                labels=labels or {},
                unit=unit
            )
            
            # Store metric
            self.metrics_storage[name].append(metric)
            
            # Update real-time aggregations
            await self._update_real_time_aggregations(metric)
            
            # Update collection stats
            self.collection_stats["metrics_collected"] += 1
            self.collection_stats["last_collection"] = datetime.utcnow()
            
            return True
            
        except Exception as e:
            self.collection_stats["collection_errors"] += 1
            return False
            
    async def record_timing(self, name: str, duration: float, 
                          labels: Dict[str, str] = None) -> bool:
        """Record timing metric with automatic histogram creation."""
        return await self.record_metric(
            name=f"{name}_duration",
            value=duration,
            metric_type=MetricType.HISTOGRAM,
            labels=labels,
            unit="seconds"
        )
        
    async def increment_counter(self, name: str, increment: Union[int, float] = 1,
                              labels: Dict[str, str] = None) -> bool:
        """Increment a counter metric."""
        current_value = await self.get_current_metric_value(name, labels)
        return await self.record_metric(
            name=name,
            value=current_value + increment,
            metric_type=MetricType.COUNTER,
            labels=labels
        )
        
    async def set_gauge(self, name: str, value: Union[int, float],
                       labels: Dict[str, str] = None) -> bool:
        """Set a gauge metric value."""
        return await self.record_metric(
            name=name,
            value=value,
            metric_type=MetricType.GAUGE,
            labels=labels
        )
        
    async def get_metric_stats(self, name: str, window_minutes: int = 60,
                             labels: Dict[str, str] = None) -> Dict[str, Any]:
        """Get statistical summary of metric over time window."""
        if name not in self.metrics_storage:
            return {"error": "Metric not found"}
            
        cutoff_time = datetime.utcnow() - timedelta(minutes=window_minutes)
        
        # Filter metrics by time window and labels
        filtered_metrics = []
        for metric in self.metrics_storage[name]:
            if metric.timestamp >= cutoff_time:
                if not labels or all(metric.labels.get(k) == v for k, v in labels.items()):
                    filtered_metrics.append(metric)
                    
        if not filtered_metrics:
            return {"error": "No metrics in time window"}
            
        values = [m.value for m in filtered_metrics]
        
        return {
            "count": len(values),
            "min": min(values),
            "max": max(values),
            "mean": statistics.mean(values),
            "median": statistics.median(values),
            "std_dev": statistics.stdev(values) if len(values) > 1 else 0,
            "percentiles": {
                "p50": np.percentile(values, 50),
                "p90": np.percentile(values, 90),
                "p95": np.percentile(values, 95),
                "p99": np.percentile(values, 99)
            },
            "rate_per_minute": len(values) / window_minutes,
            "window_start": cutoff_time.isoformat(),
            "window_end": datetime.utcnow().isoformat()
        }
        
    async def create_alert_rule(self, name: str, metric_name: str,
                              condition: str, threshold: Union[int, float],
                              severity: AlertSeverity, window_minutes: int = 5,
                              labels: Dict[str, str] = None) -> str:
        """Create performance alert rule."""
        rule_id = f"alert_{len(self.alert_rules)}_{int(time.time())}"
        
        self.alert_rules[rule_id] = {
            "name": name,
            "metric_name": metric_name,
            "condition": condition,
            "threshold": threshold,
            "severity": severity,
            "window_minutes": window_minutes,
            "labels": labels or {},
            "enabled": True,
            "created_at": datetime.utcnow(),
            "trigger_count": 0,
            "last_triggered": None
        }
        
        return rule_id
        
    async def create_sla_target(self, name: str, sla_type: SLAType,
                              target_value: float, metric_names: List[str],
                              window_hours: int = 24) -> str:
        """Create SLA monitoring target."""
        sla_id = f"sla_{len(self.sla_targets)}_{int(time.time())}"
        
        sla_target = SLATarget(
            sla_id=sla_id,
            name=name,
            sla_type=sla_type,
            target_value=target_value,
            measurement_window=timedelta(hours=window_hours),
            metric_names=metric_names,
            conditions={}
        )
        
        self.sla_targets[sla_id] = sla_target
        return sla_id
        
    async def get_performance_dashboard_data(self) -> Dict[str, Any]:
        """Get comprehensive performance dashboard data."""
        current_time = datetime.utcnow()
        
        # System resource metrics
        system_metrics = await self._collect_system_metrics()
        
        # Application performance metrics
        app_metrics = await self._get_application_metrics()
        
        # Recent alerts
        recent_alerts = [
            alert for alert in self.active_alerts.values()
            if (current_time - alert.triggered_at).total_seconds() < 3600
        ]
        
        # SLA status
        sla_status = {}
        for sla_id, sla_target in self.sla_targets.items():
            sla_status[sla_id] = await self._calculate_sla_status(sla_target)
            
        # Top metrics by volume
        top_metrics = await self._get_top_metrics_by_volume()
        
        return {
            "timestamp": current_time.isoformat(),
            "system_metrics": system_metrics,
            "application_metrics": app_metrics,
            "active_alerts": len(self.active_alerts),
            "recent_alerts": [
                {
                    "alert_id": alert.alert_id,
                    "name": alert.name,
                    "severity": alert.severity.value,
                    "triggered_at": alert.triggered_at.isoformat(),
                    "metric_name": alert.metric_name,
                    "current_value": alert.current_value
                }
                for alert in recent_alerts
            ],
            "sla_status": sla_status,
            "collection_stats": self.collection_stats,
            "top_metrics": top_metrics,
            "performance_summary": {
                "total_metrics_stored": sum(len(deque_obj) for deque_obj in self.metrics_storage.values()),
                "unique_metric_names": len(self.metrics_storage),
                "alert_rules_count": len(self.alert_rules),
                "sla_targets_count": len(self.sla_targets)
            }
        }
        
    async def _collect_metrics_continuously(self):
        """Continuously collect system and application metrics."""
        while True:
            try:
                start_time = time.time()
                
                # Collect from all registered collectors
                for collector_name, collector in self.metric_collectors.items():
                    try:
                        metrics = await collector.collect()
                        for metric in metrics:
                            await self.record_metric(
                                metric.name, metric.value, metric.metric_type,
                                metric.labels, metric.unit
                            )
                    except Exception as e:
                        self.collection_stats["collection_errors"] += 1
                        
                # Update collection time stats
                collection_time = time.time() - start_time
                self.collection_stats["collection_time_total"] += collection_time
                
                # Sleep until next collection interval
                await asyncio.sleep(self.config["collection_interval"])
                
            except Exception as e:
                await asyncio.sleep(1)  # Prevent tight error loops
                
    async def _evaluate_alerts_continuously(self):
        """Continuously evaluate alert rules."""
        while True:
            try:
                for rule_id, rule in self.alert_rules.items():
                    if not rule["enabled"]:
                        continue
                        
                    # Get recent metric values
                    stats = await self.get_metric_stats(
                        rule["metric_name"],
                        rule["window_minutes"],
                        rule["labels"]
                    )
                    
                    if "error" in stats:
                        continue
                        
                    # Evaluate condition
                    current_value = stats["mean"]  # Can be customized per rule
                    threshold = rule["threshold"]
                    condition = rule["condition"]
                    
                    triggered = False
                    if condition == "greater_than" and current_value > threshold:
                        triggered = True
                    elif condition == "less_than" and current_value < threshold:
                        triggered = True
                    elif condition == "equals" and abs(current_value - threshold) < 0.001:
                        triggered = True
                        
                    if triggered:
                        await self._trigger_alert(rule_id, rule, current_value)
                        
                await asyncio.sleep(self.config["alert_evaluation_interval"])
                
            except Exception as e:
                await asyncio.sleep(1)
                
    async def _trigger_alert(self, rule_id: str, rule: Dict[str, Any], current_value: float):
        """Trigger an alert."""
        alert_id = f"alert_{rule_id}_{int(time.time())}"
        
        # Check if similar alert is already active
        existing_alert = None
        for alert in self.active_alerts.values():
            if (alert.metric_name == rule["metric_name"] and
                alert.labels == rule["labels"] and
                not alert.resolved_at):
                existing_alert = alert
                break
                
        if existing_alert:
            # Update existing alert
            existing_alert.current_value = current_value
            existing_alert.triggered_at = datetime.utcnow()
        else:
            # Create new alert
            alert = Alert(
                alert_id=alert_id,
                name=rule["name"],
                description=f"Metric {rule['metric_name']} {rule['condition']} {rule['threshold']}",
                severity=rule["severity"],
                triggered_at=datetime.utcnow(),
                metric_name=rule["metric_name"],
                current_value=current_value,
                threshold_value=rule["threshold"],
                condition=rule["condition"],
                labels=rule["labels"]
            )
            
            self.active_alerts[alert_id] = alert
            
            # Notify alert handlers
            for handler in self.alert_handlers:
                await handler.handle_alert(alert)
                
        # Update rule stats
        rule["trigger_count"] += 1
        rule["last_triggered"] = datetime.utcnow()
        
    async def _collect_system_metrics(self) -> Dict[str, Any]:
        """Collect current system performance metrics."""
        # CPU metrics
        cpu_percent = psutil.cpu_percent(interval=None)
        cpu_count = psutil.cpu_count()
        load_avg = psutil.getloadavg()
        
        # Memory metrics
        memory = psutil.virtual_memory()
        swap = psutil.swap_memory()
        
        # Disk metrics
        disk = psutil.disk_usage('/')
        disk_io = psutil.disk_io_counters()
        
        # Network metrics
        network_io = psutil.net_io_counters()
        
        # Process metrics
        process = psutil.Process()
        process_memory = process.memory_info()
        
        return {
            "cpu": {
                "usage_percent": cpu_percent,
                "core_count": cpu_count,
                "load_average": {
                    "1min": load_avg[0],
                    "5min": load_avg[1],
                    "15min": load_avg[2]
                }
            },
            "memory": {
                "total_gb": memory.total / (1024**3),
                "available_gb": memory.available / (1024**3),
                "used_gb": memory.used / (1024**3),
                "usage_percent": memory.percent,
                "swap_total_gb": swap.total / (1024**3),
                "swap_used_gb": swap.used / (1024**3),
                "swap_percent": swap.percent
            },
            "disk": {
                "total_gb": disk.total / (1024**3),
                "used_gb": disk.used / (1024**3),
                "free_gb": disk.free / (1024**3),
                "usage_percent": (disk.used / disk.total) * 100,
                "read_count": disk_io.read_count if disk_io else 0,
                "write_count": disk_io.write_count if disk_io else 0,
                "read_bytes": disk_io.read_bytes if disk_io else 0,
                "write_bytes": disk_io.write_bytes if disk_io else 0
            },
            "network": {
                "bytes_sent": network_io.bytes_sent,
                "bytes_recv": network_io.bytes_recv,
                "packets_sent": network_io.packets_sent,
                "packets_recv": network_io.packets_recv
            },
            "process": {
                "memory_rss_mb": process_memory.rss / (1024**2),
                "memory_vms_mb": process_memory.vms / (1024**2),
                "cpu_percent": process.cpu_percent(),
                "open_files": len(process.open_files()),
                "connections": len(process.connections())
            }
        }
```

2. **Metric Collectors** (`app/monitoring/metric_collectors.py`):
```python
from abc import ABC, abstractmethod
from typing import List, Dict, Any
import asyncio
import time
import aiohttp
import psutil
from datetime import datetime

class BaseMetricCollector(ABC):
    @abstractmethod
    async def collect(self) -> List[Metric]:
        """Collect metrics and return list of Metric objects."""
        pass

class SystemMetricsCollector(BaseMetricCollector):
    async def collect(self) -> List[Metric]:
        """Collect system-level performance metrics."""
        metrics = []
        timestamp = datetime.utcnow()
        
        # CPU metrics
        cpu_percent = psutil.cpu_percent(interval=None)
        metrics.append(Metric(
            name="system_cpu_usage_percent",
            value=cpu_percent,
            metric_type=MetricType.GAUGE,
            timestamp=timestamp,
            unit="percent"
        ))
        
        # Memory metrics
        memory = psutil.virtual_memory()
        metrics.extend([
            Metric("system_memory_usage_percent", memory.percent, MetricType.GAUGE, timestamp, unit="percent"),
            Metric("system_memory_used_bytes", memory.used, MetricType.GAUGE, timestamp, unit="bytes"),
            Metric("system_memory_available_bytes", memory.available, MetricType.GAUGE, timestamp, unit="bytes")
        ])
        
        # Disk metrics
        disk = psutil.disk_usage('/')
        metrics.extend([
            Metric("system_disk_usage_percent", (disk.used / disk.total) * 100, MetricType.GAUGE, timestamp, unit="percent"),
            Metric("system_disk_used_bytes", disk.used, MetricType.GAUGE, timestamp, unit="bytes"),
            Metric("system_disk_free_bytes", disk.free, MetricType.GAUGE, timestamp, unit="bytes")
        ])
        
        # Network I/O
        network_io = psutil.net_io_counters()
        if network_io:
            metrics.extend([
                Metric("system_network_bytes_sent", network_io.bytes_sent, MetricType.COUNTER, timestamp, unit="bytes"),
                Metric("system_network_bytes_recv", network_io.bytes_recv, MetricType.COUNTER, timestamp, unit="bytes"),
                Metric("system_network_packets_sent", network_io.packets_sent, MetricType.COUNTER, timestamp),
                Metric("system_network_packets_recv", network_io.packets_recv, MetricType.COUNTER, timestamp)
            ])
            
        return metrics

class ApplicationMetricsCollector(BaseMetricCollector):
    def __init__(self):
        self.request_count = 0
        self.error_count = 0
        self.response_times = deque(maxlen=1000)
        
    async def collect(self) -> List[Metric]:
        """Collect application-level performance metrics."""
        metrics = []
        timestamp = datetime.utcnow()
        
        # Request metrics
        metrics.append(Metric(
            name="app_requests_total",
            value=self.request_count,
            metric_type=MetricType.COUNTER,
            timestamp=timestamp
        ))
        
        metrics.append(Metric(
            name="app_errors_total", 
            value=self.error_count,
            metric_type=MetricType.COUNTER,
            timestamp=timestamp
        ))
        
        # Response time metrics
        if self.response_times:
            metrics.extend([
                Metric("app_response_time_mean", statistics.mean(self.response_times), MetricType.GAUGE, timestamp, unit="seconds"),
                Metric("app_response_time_p95", np.percentile(self.response_times, 95), MetricType.GAUGE, timestamp, unit="seconds"),
                Metric("app_response_time_p99", np.percentile(self.response_times, 99), MetricType.GAUGE, timestamp, unit="seconds")
            ])
            
        # Error rate
        error_rate = (self.error_count / max(1, self.request_count)) * 100
        metrics.append(Metric(
            name="app_error_rate_percent",
            value=error_rate,
            metric_type=MetricType.GAUGE,
            timestamp=timestamp,
            unit="percent"
        ))
        
        return metrics
        
    def record_request(self, response_time: float, is_error: bool = False):
        """Record application request metrics."""
        self.request_count += 1
        self.response_times.append(response_time)
        
        if is_error:
            self.error_count += 1

class DatabaseMetricsCollector(BaseMetricCollector):
    def __init__(self, database_url: str):
        self.database_url = database_url
        self.db_pool = None
        
    async def collect(self) -> List[Metric]:
        """Collect database performance metrics."""
        metrics = []
        timestamp = datetime.utcnow()
        
        if not self.db_pool:
            return metrics
            
        try:
            # Connection pool metrics
            pool_stats = self.db_pool.get_stats()
            metrics.extend([
                Metric("db_pool_size", pool_stats.get("pool_size", 0), MetricType.GAUGE, timestamp),
                Metric("db_pool_available", pool_stats.get("available", 0), MetricType.GAUGE, timestamp),
                Metric("db_pool_used", pool_stats.get("used", 0), MetricType.GAUGE, timestamp)
            ])
            
            # Query performance metrics
            async with self.db_pool.acquire() as conn:
                # Active connections
                result = await conn.fetchval("SELECT count(*) FROM pg_stat_activity")
                metrics.append(Metric("db_active_connections", result, MetricType.GAUGE, timestamp))
                
                # Database size
                result = await conn.fetchval("SELECT pg_database_size(current_database())")
                metrics.append(Metric("db_size_bytes", result, MetricType.GAUGE, timestamp, unit="bytes"))
                
                # Transaction stats
                stats = await conn.fetchrow("""
                    SELECT xact_commit, xact_rollback, blks_read, blks_hit 
                    FROM pg_stat_database WHERE datname = current_database()
                """)
                
                if stats:
                    metrics.extend([
                        Metric("db_transactions_committed", stats["xact_commit"], MetricType.COUNTER, timestamp),
                        Metric("db_transactions_rolled_back", stats["xact_rollback"], MetricType.COUNTER, timestamp),
                        Metric("db_blocks_read", stats["blks_read"], MetricType.COUNTER, timestamp),
                        Metric("db_blocks_hit", stats["blks_hit"], MetricType.COUNTER, timestamp)
                    ])
                    
                    # Cache hit ratio
                    total_reads = stats["blks_read"] + stats["blks_hit"]
                    if total_reads > 0:
                        hit_ratio = (stats["blks_hit"] / total_reads) * 100
                        metrics.append(Metric("db_cache_hit_ratio", hit_ratio, MetricType.GAUGE, timestamp, unit="percent"))
                        
        except Exception as e:
            # Record collection error
            metrics.append(Metric("db_collection_errors", 1, MetricType.COUNTER, timestamp))
            
        return metrics

class CustomBusinessMetricsCollector(BaseMetricCollector):
    def __init__(self):
        self.business_metrics = {}
        
    async def collect(self) -> List[Metric]:
        """Collect custom business metrics."""
        metrics = []
        timestamp = datetime.utcnow()
        
        for metric_name, metric_data in self.business_metrics.items():
            metrics.append(Metric(
                name=f"business_{metric_name}",
                value=metric_data["value"],
                metric_type=metric_data["type"],
                timestamp=timestamp,
                labels=metric_data.get("labels", {}),
                unit=metric_data.get("unit")
            ))
            
        return metrics
        
    def set_business_metric(self, name: str, value: Union[int, float], 
                          metric_type: MetricType, labels: Dict[str, str] = None,
                          unit: str = None):
        """Set a custom business metric."""
        self.business_metrics[name] = {
            "value": value,
            "type": metric_type,
            "labels": labels or {},
            "unit": unit
        }
```

3. **SLA Monitoring** (`app/monitoring/sla_monitors.py`):
```python
from abc import ABC, abstractmethod
from typing import Dict, Any, List
from datetime import datetime, timedelta
import asyncio

class BaseSLAMonitor(ABC):
    @abstractmethod
    async def evaluate_sla(self, sla_target: SLATarget, 
                          metrics: Dict[str, Any]) -> Dict[str, Any]:
        """Evaluate SLA compliance."""
        pass

class AvailabilitySLAMonitor(BaseSLAMonitor):
    async def evaluate_sla(self, sla_target: SLATarget, 
                          metrics: Dict[str, Any]) -> Dict[str, Any]:
        """Evaluate availability SLA."""
        # Calculate uptime percentage
        total_time = sla_target.measurement_window.total_seconds()
        downtime_seconds = 0
        
        # Check for downtime events
        for metric_name in sla_target.metric_names:
            if metric_name in metrics:
                metric_data = metrics[metric_name]
                # Count periods where service was down
                for data_point in metric_data:
                    if data_point["value"] == 0:  # Service down
                        downtime_seconds += 60  # Assume 1-minute granularity
                        
        uptime_percentage = ((total_time - downtime_seconds) / total_time) * 100
        
        return {
            "sla_id": sla_target.sla_id,
            "current_value": uptime_percentage,
            "target_value": sla_target.target_value,
            "compliant": uptime_percentage >= sla_target.target_value,
            "deviation": uptime_percentage - sla_target.target_value,
            "measurement_period": sla_target.measurement_window.total_seconds(),
            "downtime_seconds": downtime_seconds,
            "evaluation_time": datetime.utcnow().isoformat()
        }

class ResponseTimeSLAMonitor(BaseSLAMonitor):
    async def evaluate_sla(self, sla_target: SLATarget, 
                          metrics: Dict[str, Any]) -> Dict[str, Any]:
        """Evaluate response time SLA."""
        response_times = []
        
        # Collect response time data
        for metric_name in sla_target.metric_names:
            if metric_name in metrics:
                metric_data = metrics[metric_name]
                response_times.extend([d["value"] for d in metric_data])
                
        if not response_times:
            return {
                "sla_id": sla_target.sla_id,
                "error": "No response time data available"
            }
            
        # Calculate P95 response time
        p95_response_time = np.percentile(response_times, 95)
        
        return {
            "sla_id": sla_target.sla_id,
            "current_value": p95_response_time,
            "target_value": sla_target.target_value,
            "compliant": p95_response_time <= sla_target.target_value,
            "deviation": sla_target.target_value - p95_response_time,
            "sample_count": len(response_times),
            "min_response_time": min(response_times),
            "max_response_time": max(response_times),
            "mean_response_time": statistics.mean(response_times),
            "evaluation_time": datetime.utcnow().isoformat()
        }

class MetricsRateLimiter:
    def __init__(self, max_metrics_per_second: int):
        self.max_metrics_per_second = max_metrics_per_second
        self.metrics_count = 0
        self.window_start = time.time()
        self.lock = asyncio.Lock()
        
    async def allow_metric(self) -> bool:
        """Check if metric can be recorded based on rate limit."""
        async with self.lock:
            current_time = time.time()
            
            # Reset window if needed
            if current_time - self.window_start >= 1.0:
                self.metrics_count = 0
                self.window_start = current_time
                
            # Check rate limit
            if self.metrics_count >= self.max_metrics_per_second:
                return False
                
            self.metrics_count += 1
            return True
```

## Dependencies
- Task 040: Advanced Security Framework
- Task 008: Async Database Operations
- Task 025: Redis Integration
- psutil for system metrics collection
- numpy for statistical calculations
- asyncio for concurrent metric collection

## Estimated Time
24-28 hours

## Required Skills
- Performance monitoring and observability
- Time-series data management
- Statistical analysis and anomaly detection
- Real-time alerting systems
- SLA monitoring and reporting
- High-performance metrics collection