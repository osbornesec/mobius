# Task 051: High-Performance System Architecture Implementation for 10k+ Concurrent Users

## Overview
Implement a comprehensive high-performance system architecture that supports 10,000+ concurrent users with sub-200ms response times. This task focuses on designing and implementing the foundational infrastructure components including advanced caching strategies, connection pooling, horizontal pod autoscaling, and multi-tier request processing to achieve enterprise-scale performance targets.

## Success Criteria
- [ ] System supports 10,000+ concurrent users with stable performance
- [ ] Average response time remains below 200ms (99th percentile <500ms)
- [ ] Connection pooling achieves 95%+ pool utilization efficiency
- [ ] Multi-level caching delivers 85%+ cache hit rates for frequently accessed data
- [ ] Horizontal Pod Autoscaler (HPA) scales pods within 30 seconds of load spikes
- [ ] Load balancer distributes traffic with <1% variance across healthy pods

## Test First Approach

### Tests to Write BEFORE Implementation:

1. **Performance Load Tests** (`tests/backend/unit/test_performance_load.py`):
```python
def test_concurrent_user_capacity():
    """Test system capacity with 10k+ concurrent users."""
    # Test concurrent connection handling
    # Test response time under load
    # Test system stability during sustained load
    # Test resource utilization metrics
    # Test graceful degradation under extreme load

def test_response_time_requirements():
    """Test response time compliance under various loads."""
    # Test average response time <200ms
    # Test 99th percentile response time <500ms
    # Test response time consistency across endpoints
    # Test performance impact of different payload sizes
    # Test response time during scale-up events

def test_horizontal_scaling():
    """Test automatic horizontal scaling behavior."""
    # Test HPA trigger thresholds
    # Test pod scaling speed and accuracy
    # Test scale-down behavior during low load
    # Test scaling during traffic spikes
    # Test scaling limits and boundaries
```

2. **Caching Performance Tests** (`tests/backend/unit/test_caching_performance.py`):
```python
def test_multi_level_caching():
    """Test multi-level caching strategy performance."""
    # Test L1 cache (in-memory) hit rates
    # Test L2 cache (Redis) hit rates
    # Test cache invalidation strategies
    # Test cache coherency across nodes
    # Test cache performance under high concurrency

def test_cache_efficiency():
    """Test caching efficiency and hit rates."""
    # Test cache hit rate targets (85%+)
    # Test cache eviction policies
    # Test cache warming strategies
    # Test cache performance with different data types
    # Test cache memory usage optimization

def test_cache_consistency():
    """Test cache consistency mechanisms."""
    # Test cache invalidation propagation
    # Test cache update consistency
    # Test cache failover behavior
    # Test cache recovery after failures
    # Test cache synchronization across regions
```

3. **Connection Pool Tests** (`tests/backend/unit/test_connection_pooling.py`):
```python
def test_connection_pool_efficiency():
    """Test connection pool utilization and efficiency."""
    # Test pool utilization rates (95%+)
    # Test connection acquisition speed
    # Test connection lifecycle management
    # Test pool sizing under different loads
    # Test connection pool failover

def test_database_connection_pooling():
    """Test database connection pool performance."""
    # Test PostgreSQL connection pool efficiency
    # Test connection pool sizing strategies
    # Test connection timeout handling
    # Test connection health monitoring
    # Test pool overflow behavior

def test_external_service_pooling():
    """Test external service connection pooling."""
    # Test HTTP connection pool management
    # Test connection reuse efficiency
    # Test pool performance with external APIs
    # Test connection pool circuit breaker integration
    # Test connection pool monitoring
```

## Implementation Details

1. **High-Performance Core Infrastructure** (`app/performance/high_performance_core.py`):
```python
from typing import Dict, Any, List, Optional, Set, Callable, Awaitable
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime, timedelta
import asyncio
import json
import time
import logging
import weakref
from abc import ABC, abstractmethod
from collections import defaultdict, deque
import uvloop
import orjson
import aioredis
import asyncpg
from prometheus_client import Counter, Histogram, Gauge

class PerformanceTier(Enum):
    PREMIUM = "premium"
    STANDARD = "standard"
    BASIC = "basic"
    THROTTLED = "throttled"

class RequestPriority(Enum):
    CRITICAL = "critical"
    HIGH = "high"
    NORMAL = "normal"
    LOW = "low"
    BACKGROUND = "background"

class CacheLevel(Enum):
    L1_MEMORY = "l1_memory"
    L2_REDIS = "l2_redis"
    L3_DATABASE = "l3_database"
    CDN = "cdn"

@dataclass
class PerformanceMetrics:
    request_count: int = 0
    response_time_ms: float = 0.0
    cache_hit_rate: float = 0.0
    connection_pool_utilization: float = 0.0
    concurrent_users: int = 0
    error_rate: float = 0.0
    throughput_rps: float = 0.0
    memory_usage_mb: float = 0.0
    cpu_utilization: float = 0.0

@dataclass
class ConnectionPoolConfig:
    min_size: int
    max_size: int
    max_idle_time: int
    max_lifetime: int
    health_check_interval: int
    acquire_timeout: int
    connection_timeout: int

@dataclass
class CacheConfig:
    ttl_seconds: int
    max_memory_mb: int
    eviction_policy: str
    compression_enabled: bool
    serialization_format: str
    cache_warming_enabled: bool

@dataclass
class PerformanceConfig:
    max_concurrent_requests: int
    request_timeout_seconds: int
    circuit_breaker_threshold: int
    rate_limiting_enabled: bool
    connection_pools: Dict[str, ConnectionPoolConfig]
    cache_configs: Dict[CacheLevel, CacheConfig]
    performance_monitoring_interval: int

class HighPerformanceCore:
    def __init__(self, config: PerformanceConfig):
        self.config = config
        self.metrics = PerformanceMetrics()

        # Performance monitoring
        self.request_counter = Counter('requests_total', 'Total requests', ['endpoint', 'status'])
        self.response_time_histogram = Histogram('response_time_seconds', 'Response time')
        self.concurrent_users_gauge = Gauge('concurrent_users', 'Current concurrent users')
        self.cache_hit_rate_gauge = Gauge('cache_hit_rate', 'Cache hit rate', ['level'])

        # Connection pools
        self.connection_pools: Dict[str, 'BaseConnectionPool'] = {}
        self.pool_managers: Dict[str, 'PoolManager'] = {}

        # Caching layers
        self.cache_layers: Dict[CacheLevel, 'BaseCacheLayer'] = {}
        self.cache_manager: Optional['CacheManager'] = None

        # Request processing
        self.request_processor: Optional['RequestProcessor'] = None
        self.load_balancer: Optional['LoadBalancer'] = None
        self.circuit_breakers: Dict[str, 'CircuitBreaker'] = {}

        # Performance optimization
        self.performance_optimizer: Optional['PerformanceOptimizer'] = None
        self.resource_monitor: Optional['ResourceMonitor'] = None

        # Initialize event loop optimization
        self._setup_event_loop()

        # Initialize core components
        asyncio.create_task(self._initialize_performance_core())

    def _setup_event_loop(self):
        """Setup optimized event loop for high performance."""
        try:
            # Use uvloop for better performance on Linux
            if hasattr(uvloop, 'install'):
                uvloop.install()
                logging.info("Installed uvloop for enhanced performance")
        except ImportError:
            logging.warning("uvloop not available, using default event loop")

        # Configure event loop settings
        loop = asyncio.get_event_loop()
        loop.set_debug(False)  # Disable debug mode for production performance

    async def _initialize_performance_core(self):
        """Initialize all performance components."""
        # Initialize connection pools
        await self._initialize_connection_pools()

        # Initialize caching layers
        await self._initialize_cache_layers()

        # Initialize request processing
        await self._initialize_request_processing()

        # Initialize performance monitoring
        await self._initialize_performance_monitoring()

        # Start background tasks
        asyncio.create_task(self._performance_monitoring_loop())
        asyncio.create_task(self._connection_pool_health_check())
        asyncio.create_task(self._cache_maintenance_loop())
        asyncio.create_task(self._metrics_collection_loop())

    async def _initialize_connection_pools(self):
        """Initialize all connection pools."""
        # Database connection pool
        if 'database' in self.config.connection_pools:
            db_config = self.config.connection_pools['database']
            self.connection_pools['database'] = DatabaseConnectionPool(db_config)

        # Redis connection pool
        if 'redis' in self.config.connection_pools:
            redis_config = self.config.connection_pools['redis']
            self.connection_pools['redis'] = RedisConnectionPool(redis_config)

        # HTTP connection pool
        if 'http' in self.config.connection_pools:
            http_config = self.config.connection_pools['http']
            self.connection_pools['http'] = HTTPConnectionPool(http_config)

        # Initialize pool managers
        for pool_name, pool in self.connection_pools.items():
            await pool.initialize()
            self.pool_managers[pool_name] = PoolManager(pool)

    async def _initialize_cache_layers(self):
        """Initialize multi-level caching."""
        # L1 Memory Cache
        if CacheLevel.L1_MEMORY in self.config.cache_configs:
            l1_config = self.config.cache_configs[CacheLevel.L1_MEMORY]
            self.cache_layers[CacheLevel.L1_MEMORY] = MemoryCacheLayer(l1_config)

        # L2 Redis Cache
        if CacheLevel.L2_REDIS in self.config.cache_configs:
            l2_config = self.config.cache_configs[CacheLevel.L2_REDIS]
            self.cache_layers[CacheLevel.L2_REDIS] = RedisCacheLayer(
                l2_config,
                self.connection_pools.get('redis')
            )

        # Initialize cache manager
        self.cache_manager = CacheManager(self.cache_layers)
        await self.cache_manager.initialize()

    async def _initialize_request_processing(self):
        """Initialize request processing components."""
        self.request_processor = RequestProcessor(
            cache_manager=self.cache_manager,
            connection_pools=self.connection_pools,
            config=self.config
        )

        self.load_balancer = LoadBalancer(
            strategy="least_connections",
            health_check_interval=30
        )

        # Initialize circuit breakers for external services
        services = ['ai_models', 'vector_db', 'external_apis']
        for service in services:
            self.circuit_breakers[service] = CircuitBreaker(
                failure_threshold=self.config.circuit_breaker_threshold,
                recovery_timeout=60,
                expected_exception=Exception
            )

    async def _initialize_performance_monitoring(self):
        """Initialize performance monitoring systems."""
        self.performance_optimizer = PerformanceOptimizer(
            metrics=self.metrics,
            cache_manager=self.cache_manager,
            pool_managers=self.pool_managers
        )

        self.resource_monitor = ResourceMonitor(
            monitoring_interval=self.config.performance_monitoring_interval
        )

    async def process_request(self, request: 'Request',
                            priority: RequestPriority = RequestPriority.NORMAL) -> Dict[str, Any]:
        """Process incoming request with high-performance optimizations."""
        start_time = time.time()
        request_id = request.headers.get('X-Request-ID', 'unknown')

        try:
            # Update concurrent user count
            self.concurrent_users_gauge.inc()
            self.metrics.concurrent_users += 1

            # Apply rate limiting if enabled
            if self.config.rate_limiting_enabled:
                await self._apply_rate_limiting(request)

            # Process request through performance-optimized pipeline
            result = await self.request_processor.process(request, priority)

            # Update metrics
            response_time = (time.time() - start_time) * 1000  # Convert to ms
            self.response_time_histogram.observe(response_time / 1000)  # Prometheus expects seconds
            self.request_counter.labels(
                endpoint=request.path,
                status='success'
            ).inc()

            # Update performance metrics
            self._update_performance_metrics(response_time, True)

            return {
                "request_id": request_id,
                "result": result,
                "response_time_ms": response_time,
                "cache_hit": result.get("cache_hit", False),
                "processing_tier": result.get("tier", PerformanceTier.STANDARD.value),
                "timestamp": datetime.utcnow().isoformat()
            }

        except Exception as e:
            # Handle errors and update metrics
            response_time = (time.time() - start_time) * 1000
            self.request_counter.labels(
                endpoint=request.path,
                status='error'
            ).inc()

            self._update_performance_metrics(response_time, False)

            logging.error(f"Request processing failed: {str(e)}",
                         extra={"request_id": request_id})

            return {
                "request_id": request_id,
                "error": str(e),
                "response_time_ms": response_time,
                "timestamp": datetime.utcnow().isoformat()
            }

        finally:
            # Decrement concurrent user count
            self.concurrent_users_gauge.dec()
            self.metrics.concurrent_users = max(0, self.metrics.concurrent_users - 1)

    async def get_performance_status(self) -> Dict[str, Any]:
        """Get comprehensive performance status."""
        # Calculate current performance metrics
        cache_hit_rates = {}
        for level, cache_layer in self.cache_layers.items():
            cache_hit_rates[level.value] = await cache_layer.get_hit_rate()

        pool_utilizations = {}
        for pool_name, pool_manager in self.pool_managers.items():
            pool_utilizations[pool_name] = await pool_manager.get_utilization()

        return {
            "performance_metrics": {
                "concurrent_users": self.metrics.concurrent_users,
                "average_response_time_ms": self.metrics.response_time_ms,
                "cache_hit_rate": self.metrics.cache_hit_rate,
                "throughput_rps": self.metrics.throughput_rps,
                "error_rate": self.metrics.error_rate,
                "cpu_utilization": self.metrics.cpu_utilization,
                "memory_usage_mb": self.metrics.memory_usage_mb
            },
            "cache_performance": {
                "hit_rates_by_level": cache_hit_rates,
                "total_hit_rate": self.metrics.cache_hit_rate,
                "cache_efficiency": await self.cache_manager.get_efficiency_metrics()
            },
            "connection_pools": {
                "utilizations": pool_utilizations,
                "health_status": await self._get_pool_health_status(),
                "performance_impact": await self._calculate_pool_performance_impact()
            },
            "scaling_status": {
                "current_capacity": await self._get_current_capacity(),
                "scaling_recommendations": await self.performance_optimizer.get_scaling_recommendations(),
                "resource_utilization": await self.resource_monitor.get_utilization_summary()
            },
            "performance_targets": {
                "response_time_target_ms": 200,
                "response_time_compliant": self.metrics.response_time_ms <= 200,
                "concurrent_users_target": 10000,
                "concurrent_users_compliant": self.metrics.concurrent_users <= 10000,
                "cache_hit_rate_target": 0.85,
                "cache_hit_rate_compliant": self.metrics.cache_hit_rate >= 0.85
            },
            "last_updated": datetime.utcnow().isoformat()
        }

    async def _apply_rate_limiting(self, request: 'Request'):
        """Apply rate limiting based on request characteristics."""
        # Get client identifier
        client_id = request.headers.get('X-Client-ID', request.remote_addr)

        # Check rate limits through cache layer
        rate_limit_key = f"rate_limit:{client_id}"
        current_requests = await self.cache_manager.get(rate_limit_key, CacheLevel.L2_REDIS)

        if current_requests and int(current_requests) > 1000:  # 1000 requests per minute
            raise Exception("Rate limit exceeded")

        # Increment request count
        await self.cache_manager.increment(rate_limit_key, CacheLevel.L2_REDIS, ttl=60)

    def _update_performance_metrics(self, response_time_ms: float, success: bool):
        """Update internal performance metrics."""
        self.metrics.request_count += 1

        # Update response time (rolling average)
        if self.metrics.request_count == 1:
            self.metrics.response_time_ms = response_time_ms
        else:
            alpha = 0.1  # Smoothing factor
            self.metrics.response_time_ms = (
                alpha * response_time_ms +
                (1 - alpha) * self.metrics.response_time_ms
            )

        # Update error rate
        if not success:
            error_count = getattr(self, '_error_count', 0) + 1
            setattr(self, '_error_count', error_count)
            self.metrics.error_rate = error_count / self.metrics.request_count

        # Update throughput (requests per second)
        current_time = time.time()
        if not hasattr(self, '_last_throughput_calculation'):
            self._last_throughput_calculation = current_time
            self._request_count_for_throughput = 1
        else:
            time_diff = current_time - self._last_throughput_calculation
            if time_diff >= 1.0:  # Calculate every second
                self.metrics.throughput_rps = self._request_count_for_throughput / time_diff
                self._last_throughput_calculation = current_time
                self._request_count_for_throughput = 1
            else:
                self._request_count_for_throughput += 1

    async def _performance_monitoring_loop(self):
        """Background task for continuous performance monitoring."""
        while True:
            try:
                # Update cache hit rates
                total_hits = 0
                total_requests = 0

                for level, cache_layer in self.cache_layers.items():
                    hit_rate = await cache_layer.get_hit_rate()
                    hits = await cache_layer.get_hit_count()
                    requests = await cache_layer.get_request_count()

                    total_hits += hits
                    total_requests += requests

                    # Update Prometheus metrics
                    self.cache_hit_rate_gauge.labels(level=level.value).set(hit_rate)

                # Update overall cache hit rate
                if total_requests > 0:
                    self.metrics.cache_hit_rate = total_hits / total_requests

                # Update connection pool utilization
                total_utilization = 0
                active_pools = 0

                for pool_name, pool_manager in self.pool_managers.items():
                    utilization = await pool_manager.get_utilization()
                    if utilization >= 0:
                        total_utilization += utilization
                        active_pools += 1

                if active_pools > 0:
                    self.metrics.connection_pool_utilization = total_utilization / active_pools

                # Update resource utilization
                resource_stats = await self.resource_monitor.get_current_stats()
                self.metrics.cpu_utilization = resource_stats.get('cpu_percent', 0.0)
                self.metrics.memory_usage_mb = resource_stats.get('memory_mb', 0.0)

                await asyncio.sleep(self.config.performance_monitoring_interval)

            except Exception as e:
                logging.error(f"Performance monitoring error: {str(e)}")
                await asyncio.sleep(10)  # Wait before retrying

    async def _connection_pool_health_check(self):
        """Background task for connection pool health monitoring."""
        while True:
            try:
                for pool_name, pool in self.connection_pools.items():
                    health_status = await pool.health_check()
                    if not health_status['healthy']:
                        logging.warning(
                            f"Connection pool {pool_name} unhealthy: {health_status['reason']}"
                        )

                        # Attempt pool recovery
                        await pool.recover()

                await asyncio.sleep(30)  # Health check every 30 seconds

            except Exception as e:
                logging.error(f"Connection pool health check error: {str(e)}")
                await asyncio.sleep(10)

    async def _cache_maintenance_loop(self):
        """Background task for cache maintenance."""
        while True:
            try:
                if self.cache_manager:
                    await self.cache_manager.perform_maintenance()

                await asyncio.sleep(300)  # Cache maintenance every 5 minutes

            except Exception as e:
                logging.error(f"Cache maintenance error: {str(e)}")
                await asyncio.sleep(60)

    async def _metrics_collection_loop(self):
        """Background task for metrics collection and aggregation."""
        while True:
            try:
                # Collect and aggregate metrics from all components
                await self._collect_performance_metrics()

                # Trigger performance optimization if needed
                if self.performance_optimizer:
                    await self.performance_optimizer.optimize_if_needed()

                await asyncio.sleep(60)  # Collect metrics every minute

            except Exception as e:
                logging.error(f"Metrics collection error: {str(e)}")
                await asyncio.sleep(30)
```

2. **Multi-Level Caching System** (`app/performance/caching.py`):
```python
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, List, Union, Set
import asyncio
import time
import json
import pickle
import lz4.frame
import zstandard as zstd
from dataclasses import dataclass
import weakref
import logging
from enum import Enum
import orjson

class SerializationFormat(Enum):
    JSON = "json"
    ORJSON = "orjson"
    PICKLE = "pickle"
    MSGPACK = "msgpack"

class EvictionPolicy(Enum):
    LRU = "lru"
    LFU = "lfu"
    TTL = "ttl"
    FIFO = "fifo"

@dataclass
class CacheStats:
    hits: int = 0
    misses: int = 0
    evictions: int = 0
    memory_usage: int = 0
    hit_rate: float = 0.0

class BaseCacheLayer(ABC):
    """Base class for cache layers."""

    @abstractmethod
    async def get(self, key: str) -> Optional[Any]:
        """Get value from cache."""
        pass

    @abstractmethod
    async def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """Set value in cache."""
        pass

    @abstractmethod
    async def delete(self, key: str) -> bool:
        """Delete value from cache."""
        pass

    @abstractmethod
    async def exists(self, key: str) -> bool:
        """Check if key exists in cache."""
        pass

    @abstractmethod
    async def get_stats(self) -> CacheStats:
        """Get cache statistics."""
        pass

    @abstractmethod
    async def clear(self) -> bool:
        """Clear all cache entries."""
        pass

class MemoryCacheLayer(BaseCacheLayer):
    """High-performance in-memory cache layer (L1)."""

    def __init__(self, config: CacheConfig):
        self.config = config
        self.cache: Dict[str, Dict[str, Any]] = {}
        self.access_times: Dict[str, float] = {}
        self.access_counts: Dict[str, int] = {}
        self.stats = CacheStats()
        self.lock = asyncio.Lock()

        # Configure serialization
        self.serializer = self._get_serializer(config.serialization_format)
        self.compressor = self._get_compressor() if config.compression_enabled else None

    def _get_serializer(self, format_type: str):
        """Get serialization handler."""
        if format_type == SerializationFormat.ORJSON.value:
            return {
                'dumps': lambda x: orjson.dumps(x).decode('utf-8'),
                'loads': orjson.loads
            }
        elif format_type == SerializationFormat.JSON.value:
            return {
                'dumps': json.dumps,
                'loads': json.loads
            }
        elif format_type == SerializationFormat.PICKLE.value:
            return {
                'dumps': pickle.dumps,
                'loads': pickle.loads
            }
        else:
            return {
                'dumps': str,
                'loads': lambda x: x
            }

    def _get_compressor(self):
        """Get compression handler."""
        return {
            'compress': lz4.frame.compress,
            'decompress': lz4.frame.decompress
        }

    async def get(self, key: str) -> Optional[Any]:
        """Get value from memory cache."""
        async with self.lock:
            if key in self.cache:
                entry = self.cache[key]

                # Check TTL
                if entry['expires_at'] and time.time() > entry['expires_at']:
                    await self._remove_entry(key)
                    self.stats.misses += 1
                    return None

                # Update access tracking
                self.access_times[key] = time.time()
                self.access_counts[key] = self.access_counts.get(key, 0) + 1

                self.stats.hits += 1
                self._update_hit_rate()

                # Decompress and deserialize if needed
                value = entry['value']
                if self.compressor and entry.get('compressed', False):
                    value = self.compressor['decompress'](value)
                    value = self.serializer['loads'](value)

                return value
            else:
                self.stats.misses += 1
                self._update_hit_rate()
                return None

    async def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """Set value in memory cache."""
        async with self.lock:
            try:
                # Serialize and compress if configured
                processed_value = value
                compressed = False

                if self.compressor:
                    serialized = self.serializer['dumps'](value)
                    if isinstance(serialized, str):
                        serialized = serialized.encode('utf-8')
                    processed_value = self.compressor['compress'](serialized)
                    compressed = True

                # Calculate expiration time
                expires_at = None
                if ttl:
                    expires_at = time.time() + ttl
                elif self.config.ttl_seconds:
                    expires_at = time.time() + self.config.ttl_seconds

                # Store entry
                entry = {
                    'value': processed_value,
                    'created_at': time.time(),
                    'expires_at': expires_at,
                    'compressed': compressed,
                    'size': len(str(processed_value))
                }

                self.cache[key] = entry
                self.access_times[key] = time.time()
                self.access_counts[key] = 1

                # Check memory limits and evict if necessary
                await self._enforce_memory_limits()

                return True

            except Exception as e:
                logging.error(f"Memory cache set error: {str(e)}")
                return False

    async def delete(self, key: str) -> bool:
        """Delete value from memory cache."""
        async with self.lock:
            return await self._remove_entry(key)

    async def exists(self, key: str) -> bool:
        """Check if key exists in memory cache."""
        async with self.lock:
            if key in self.cache:
                entry = self.cache[key]
                if entry['expires_at'] and time.time() > entry['expires_at']:
                    await self._remove_entry(key)
                    return False
                return True
            return False

    async def get_stats(self) -> CacheStats:
        """Get memory cache statistics."""
        async with self.lock:
            # Calculate memory usage
            total_size = sum(
                entry.get('size', 0) for entry in self.cache.values()
            )

            self.stats.memory_usage = total_size
            return self.stats

    async def clear(self) -> bool:
        """Clear all memory cache entries."""
        async with self.lock:
            self.cache.clear()
            self.access_times.clear()
            self.access_counts.clear()
            self.stats = CacheStats()
            return True

    async def _remove_entry(self, key: str) -> bool:
        """Remove entry from cache."""
        if key in self.cache:
            del self.cache[key]
            self.access_times.pop(key, None)
            self.access_counts.pop(key, None)
            self.stats.evictions += 1
            return True
        return False

    async def _enforce_memory_limits(self):
        """Enforce memory limits and evict entries if necessary."""
        if self.config.max_memory_mb <= 0:
            return

        max_bytes = self.config.max_memory_mb * 1024 * 1024
        current_size = sum(entry.get('size', 0) for entry in self.cache.values())

        if current_size <= max_bytes:
            return

        # Evict entries based on policy
        if self.config.eviction_policy == EvictionPolicy.LRU.value:
            await self._evict_lru()
        elif self.config.eviction_policy == EvictionPolicy.LFU.value:
            await self._evict_lfu()
        elif self.config.eviction_policy == EvictionPolicy.TTL.value:
            await self._evict_ttl()
        else:
            await self._evict_fifo()

    async def _evict_lru(self):
        """Evict least recently used entries."""
        # Sort by access time
        sorted_keys = sorted(
            self.access_times.items(),
            key=lambda x: x[1]
        )

        # Remove oldest 25% of entries
        evict_count = max(1, len(sorted_keys) // 4)
        for key, _ in sorted_keys[:evict_count]:
            await self._remove_entry(key)

    async def _evict_lfu(self):
        """Evict least frequently used entries."""
        # Sort by access count
        sorted_keys = sorted(
            self.access_counts.items(),
            key=lambda x: x[1]
        )

        # Remove least used 25% of entries
        evict_count = max(1, len(sorted_keys) // 4)
        for key, _ in sorted_keys[:evict_count]:
            await self._remove_entry(key)

    async def _evict_ttl(self):
        """Evict expired entries."""
        current_time = time.time()
        expired_keys = [
            key for key, entry in self.cache.items()
            if entry['expires_at'] and current_time > entry['expires_at']
        ]

        for key in expired_keys:
            await self._remove_entry(key)

    async def _evict_fifo(self):
        """Evict first-in-first-out entries."""
        # Sort by creation time
        sorted_keys = sorted(
            self.cache.items(),
            key=lambda x: x[1]['created_at']
        )

        # Remove oldest 25% of entries
        evict_count = max(1, len(sorted_keys) // 4)
        for key, _ in sorted_keys[:evict_count]:
            await self._remove_entry(key)

    def _update_hit_rate(self):
        """Update cache hit rate."""
        total_requests = self.stats.hits + self.stats.misses
        if total_requests > 0:
            self.stats.hit_rate = self.stats.hits / total_requests

    async def get_hit_rate(self) -> float:
        """Get current hit rate."""
        return self.stats.hit_rate

    async def get_hit_count(self) -> int:
        """Get hit count."""
        return self.stats.hits

    async def get_request_count(self) -> int:
        """Get total request count."""
        return self.stats.hits + self.stats.misses

class CacheManager:
    """Manages multiple cache layers with intelligent routing."""

    def __init__(self, cache_layers: Dict[CacheLevel, BaseCacheLayer]):
        self.cache_layers = cache_layers
        self.routing_strategy = "hierarchical"  # hierarchical, parallel, adaptive
        self.cache_warming_enabled = True

    async def initialize(self):
        """Initialize cache manager."""
        logging.info(f"Initialized cache manager with {len(self.cache_layers)} layers")

        if self.cache_warming_enabled:
            asyncio.create_task(self._warm_caches())

    async def get(self, key: str, preferred_level: Optional[CacheLevel] = None) -> Optional[Any]:
        """Get value from cache with hierarchical lookup."""
        if preferred_level and preferred_level in self.cache_layers:
            return await self.cache_layers[preferred_level].get(key)

        # Hierarchical lookup (L1 -> L2 -> L3)
        levels = [CacheLevel.L1_MEMORY, CacheLevel.L2_REDIS, CacheLevel.L3_DATABASE]

        for level in levels:
            if level in self.cache_layers:
                value = await self.cache_layers[level].get(key)
                if value is not None:
                    # Cache warming: populate higher levels
                    await self._warm_higher_levels(key, value, level)
                    return value

        return None

    async def set(self, key: str, value: Any, ttl: Optional[int] = None,
                 levels: Optional[List[CacheLevel]] = None) -> bool:
        """Set value in specified cache levels."""
        if levels is None:
            levels = list(self.cache_layers.keys())

        success = True
        for level in levels:
            if level in self.cache_layers:
                result = await self.cache_layers[level].set(key, value, ttl)
                success = success and result

        return success

    async def delete(self, key: str, levels: Optional[List[CacheLevel]] = None) -> bool:
        """Delete value from specified cache levels."""
        if levels is None:
            levels = list(self.cache_layers.keys())

        success = True
        for level in levels:
            if level in self.cache_layers:
                result = await self.cache_layers[level].delete(key)
                success = success and result

        return success

    async def increment(self, key: str, level: CacheLevel,
                       amount: int = 1, ttl: Optional[int] = None) -> int:
        """Increment a numeric value in cache."""
        if level not in self.cache_layers:
            raise ValueError(f"Cache level {level} not available")

        cache_layer = self.cache_layers[level]
        current_value = await cache_layer.get(key)

        if current_value is None:
            new_value = amount
        else:
            new_value = int(current_value) + amount

        await cache_layer.set(key, new_value, ttl)
        return new_value

    async def get_efficiency_metrics(self) -> Dict[str, Any]:
        """Get cache efficiency metrics across all levels."""
        metrics = {}

        for level, cache_layer in self.cache_layers.items():
            stats = await cache_layer.get_stats()
            metrics[level.value] = {
                "hit_rate": stats.hit_rate,
                "hits": stats.hits,
                "misses": stats.misses,
                "evictions": stats.evictions,
                "memory_usage": stats.memory_usage
            }

        return metrics

    async def perform_maintenance(self):
        """Perform maintenance tasks on all cache layers."""
        for level, cache_layer in self.cache_layers.items():
            try:
                if hasattr(cache_layer, 'perform_maintenance'):
                    await cache_layer.perform_maintenance()
            except Exception as e:
                logging.error(f"Cache maintenance error for {level}: {str(e)}")

    async def _warm_higher_levels(self, key: str, value: Any, source_level: CacheLevel):
        """Warm cache levels higher than the source level."""
        if not self.cache_warming_enabled:
            return

        level_order = [CacheLevel.L1_MEMORY, CacheLevel.L2_REDIS, CacheLevel.L3_DATABASE]
        source_index = level_order.index(source_level)

        # Warm all levels above the source
        for i in range(source_index):
            target_level = level_order[i]
            if target_level in self.cache_layers:
                try:
                    await self.cache_layers[target_level].set(key, value)
                except Exception as e:
                    logging.error(f"Cache warming error for {target_level}: {str(e)}")

    async def _warm_caches(self):
        """Background task for cache warming."""
        # Implementation for pre-loading frequently accessed data
        # This would be customized based on application patterns
        pass
```

3. **Connection Pool Management** (`app/performance/connection_pools.py`):
```python
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, List, Set, AsyncContextManager
import asyncio
import time
import logging
import weakref
from dataclasses import dataclass
from enum import Enum
import asyncpg
import aioredis
import aiohttp
from contextlib import asynccontextmanager

class PoolStatus(Enum):
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    UNHEALTHY = "unhealthy"
    RECOVERING = "recovering"

@dataclass
class PoolMetrics:
    total_connections: int = 0
    active_connections: int = 0
    idle_connections: int = 0
    utilization_rate: float = 0.0
    average_acquisition_time: float = 0.0
    total_acquisitions: int = 0
    failed_acquisitions: int = 0
    status: PoolStatus = PoolStatus.HEALTHY

class BaseConnectionPool(ABC):
    """Base class for connection pools."""

    @abstractmethod
    async def initialize(self) -> bool:
        """Initialize the connection pool."""
        pass

    @abstractmethod
    async def get_connection(self) -> AsyncContextManager[Any]:
        """Get a connection from the pool."""
        pass

    @abstractmethod
    async def health_check(self) -> Dict[str, Any]:
        """Perform health check on the pool."""
        pass

    @abstractmethod
    async def get_metrics(self) -> PoolMetrics:
        """Get pool metrics."""
        pass

    @abstractmethod
    async def recover(self) -> bool:
        """Recover from unhealthy state."""
        pass

    @abstractmethod
    async def close(self) -> bool:
        """Close the connection pool."""
        pass

class DatabaseConnectionPool(BaseConnectionPool):
    """High-performance PostgreSQL connection pool."""

    def __init__(self, config: ConnectionPoolConfig):
        self.config = config
        self.pool: Optional[asyncpg.Pool] = None
        self.metrics = PoolMetrics()
        self.acquisition_times: List[float] = []
        self.last_health_check = 0
        self.health_check_lock = asyncio.Lock()

    async def initialize(self) -> bool:
        """Initialize PostgreSQL connection pool."""
        try:
            self.pool = await asyncpg.create_pool(
                dsn=self._get_database_url(),
                min_size=self.config.min_size,
                max_size=self.config.max_size,
                max_queries=50000,  # Maximum queries per connection
                max_inactive_connection_lifetime=self.config.max_idle_time,
                timeout=self.config.connection_timeout,
                command_timeout=self.config.acquire_timeout,
                server_settings={
                    'application_name': 'mobius_context_engine',
                    'tcp_keepalives_idle': '600',
                    'tcp_keepalives_interval': '30',
                    'tcp_keepalives_count': '3',
                }
            )

            self.metrics.status = PoolStatus.HEALTHY
            logging.info(f"Database connection pool initialized: {self.config.min_size}-{self.config.max_size}")
            return True

        except Exception as e:
            logging.error(f"Database pool initialization failed: {str(e)}")
            self.metrics.status = PoolStatus.UNHEALTHY
            return False

    async def get_connection(self) -> AsyncContextManager[asyncpg.Connection]:
        """Get database connection with performance tracking."""
        if not self.pool:
            raise Exception("Database pool not initialized")

        start_time = time.time()

        try:
            @asynccontextmanager
            async def connection_context():
                async with self.pool.acquire() as conn:
                    # Track acquisition time
                    acquisition_time = time.time() - start_time
                    self.acquisition_times.append(acquisition_time)

                    # Keep only recent acquisition times for averaging
                    if len(self.acquisition_times) > 1000:
                        self.acquisition_times = self.acquisition_times[-1000:]

                    self.metrics.total_acquisitions += 1
                    yield conn

            return connection_context()

        except Exception as e:
            self.metrics.failed_acquisitions += 1
            logging.error(f"Database connection acquisition failed: {str(e)}")
            raise

    async def health_check(self) -> Dict[str, Any]:
        """Perform database pool health check."""
        async with self.health_check_lock:
            current_time = time.time()

            # Rate limit health checks
            if current_time - self.last_health_check < 10:
                return {"healthy": self.metrics.status == PoolStatus.HEALTHY}

            self.last_health_check = current_time

            try:
                if not self.pool:
                    return {
                        "healthy": False,
                        "reason": "Pool not initialized",
                        "status": PoolStatus.UNHEALTHY.value
                    }

                # Test connection acquisition and simple query
                async with self.pool.acquire() as conn:
                    await conn.fetchval("SELECT 1")

                # Update pool metrics
                await self._update_metrics()

                # Determine health status
                if self.metrics.utilization_rate > 0.95:
                    self.metrics.status = PoolStatus.DEGRADED
                elif self.metrics.failed_acquisitions / max(1, self.metrics.total_acquisitions) > 0.1:
                    self.metrics.status = PoolStatus.DEGRADED
                else:
                    self.metrics.status = PoolStatus.HEALTHY

                return {
                    "healthy": self.metrics.status in [PoolStatus.HEALTHY, PoolStatus.DEGRADED],
                    "status": self.metrics.status.value,
                    "metrics": {
                        "utilization_rate": self.metrics.utilization_rate,
                        "active_connections": self.metrics.active_connections,
                        "total_connections": self.metrics.total_connections,
                        "average_acquisition_time": self.metrics.average_acquisition_time
                    }
                }

            except Exception as e:
                self.metrics.status = PoolStatus.UNHEALTHY
                return {
                    "healthy": False,
                    "reason": str(e),
                    "status": PoolStatus.UNHEALTHY.value
                }

    async def get_metrics(self) -> PoolMetrics:
        """Get current pool metrics."""
        await self._update_metrics()
        return self.metrics

    async def recover(self) -> bool:
        """Recover from unhealthy state."""
        try:
            if self.pool:
                await self.pool.close()

            self.metrics.status = PoolStatus.RECOVERING
            success = await self.initialize()

            if success:
                self.metrics.status = PoolStatus.HEALTHY
                logging.info("Database pool recovery successful")
            else:
                self.metrics.status = PoolStatus.UNHEALTHY
                logging.error("Database pool recovery failed")

            return success

        except Exception as e:
            logging.error(f"Database pool recovery error: {str(e)}")
            self.metrics.status = PoolStatus.UNHEALTHY
            return False

    async def close(self) -> bool:
        """Close database connection pool."""
        try:
            if self.pool:
                await self.pool.close()
                self.pool = None

            self.metrics.status = PoolStatus.UNHEALTHY
            return True

        except Exception as e:
            logging.error(f"Database pool close error: {str(e)}")
            return False

    async def _update_metrics(self):
        """Update pool metrics."""
        if not self.pool:
            return

        # Get pool size information
        self.metrics.total_connections = self.pool.get_size()
        self.metrics.idle_connections = self.pool.get_idle_size()
        self.metrics.active_connections = self.metrics.total_connections - self.metrics.idle_connections

        # Calculate utilization rate
        if self.metrics.total_connections > 0:
            self.metrics.utilization_rate = self.metrics.active_connections / self.metrics.total_connections

        # Calculate average acquisition time
        if self.acquisition_times:
            self.metrics.average_acquisition_time = sum(self.acquisition_times) / len(self.acquisition_times)

    def _get_database_url(self) -> str:
        """Get database connection URL from environment."""
        # This would typically come from environment variables
        return "postgresql://user:password@localhost:5432/mobius"

class PoolManager:
    """Manages multiple connection pools with performance optimization."""

    def __init__(self, pool: BaseConnectionPool):
        self.pool = pool
        self.performance_history: List[Dict[str, Any]] = []
        self.optimization_enabled = True

    async def get_utilization(self) -> float:
        """Get pool utilization rate."""
        metrics = await self.pool.get_metrics()
        return metrics.utilization_rate

    async def get_performance_impact(self) -> Dict[str, Any]:
        """Calculate performance impact of pool operations."""
        metrics = await self.pool.get_metrics()

        return {
            "acquisition_performance": {
                "average_time_ms": metrics.average_acquisition_time * 1000,
                "acquisition_success_rate": (
                    (metrics.total_acquisitions - metrics.failed_acquisitions) /
                    max(1, metrics.total_acquisitions)
                )
            },
            "utilization_efficiency": {
                "current_utilization": metrics.utilization_rate,
                "optimal_range": (0.7, 0.9),
                "efficiency_score": self._calculate_efficiency_score(metrics.utilization_rate)
            },
            "health_status": {
                "status": metrics.status.value,
                "active_connections": metrics.active_connections,
                "total_connections": metrics.total_connections
            }
        }

    def _calculate_efficiency_score(self, utilization: float) -> float:
        """Calculate efficiency score based on utilization."""
        if 0.7 <= utilization <= 0.9:
            return 1.0  # Optimal range
        elif utilization < 0.7:
            return utilization / 0.7  # Under-utilized
        else:
            return max(0.0, 1.0 - (utilization - 0.9) / 0.1)  # Over-utilized
```

## Dependencies
- Task 008: Async Database Operations
- Task 025: Redis Integration
- Task 043: Real-time Performance Monitoring System
- High-performance Python libraries (uvloop, orjson, lz4, aiohttp, asyncpg)
- Prometheus for metrics collection
- Kubernetes HPA and VPA for autoscaling
- Load balancing solutions (NGINX, HAProxy)

## Estimated Time
50-60 hours

## Required Skills
- High-performance Python development
- Kubernetes autoscaling and performance optimization
- Connection pooling and caching strategies
- Load balancing and traffic management
- Performance monitoring and optimization
- Async programming and concurrency patterns
- Memory management and resource optimization
- Distributed systems performance tuning
