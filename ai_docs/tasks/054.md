# Task 054: Advanced Analytics and API Rate Limiting System

## Overview
Implement a comprehensive advanced analytics platform with intelligent API rate limiting system that provides real-time usage tracking, predictive analytics, business intelligence dashboards, and sophisticated rate limiting mechanisms. This system will enable data-driven decision making, prevent abuse, and ensure fair resource allocation across enterprise customers while maintaining optimal performance.

## Success Criteria
- [ ] Advanced analytics pipeline processes 100k+ events per second with <50ms latency
- [ ] Real-time dashboards update within 5 seconds with 99.9% accuracy
- [ ] API rate limiting prevents abuse while maintaining 99.95% legitimate request success rate
- [ ] Business intelligence reports generate actionable insights with 95%+ accuracy
- [ ] Predictive analytics forecasts usage patterns with 85%+ accuracy
- [ ] Multi-tier rate limiting supports enterprise quotas and SLA requirements

## Test First Approach

### Tests to Write BEFORE Implementation:

1. **Analytics Pipeline Tests** (`tests/backend/unit/test_analytics_pipeline.py`):
```python
def test_high_volume_event_processing():
    """Test analytics pipeline with 100k+ events per second."""
    # Test event ingestion rate and throughput
    # Test event processing latency <50ms
    # Test data pipeline integrity under load
    # Test event deduplication and validation
    # Test pipeline recovery from failures

def test_real_time_analytics():
    """Test real-time analytics processing and aggregation."""
    # Test streaming data aggregation accuracy
    # Test real-time metric calculation
    # Test windowed analytics processing
    # Test anomaly detection in real-time
    # Test dashboard update latency <5 seconds

def test_analytics_data_integrity():
    """Test analytics data accuracy and consistency."""
    # Test event data validation and schema enforcement
    # Test data consistency across pipeline stages
    # Test analytics metric accuracy
    # Test historical data preservation
    # Test data retention and archival policies
```

2. **Rate Limiting System Tests** (`tests/backend/unit/test_rate_limiting.py`):
```python
def test_multi_tier_rate_limiting():
    """Test multi-tier rate limiting implementation."""
    # Test global rate limits for platform stability
    # Test per-user rate limits for fair usage
    # Test per-organization enterprise quotas
    # Test per-endpoint specific rate limits
    # Test rate limit inheritance and overrides

def test_intelligent_rate_limiting():
    """Test intelligent rate limiting algorithms."""
    # Test sliding window rate limiting accuracy
    # Test token bucket algorithm implementation
    # Test burst handling and smoothing
    # Test adaptive rate limiting based on load
    # Test rate limit bypass for critical operations

def test_rate_limit_enforcement():
    """Test rate limit enforcement and response handling."""
    # Test rate limit violation detection
    # Test proper HTTP 429 responses
    # Test rate limit header generation
    # Test rate limit bypass mechanisms
    # Test rate limit appeal and override processes
```

3. **Business Intelligence Tests** (`tests/backend/unit/test_business_intelligence.py`):
```python
def test_usage_analytics():
    """Test comprehensive usage analytics collection."""
    # Test user behavior tracking accuracy
    # Test feature usage analytics
    # Test performance metrics collection
    # Test error rate and pattern analysis
    # Test customer journey analytics

def test_predictive_analytics():
    """Test predictive analytics and forecasting."""
    # Test usage pattern prediction accuracy 85%+
    # Test capacity planning forecasts
    # Test anomaly prediction and alerting
    # Test seasonal pattern recognition
    # Test model accuracy and retraining

def test_business_reporting():
    """Test business intelligence reporting system."""
    # Test automated report generation
    # Test custom dashboard creation
    # Test drill-down analytics capabilities
    # Test export and sharing functionality
    # Test report performance optimization
```

## Implementation Details

### 1. Advanced Analytics Pipeline (`app/analytics/pipeline.py`):
```python
import asyncio
import time
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
import aioredis
import asyncpg
from fastapi import FastAPI, HTTPException
import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import logging

@dataclass
class AnalyticsEvent:
    """Analytics event data structure."""
    event_id: str
    event_type: str
    user_id: Optional[str]
    organization_id: Optional[str]
    timestamp: datetime
    properties: Dict[str, Any]
    session_id: Optional[str]
    ip_address: str
    user_agent: Optional[str]

class AdvancedAnalyticsPipeline:
    """Advanced analytics pipeline for real-time event processing."""

    def __init__(self, redis_url: str, postgres_url: str):
        self.redis_url = redis_url
        self.postgres_url = postgres_url
        self.redis_pool = None
        self.postgres_pool = None
        self.event_queue = asyncio.Queue(maxsize=10000)
        self.processing_stats = {
            'events_processed': 0,
            'events_per_second': 0,
            'average_latency': 0,
            'last_update': time.time()
        }
        self.anomaly_detector = IsolationForest(contamination=0.1)
        self.scaler = StandardScaler()

    async def initialize(self):
        """Initialize analytics pipeline connections."""
        try:
            # Initialize Redis connection pool
            self.redis_pool = aioredis.ConnectionPool.from_url(
                self.redis_url, max_connections=20
            )

            # Initialize PostgreSQL connection pool
            self.postgres_pool = await asyncpg.create_pool(
                self.postgres_url,
                min_size=5,
                max_size=20,
                command_timeout=60
            )

            # Start background processing tasks
            asyncio.create_task(self._process_events())
            asyncio.create_task(self._update_statistics())
            asyncio.create_task(self._detect_anomalies())

            logging.info("Analytics pipeline initialized successfully")

        except Exception as e:
            logging.error(f"Failed to initialize analytics pipeline: {e}")
            raise

    async def track_event(self, event: AnalyticsEvent) -> bool:
        """Track analytics event with high throughput."""
        try:
            start_time = time.time()

            # Validate event data
            if not self._validate_event(event):
                return False

            # Add to processing queue
            await self.event_queue.put(event)

            # Update processing metrics
            processing_time = time.time() - start_time
            await self._update_processing_metrics(processing_time)

            return True

        except Exception as e:
            logging.error(f"Error tracking event: {e}")
            return False

    async def _process_events(self):
        """Background task for processing analytics events."""
        batch_size = 100
        events_batch = []

        while True:
            try:
                # Collect events in batches for efficient processing
                while len(events_batch) < batch_size:
                    try:
                        event = await asyncio.wait_for(
                            self.event_queue.get(), timeout=1.0
                        )
                        events_batch.append(event)
                    except asyncio.TimeoutError:
                        break

                if events_batch:
                    await self._process_event_batch(events_batch)
                    events_batch.clear()

            except Exception as e:
                logging.error(f"Error processing events batch: {e}")
                events_batch.clear()

    async def _process_event_batch(self, events: List[AnalyticsEvent]):
        """Process batch of analytics events."""
        try:
            async with self.postgres_pool.acquire() as conn:
                # Insert events into database
                event_data = [
                    (
                        event.event_id,
                        event.event_type,
                        event.user_id,
                        event.organization_id,
                        event.timestamp,
                        event.properties,
                        event.session_id,
                        event.ip_address,
                        event.user_agent
                    )
                    for event in events
                ]

                await conn.executemany(
                    """
                    INSERT INTO analytics_events
                    (event_id, event_type, user_id, organization_id, timestamp,
                     properties, session_id, ip_address, user_agent)
                    VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
                    ON CONFLICT (event_id) DO NOTHING
                    """,
                    event_data
                )

            # Update real-time metrics in Redis
            await self._update_realtime_metrics(events)

            # Update processing statistics
            self.processing_stats['events_processed'] += len(events)

        except Exception as e:
            logging.error(f"Error processing event batch: {e}")
            raise

    async def _update_realtime_metrics(self, events: List[AnalyticsEvent]):
        """Update real-time metrics in Redis."""
        try:
            redis = aioredis.Redis(connection_pool=self.redis_pool)

            # Group events by type and organization
            metrics = {}
            for event in events:
                key = f"{event.organization_id}:{event.event_type}"
                if key not in metrics:
                    metrics[key] = 0
                metrics[key] += 1

            # Update Redis counters
            pipe = redis.pipeline()
            current_time = int(time.time())
            window_key = f"metrics:window:{current_time // 60}"  # 1-minute windows

            for key, count in metrics.items():
                pipe.hincrby(window_key, key, count)
                pipe.expire(window_key, 3600)  # Keep 1 hour of windows

            await pipe.execute()

        except Exception as e:
            logging.error(f"Error updating real-time metrics: {e}")

    def _validate_event(self, event: AnalyticsEvent) -> bool:
        """Validate analytics event data."""
        try:
            # Basic validation
            if not event.event_id or not event.event_type:
                return False

            # Timestamp validation
            if not event.timestamp:
                return False

            # Future timestamp check
            if event.timestamp > datetime.utcnow() + timedelta(minutes=5):
                return False

            # Properties validation
            if event.properties and not isinstance(event.properties, dict):
                return False

            return True

        except Exception:
            return False

    async def _update_processing_metrics(self, processing_time: float):
        """Update processing performance metrics."""
        try:
            current_time = time.time()
            time_delta = current_time - self.processing_stats['last_update']

            if time_delta >= 1.0:  # Update every second
                # Calculate events per second
                events_in_window = self.processing_stats['events_processed']
                self.processing_stats['events_per_second'] = events_in_window / time_delta

                # Update average latency
                self.processing_stats['average_latency'] = (
                    self.processing_stats['average_latency'] * 0.9 +
                    processing_time * 0.1
                )

                # Reset counters
                self.processing_stats['events_processed'] = 0
                self.processing_stats['last_update'] = current_time

        except Exception as e:
            logging.error(f"Error updating processing metrics: {e}")

    async def get_processing_stats(self) -> Dict[str, Any]:
        """Get current processing statistics."""
        return self.processing_stats.copy()

    async def get_usage_analytics(
        self,
        organization_id: str,
        start_time: datetime,
        end_time: datetime
    ) -> Dict[str, Any]:
        """Get usage analytics for organization."""
        try:
            async with self.postgres_pool.acquire() as conn:
                # Get basic usage metrics
                usage_data = await conn.fetch(
                    """
                    SELECT
                        event_type,
                        COUNT(*) as event_count,
                        COUNT(DISTINCT user_id) as unique_users,
                        COUNT(DISTINCT session_id) as unique_sessions
                    FROM analytics_events
                    WHERE organization_id = $1
                    AND timestamp BETWEEN $2 AND $3
                    GROUP BY event_type
                    ORDER BY event_count DESC
                    """,
                    organization_id, start_time, end_time
                )

                # Get hourly breakdown
                hourly_data = await conn.fetch(
                    """
                    SELECT
                        DATE_TRUNC('hour', timestamp) as hour,
                        COUNT(*) as event_count
                    FROM analytics_events
                    WHERE organization_id = $1
                    AND timestamp BETWEEN $2 AND $3
                    GROUP BY hour
                    ORDER BY hour
                    """,
                    organization_id, start_time, end_time
                )

                return {
                    'usage_by_type': [dict(row) for row in usage_data],
                    'hourly_usage': [dict(row) for row in hourly_data],
                    'total_events': sum(row['event_count'] for row in usage_data),
                    'time_range': {
                        'start': start_time.isoformat(),
                        'end': end_time.isoformat()
                    }
                }

        except Exception as e:
            logging.error(f"Error getting usage analytics: {e}")
            raise HTTPException(status_code=500, detail="Analytics query failed")
```

### 2. Multi-Tier Rate Limiting System (`app/rate_limiting/limiter.py`):
```python
import asyncio
import time
import json
from typing import Dict, List, Optional, Tuple, Any
from enum import Enum
from dataclasses import dataclass
import aioredis
from fastapi import Request, HTTPException
import logging

class RateLimitAlgorithm(Enum):
    """Rate limiting algorithm types."""
    SLIDING_WINDOW = "sliding_window"
    TOKEN_BUCKET = "token_bucket"
    FIXED_WINDOW = "fixed_window"
    LEAKY_BUCKET = "leaky_bucket"

@dataclass
class RateLimitRule:
    """Rate limit rule configuration."""
    name: str
    algorithm: RateLimitAlgorithm
    limit: int
    window_seconds: int
    burst_limit: Optional[int] = None
    priority: int = 0
    organization_id: Optional[str] = None
    user_id: Optional[str] = None
    endpoint_pattern: Optional[str] = None

@dataclass
class RateLimitResult:
    """Rate limit check result."""
    allowed: bool
    remaining: int
    reset_time: int
    retry_after: Optional[int] = None
    rule_name: str = ""
    current_usage: int = 0

class MultiTierRateLimiter:
    """Advanced multi-tier rate limiting system."""

    def __init__(self, redis_url: str):
        self.redis_url = redis_url
        self.redis_pool = None
        self.rules: Dict[str, RateLimitRule] = {}
        self.bypass_tokens: set = set()

    async def initialize(self):
        """Initialize rate limiter."""
        try:
            self.redis_pool = aioredis.ConnectionPool.from_url(
                self.redis_url, max_connections=20
            )

            # Load default rules
            await self._load_default_rules()

            logging.info("Rate limiter initialized successfully")

        except Exception as e:
            logging.error(f"Failed to initialize rate limiter: {e}")
            raise

    async def _load_default_rules(self):
        """Load default rate limiting rules."""
        default_rules = [
            # Global platform limits
            RateLimitRule(
                name="global_api_limit",
                algorithm=RateLimitAlgorithm.SLIDING_WINDOW,
                limit=1000,
                window_seconds=60,
                priority=1
            ),

            # Per-user limits
            RateLimitRule(
                name="user_api_limit",
                algorithm=RateLimitAlgorithm.TOKEN_BUCKET,
                limit=100,
                window_seconds=60,
                burst_limit=150,
                priority=2
            ),

            # Search endpoint specific limits
            RateLimitRule(
                name="search_endpoint_limit",
                algorithm=RateLimitAlgorithm.SLIDING_WINDOW,
                limit=50,
                window_seconds=60,
                endpoint_pattern="/api/v1/search/*",
                priority=3
            ),

            # Enterprise organization limits
            RateLimitRule(
                name="enterprise_org_limit",
                algorithm=RateLimitAlgorithm.LEAKY_BUCKET,
                limit=5000,
                window_seconds=60,
                priority=4
            )
        ]

        for rule in default_rules:
            self.rules[rule.name] = rule

    async def check_rate_limit(
        self,
        request: Request,
        user_id: Optional[str] = None,
        organization_id: Optional[str] = None
    ) -> RateLimitResult:
        """Check rate limits for incoming request."""
        try:
            # Get applicable rules for this request
            applicable_rules = self._get_applicable_rules(
                request, user_id, organization_id
            )

            # Check bypass tokens
            if self._check_bypass(request):
                return RateLimitResult(
                    allowed=True,
                    remaining=float('inf'),
                    reset_time=int(time.time()) + 3600,
                    rule_name="bypass"
                )

            # Check each rule (most restrictive wins)
            most_restrictive_result = None

            for rule in sorted(applicable_rules, key=lambda r: r.priority, reverse=True):
                result = await self._check_rule(rule, request, user_id, organization_id)

                if not result.allowed:
                    return result

                if (most_restrictive_result is None or
                    result.remaining < most_restrictive_result.remaining):
                    most_restrictive_result = result

            return most_restrictive_result or RateLimitResult(
                allowed=True,
                remaining=1000,
                reset_time=int(time.time()) + 3600
            )

        except Exception as e:
            logging.error(f"Error checking rate limit: {e}")
            # Fail open for availability
            return RateLimitResult(
                allowed=True,
                remaining=0,
                reset_time=int(time.time()) + 3600,
                rule_name="error_fallback"
            )

    def _get_applicable_rules(
        self,
        request: Request,
        user_id: Optional[str],
        organization_id: Optional[str]
    ) -> List[RateLimitRule]:
        """Get rules applicable to this request."""
        applicable_rules = []

        for rule in self.rules.values():
            # Check organization match
            if rule.organization_id and rule.organization_id != organization_id:
                continue

            # Check user match
            if rule.user_id and rule.user_id != user_id:
                continue

            # Check endpoint pattern match
            if rule.endpoint_pattern:
                if not self._matches_pattern(str(request.url.path), rule.endpoint_pattern):
                    continue

            applicable_rules.append(rule)

        return applicable_rules

    def _matches_pattern(self, path: str, pattern: str) -> bool:
        """Check if path matches pattern."""
        # Simple wildcard matching
        if pattern.endswith("*"):
            return path.startswith(pattern[:-1])
        return path == pattern

    def _check_bypass(self, request: Request) -> bool:
        """Check if request should bypass rate limiting."""
        # Check for bypass token in headers
        bypass_token = request.headers.get("X-Rate-Limit-Bypass")
        if bypass_token in self.bypass_tokens:
            return True

        # Check for internal service requests
        if request.headers.get("X-Internal-Service") == "true":
            return True

        return False

    async def _check_rule(
        self,
        rule: RateLimitRule,
        request: Request,
        user_id: Optional[str],
        organization_id: Optional[str]
    ) -> RateLimitResult:
        """Check specific rate limit rule."""
        # Generate key for this rule and request
        key = self._generate_key(rule, request, user_id, organization_id)

        # Apply algorithm-specific logic
        if rule.algorithm == RateLimitAlgorithm.SLIDING_WINDOW:
            return await self._check_sliding_window(rule, key)
        elif rule.algorithm == RateLimitAlgorithm.TOKEN_BUCKET:
            return await self._check_token_bucket(rule, key)
        elif rule.algorithm == RateLimitAlgorithm.FIXED_WINDOW:
            return await self._check_fixed_window(rule, key)
        elif rule.algorithm == RateLimitAlgorithm.LEAKY_BUCKET:
            return await self._check_leaky_bucket(rule, key)
        else:
            # Default to sliding window
            return await self._check_sliding_window(rule, key)

    def _generate_key(
        self,
        rule: RateLimitRule,
        request: Request,
        user_id: Optional[str],
        organization_id: Optional[str]
    ) -> str:
        """Generate Redis key for rate limit rule."""
        key_parts = [f"ratelimit:{rule.name}"]

        if organization_id:
            key_parts.append(f"org:{organization_id}")

        if user_id:
            key_parts.append(f"user:{user_id}")

        # Add IP as fallback identifier
        client_ip = request.client.host if request.client else "unknown"
        key_parts.append(f"ip:{client_ip}")

        return ":".join(key_parts)

    async def _check_sliding_window(self, rule: RateLimitRule, key: str) -> RateLimitResult:
        """Check sliding window rate limit."""
        try:
            redis = aioredis.Redis(connection_pool=self.redis_pool)
            current_time = time.time()
            window_start = current_time - rule.window_seconds

            pipe = redis.pipeline()

            # Remove old entries
            pipe.zremrangebyscore(key, 0, window_start)

            # Count current entries
            pipe.zcard(key)

            # Add current request
            pipe.zadd(key, {str(current_time): current_time})

            # Set expiration
            pipe.expire(key, rule.window_seconds)

            results = await pipe.execute()
            current_count = results[1]

            allowed = current_count < rule.limit
            remaining = max(0, rule.limit - current_count - 1)
            reset_time = int(current_time + rule.window_seconds)

            return RateLimitResult(
                allowed=allowed,
                remaining=remaining,
                reset_time=reset_time,
                retry_after=rule.window_seconds if not allowed else None,
                rule_name=rule.name,
                current_usage=current_count + 1
            )

        except Exception as e:
            logging.error(f"Error checking sliding window rate limit: {e}")
            return RateLimitResult(allowed=True, remaining=0, reset_time=0)

    async def _check_token_bucket(self, rule: RateLimitRule, key: str) -> RateLimitResult:
        """Check token bucket rate limit."""
        try:
            redis = aioredis.Redis(connection_pool=self.redis_pool)
            current_time = time.time()

            # Get current bucket state
            bucket_data = await redis.hmget(
                key, "tokens", "last_refill"
            )

            tokens = float(bucket_data[0] or rule.limit)
            last_refill = float(bucket_data[1] or current_time)

            # Calculate tokens to add based on time elapsed
            time_elapsed = current_time - last_refill
            tokens_to_add = time_elapsed * (rule.limit / rule.window_seconds)

            # Update token count (capped at burst limit or normal limit)
            max_tokens = rule.burst_limit or rule.limit
            tokens = min(max_tokens, tokens + tokens_to_add)

            # Check if request can be served
            allowed = tokens >= 1

            if allowed:
                tokens -= 1

            # Update bucket state
            await redis.hset(
                key,
                mapping={
                    "tokens": str(tokens),
                    "last_refill": str(current_time)
                }
            )
            await redis.expire(key, rule.window_seconds * 2)

            remaining = int(tokens)
            reset_time = int(current_time + rule.window_seconds)

            return RateLimitResult(
                allowed=allowed,
                remaining=remaining,
                reset_time=reset_time,
                retry_after=1 if not allowed else None,
                rule_name=rule.name,
                current_usage=rule.limit - remaining
            )

        except Exception as e:
            logging.error(f"Error checking token bucket rate limit: {e}")
            return RateLimitResult(allowed=True, remaining=0, reset_time=0)

    async def add_rule(self, rule: RateLimitRule):
        """Add or update rate limit rule."""
        self.rules[rule.name] = rule
        logging.info(f"Added rate limit rule: {rule.name}")

    async def remove_rule(self, rule_name: str):
        """Remove rate limit rule."""
        if rule_name in self.rules:
            del self.rules[rule_name]
            logging.info(f"Removed rate limit rule: {rule_name}")

    def add_bypass_token(self, token: str):
        """Add bypass token for privileged access."""
        self.bypass_tokens.add(token)

    def remove_bypass_token(self, token: str):
        """Remove bypass token."""
        self.bypass_tokens.discard(token)
```

### 3. Business Intelligence Dashboard (`app/business_intelligence/dashboard.py`):
```python
import asyncio
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
import asyncpg
import aioredis
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from fastapi import FastAPI, HTTPException
import json
import logging

class BusinessIntelligenceDashboard:
    """Advanced business intelligence and analytics dashboard."""

    def __init__(self, postgres_url: str, redis_url: str):
        self.postgres_url = postgres_url
        self.redis_url = redis_url
        self.postgres_pool = None
        self.redis_pool = None
        self.prediction_models = {}
        self.cache_ttl = 300  # 5 minutes

    async def initialize(self):
        """Initialize BI dashboard."""
        try:
            # Initialize database connections
            self.postgres_pool = await asyncpg.create_pool(
                self.postgres_url,
                min_size=3,
                max_size=10
            )

            self.redis_pool = aioredis.ConnectionPool.from_url(
                self.redis_url, max_connections=10
            )

            # Initialize prediction models
            await self._initialize_prediction_models()

            logging.info("BI Dashboard initialized successfully")

        except Exception as e:
            logging.error(f"Failed to initialize BI dashboard: {e}")
            raise

    async def get_realtime_metrics(self, organization_id: Optional[str] = None) -> Dict[str, Any]:
        """Get real-time platform metrics."""
        try:
            cache_key = f"realtime_metrics:{organization_id or 'global'}"

            # Try cache first
            redis = aioredis.Redis(connection_pool=self.redis_pool)
            cached_data = await redis.get(cache_key)

            if cached_data:
                return json.loads(cached_data)

            # Calculate metrics from database
            async with self.postgres_pool.acquire() as conn:
                # Get current active users
                active_users = await conn.fetchval(
                    """
                    SELECT COUNT(DISTINCT user_id)
                    FROM analytics_events
                    WHERE timestamp > NOW() - INTERVAL '15 minutes'
                    AND ($1 IS NULL OR organization_id = $1)
                    """,
                    organization_id
                )

                # Get requests per minute
                requests_per_minute = await conn.fetchval(
                    """
                    SELECT COUNT(*)
                    FROM analytics_events
                    WHERE timestamp > NOW() - INTERVAL '1 minute'
                    AND event_type = 'api_request'
                    AND ($1 IS NULL OR organization_id = $1)
                    """,
                    organization_id
                )

                # Get error rate
                error_rate = await conn.fetchval(
                    """
                    SELECT
                        COALESCE(
                            COUNT(*) FILTER (WHERE properties->>'status_code' >= '400') * 100.0 /
                            NULLIF(COUNT(*), 0),
                            0
                        ) as error_rate
                    FROM analytics_events
                    WHERE timestamp > NOW() - INTERVAL '5 minutes'
                    AND event_type = 'api_request'
                    AND ($1 IS NULL OR organization_id = $1)
                    """,
                    organization_id
                )

                # Get average response time
                avg_response_time = await conn.fetchval(
                    """
                    SELECT AVG((properties->>'response_time_ms')::float)
                    FROM analytics_events
                    WHERE timestamp > NOW() - INTERVAL '5 minutes'
                    AND event_type = 'api_request'
                    AND properties->>'response_time_ms' IS NOT NULL
                    AND ($1 IS NULL OR organization_id = $1)
                    """,
                    organization_id
                )

            metrics = {
                'active_users': active_users or 0,
                'requests_per_minute': requests_per_minute or 0,
                'error_rate_percentage': float(error_rate or 0),
                'avg_response_time_ms': float(avg_response_time or 0),
                'timestamp': datetime.utcnow().isoformat(),
                'organization_id': organization_id
            }

            # Cache the results
            await redis.setex(
                cache_key,
                self.cache_ttl,
                json.dumps(metrics, default=str)
            )

            return metrics

        except Exception as e:
            logging.error(f"Error getting realtime metrics: {e}")
            raise HTTPException(status_code=500, detail="Failed to get realtime metrics")

    async def get_usage_forecast(
        self,
        organization_id: str,
        forecast_days: int = 30
    ) -> Dict[str, Any]:
        """Generate usage forecast using predictive analytics."""
        try:
            # Get historical usage data
            end_date = datetime.utcnow()
            start_date = end_date - timedelta(days=90)  # Use 90 days of history

            async with self.postgres_pool.acquire() as conn:
                usage_data = await conn.fetch(
                    """
                    SELECT
                        DATE_TRUNC('day', timestamp) as date,
                        COUNT(*) as daily_requests,
                        COUNT(DISTINCT user_id) as daily_active_users,
                        AVG((properties->>'response_time_ms')::float) as avg_response_time
                    FROM analytics_events
                    WHERE organization_id = $1
                    AND timestamp BETWEEN $2 AND $3
                    AND event_type = 'api_request'
                    GROUP BY DATE_TRUNC('day', timestamp)
                    ORDER BY date
                    """,
                    organization_id, start_date, end_date
                )

            if not usage_data:
                return {'error': 'Insufficient historical data for forecasting'}

            # Prepare data for ML model
            df = pd.DataFrame([dict(row) for row in usage_data])
            df['date'] = pd.to_datetime(df['date'])
            df = df.set_index('date').fillna(0)

            # Create features
            df['day_of_week'] = df.index.dayofweek
            df['day_of_month'] = df.index.day
            df['week_of_year'] = df.index.isocalendar().week

            # Use rolling averages as features
            df['requests_7d_avg'] = df['daily_requests'].rolling(7).mean()
            df['requests_30d_avg'] = df['daily_requests'].rolling(30).mean()
            df['users_7d_avg'] = df['daily_active_users'].rolling(7).mean()

            # Prepare training data
            feature_cols = [
                'day_of_week', 'day_of_month', 'week_of_year',
                'requests_7d_avg', 'requests_30d_avg', 'users_7d_avg'
            ]

            # Remove rows with NaN values
            df_clean = df.dropna()

            if len(df_clean) < 14:  # Need minimum data for reliable prediction
                return {'error': 'Insufficient clean data for forecasting'}

            X = df_clean[feature_cols].values
            y_requests = df_clean['daily_requests'].values
            y_users = df_clean['daily_active_users'].values

            # Train models
            model_requests = RandomForestRegressor(n_estimators=100, random_state=42)
            model_users = RandomForestRegressor(n_estimators=100, random_state=42)

            model_requests.fit(X, y_requests)
            model_users.fit(X, y_users)

            # Generate forecasts
            forecast_dates = pd.date_range(
                start=end_date.date() + timedelta(days=1),
                periods=forecast_days,
                freq='D'
            )

            forecasts = []
            last_row = df_clean.iloc[-1].copy()

            for date in forecast_dates:
                # Create features for forecast date
                features = [
                    date.dayofweek,
                    date.day,
                    date.isocalendar()[1],
                    last_row['requests_7d_avg'],
                    last_row['requests_30d_avg'],
                    last_row['users_7d_avg']
                ]

                # Make predictions
                pred_requests = model_requests.predict([features])[0]
                pred_users = model_users.predict([features])[0]

                forecasts.append({
                    'date': date.isoformat(),
                    'predicted_requests': max(0, int(pred_requests)),
                    'predicted_active_users': max(0, int(pred_users)),
                    'confidence_interval': {
                        'requests_lower': max(0, int(pred_requests * 0.8)),
                        'requests_upper': int(pred_requests * 1.2),
                        'users_lower': max(0, int(pred_users * 0.8)),
                        'users_upper': int(pred_users * 1.2)
                    }
                })

                # Update rolling averages for next prediction
                last_row['requests_7d_avg'] = pred_requests
                last_row['users_7d_avg'] = pred_users

            # Calculate summary statistics
            total_predicted_requests = sum(f['predicted_requests'] for f in forecasts)
            avg_predicted_users = sum(f['predicted_active_users'] for f in forecasts) / len(forecasts)

            # Calculate growth rate
            recent_avg_requests = df_clean['daily_requests'].tail(7).mean()
            forecast_avg_requests = sum(f['predicted_requests'] for f in forecasts[:7]) / 7
            growth_rate = ((forecast_avg_requests - recent_avg_requests) / recent_avg_requests) * 100

            return {
                'organization_id': organization_id,
                'forecast_period': {
                    'start_date': forecast_dates[0].isoformat(),
                    'end_date': forecast_dates[-1].isoformat(),
                    'days': forecast_days
                },
                'forecasts': forecasts,
                'summary': {
                    'total_predicted_requests': int(total_predicted_requests),
                    'avg_predicted_daily_users': int(avg_predicted_users),
                    'predicted_growth_rate_percentage': round(growth_rate, 2),
                    'model_accuracy_score': round(model_requests.score(X, y_requests), 3)
                },
                'generated_at': datetime.utcnow().isoformat()
            }

        except Exception as e:
            logging.error(f"Error generating usage forecast: {e}")
            raise HTTPException(status_code=500, detail="Failed to generate forecast")

    async def get_revenue_analytics(self, organization_id: Optional[str] = None) -> Dict[str, Any]:
        """Get revenue and business metrics analytics."""
        try:
            async with self.postgres_pool.acquire() as conn:
                # Get subscription metrics
                subscription_metrics = await conn.fetch(
                    """
                    SELECT
                        plan_type,
                        COUNT(*) as subscriber_count,
                        SUM(monthly_revenue) as total_monthly_revenue
                    FROM organizations
                    WHERE ($1 IS NULL OR id = $1)
                    AND status = 'active'
                    GROUP BY plan_type
                    """,
                    organization_id
                )

                # Get usage-based revenue
                usage_revenue = await conn.fetchval(
                    """
                    SELECT COALESCE(SUM(
                        CASE
                            WHEN usage_units > included_units
                            THEN (usage_units - included_units) * price_per_unit
                            ELSE 0
                        END
                    ), 0) as overage_revenue
                    FROM monthly_usage_summary
                    WHERE month = DATE_TRUNC('month', CURRENT_DATE)
                    AND ($1 IS NULL OR organization_id = $1)
                    """,
                    organization_id
                )

                # Get churn analysis
                churn_data = await conn.fetch(
                    """
                    SELECT
                        DATE_TRUNC('month', churned_at) as month,
                        COUNT(*) as churned_customers,
                        AVG(lifetime_value) as avg_lost_ltv
                    FROM organizations
                    WHERE churned_at >= CURRENT_DATE - INTERVAL '12 months'
                    AND ($1 IS NULL OR id = $1)
                    GROUP BY DATE_TRUNC('month', churned_at)
                    ORDER BY month
                    """,
                    organization_id
                )

                # Calculate key metrics
                total_arr = sum(row['total_monthly_revenue'] for row in subscription_metrics) * 12
                total_customers = sum(row['subscriber_count'] for row in subscription_metrics)

                return {
                    'subscription_breakdown': [dict(row) for row in subscription_metrics],
                    'usage_based_revenue': float(usage_revenue or 0),
                    'churn_analysis': [dict(row) for row in churn_data],
                    'key_metrics': {
                        'total_arr': float(total_arr or 0),
                        'total_active_customers': total_customers or 0,
                        'average_revenue_per_customer': float(total_arr / max(1, total_customers)),
                        'monthly_overage_revenue': float(usage_revenue or 0)
                    },
                    'generated_at': datetime.utcnow().isoformat()
                }

        except Exception as e:
            logging.error(f"Error getting revenue analytics: {e}")
            raise HTTPException(status_code=500, detail="Failed to get revenue analytics")

    async def _initialize_prediction_models(self):
        """Initialize ML models for predictive analytics."""
        try:
            # This would typically load pre-trained models from storage
            # For now, we'll initialize empty models that get trained on demand
            self.prediction_models = {
                'usage_forecast': RandomForestRegressor(n_estimators=100, random_state=42),
                'churn_prediction': RandomForestRegressor(n_estimators=100, random_state=42),
                'capacity_planning': RandomForestRegressor(n_estimators=100, random_state=42)
            }

            logging.info("Prediction models initialized")

        except Exception as e:
            logging.error(f"Error initializing prediction models: {e}")
```

## Dependencies
- FastAPI with advanced middleware support
- PostgreSQL with analytics tables and indexes
- Redis for high-performance caching and rate limiting
- pandas and scikit-learn for analytics and ML
- aioredis for async Redis operations
- asyncpg for high-performance PostgreSQL access

## Acceptance Criteria
- [ ] Analytics pipeline processes 100k+ events/second with <50ms latency
- [ ] Rate limiting prevents abuse while maintaining 99.95% legitimate request success
- [ ] Real-time dashboards update within 5 seconds
- [ ] Predictive analytics achieve 85%+ accuracy on usage forecasts
- [ ] Business intelligence reports provide actionable insights
- [ ] Multi-tier rate limiting supports enterprise SLA requirements

## Time Estimate
**Total: 18-22 days**
- Analytics pipeline implementation: 6-8 days
- Rate limiting system: 5-6 days
- BI dashboard and reporting: 4-5 days
- ML models and predictions: 3-4 days
- Testing and optimization: 2-3 days

## Required Skills
- Advanced Python async programming
- Real-time analytics and streaming data
- Machine learning and predictive modeling
- Rate limiting algorithms and patterns
- High-performance Redis operations
- Business intelligence and data visualization

## Risks
- **High**: Analytics pipeline performance under extreme load
- **Medium**: Rate limiting algorithm accuracy and fairness
- **Medium**: ML model accuracy for diverse usage patterns
- **Low**: Dashboard real-time update latency

## Additional Notes
- Implement comprehensive monitoring for all analytics components
- Use feature flags for gradual rollout of new rate limiting rules
- Ensure GDPR compliance for all analytics data collection
- Consider data retention policies for long-term analytics storage
